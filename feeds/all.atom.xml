<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Vlad Niculae (~vene)</title><link href="http://vene.ro/" rel="alternate"></link><link href="http://vene.ro/feeds/all.atom.xml" rel="self"></link><id>http://vene.ro/</id><updated>2013-04-22T10:45:00+02:00</updated><entry><title>BibTeX-powered publications list for Pelican with pelican-bibtex</title><link href="http://vene.ro/blog/bibtex-powered-publications-list-for-pelican-with-pelican-bibtex.html" rel="alternate"></link><updated>2013-04-22T10:45:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2013-04-22:blog/bibtex-powered-publications-list-for-pelican-with-pelican-bibtex.html</id><summary type="html">&lt;h2&gt;Hook&lt;/h2&gt;
&lt;p&gt;Wouldn&amp;#8217;t you like to manage your academic publications list easily
within the context of your static website? Without resorting to external
services, or to software like &lt;em&gt;bibtex2html&lt;/em&gt;, which is very nice but will
then require restyling to fit your&amp;nbsp;templates?&lt;/p&gt;
&lt;p&gt;Look no more, with the help of &lt;a href="https://github.com/vene/pelican-bibtex"&gt;pelican-bibtex&lt;/a&gt; you can now manage
your papers from within&amp;nbsp;Pelican!&lt;/p&gt;
&lt;h2&gt;Backstory&lt;/h2&gt;
&lt;p&gt;At &lt;a href="http://fseoane.net"&gt;Fabian&lt;/a&gt;&amp;#8216;s advice, I started playing around with &lt;a href="http://getpelican.com"&gt;Pelican&lt;/a&gt;, a
static website/blog generator for Python. I like it better than the
other generators I used before, so I chose it the next time I had to set
up a website. I still didn&amp;#8217;t make the courage to migrate my current
website and blog to it, but I promise I&amp;nbsp;will.&lt;/p&gt;
&lt;p&gt;Pelican has a public plugins repository, but they have a license
constraint for all contributions. My plugin isn&amp;#8217;t complicated, but I had
to &amp;#8220;reverse engineer&amp;#8221; undocumented parts of the &lt;a href="http://pybtex.sourceforge.net"&gt;pybtex&lt;/a&gt; &lt;span class="caps"&gt;API&lt;/span&gt;. I think
that maybe that code that I used to render citations programatically can
be useful to others, so I don&amp;#8217;t want to release it under a restrictive
license. For this reason, I publish &lt;a href="https://github.com/vene/pelican-bibtex"&gt;pelican-bibtex&lt;/a&gt; in my personal
GitHub&amp;nbsp;account.&lt;/p&gt;
&lt;p&gt;You can see it in action in the &lt;a href="https://github.com/nlp-unibuc/nlp-unibuc-website/"&gt;source code&lt;/a&gt; for the website I am
working on at the moment, the home page of my research group. Example
output generated using pelican-bibtex can be seen &lt;a href="http://nlp-unibuc.github.io/publications.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Possible&amp;nbsp;extensions&lt;/h2&gt;
&lt;p&gt;I have not dug in too deeply but I believe this plugin can be extended,
with not much difficulty, to support referencing in Pelican blogs, and
render BibTeX references at the end of every post. This idea was
suggested by Avaris on #pelican, and I find it very cool. Since I don&amp;#8217;t
need this feature at the moment, it&amp;#8217;s not a priority, but it&amp;#8217;s something
that I would like to see at some&amp;nbsp;point.&lt;/p&gt;</summary><category term="bibtex"></category><category term="blog"></category><category term="citations"></category><category term="pelican"></category><category term="publications"></category><category term="pybtex"></category><category term="references"></category><category term="static blog"></category><category term="static website"></category><category term="Uncategorized"></category></entry><entry><title>Really the most common english idioms?</title><link href="http://vene.ro/blog/really-most-common-english-idioms.html" rel="alternate"></link><updated>2013-02-11T16:50:00+01:00</updated><author><name>vene</name></author><id>tag:vene.ro,2013-02-11:blog/really-most-common-english-idioms.html</id><summary type="html">&lt;p&gt;A while back I ran into &lt;a href="http://voxy.com/blog/index.php/2012/02/top-10-most-common-idioms-in-english/"&gt;this blog post&lt;/a&gt; and it made me wonder. I&amp;#8217;m
not a native speaker but the idiomatic phrases that they note as common
don&amp;#8217;t strike me as such. I don&amp;#8217;t think I have ever encountered them very
often in real&amp;nbsp;dialogue.&lt;/p&gt;
&lt;p&gt;The blog post lists the 10 most common idioms in English. &lt;strong&gt;Idioms&lt;/strong&gt;,
also known less ambiguously as &lt;strong&gt;fixed expressions&lt;/strong&gt;, are units of
language that span at least two words. Their meaning, relatively to the
individual meaning of the parts of the phrase, are figurative. Despite
this, fixed expressions don&amp;#8217;t classify as creative language, or
exploitations. By definition most speakers will unequivocally be
familiar with&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;For example, they cite &lt;em&gt;piece of cake&lt;/em&gt; as the most common idiomatic
expression. This refers to using the phrase to mean that something is
easy, that it isn&amp;#8217;t challenging. An example of literal use, however,
would be when ordering &lt;em&gt;a piece of cake&lt;/em&gt; for desert in a&amp;nbsp;restaurant.&lt;/p&gt;
&lt;p&gt;Everyone knows that language is a perpetually changing thing, so to
begin with it&amp;#8217;s even slightly misleading to discuss of the commonness of
a phrase, without giving more context. The blog post doesn&amp;#8217;t justify the
ranking with any numbers anyway, so let&amp;#8217;s take them one by one and find
out how common they really&amp;nbsp;are!&lt;/p&gt;
&lt;h2&gt;Corpus&amp;nbsp;Linguistics&lt;/h2&gt;
&lt;p&gt;The approach we are taking here is known as corpus linguistics. The best
way to argue that a certain phrase is common, that something is used
with a specific meaning or that some constructions are normal is, under
corpus linguistics, not to make up examples that seem reasonable, but to
look at &lt;strong&gt;representative collections of text&lt;/strong&gt; (corpora) and trying to
find the examples there. The conclusions you get this way are backed by
real-world language&amp;nbsp;use.&lt;/p&gt;
&lt;p&gt;An argument often brought against generative linguistics is that it
focuses on the (hard) border between grammatical and not grammatical,
and the border is usually defined by made-up examples. This is
inappropriate for studying how the norms are exploited in real language
use, for example. I refer the interested to the work of &lt;a href="http://www.patrickhanks.com/"&gt;Patrick
Hanks&lt;/a&gt; [&lt;a href="#f1"&gt;1&lt;/a&gt;, &lt;a href="#f2"&gt;2&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Corpus linguistics is sensitive to the corpus used. For this example
let&amp;#8217;s use two British English corpora: the &lt;a href="http://www.natcorp.ox.ac.uk/"&gt;British National Corpus&lt;/a&gt;
and the &lt;a href="http://oxforddictionaries.com/words/the-oxford-english-corpus"&gt;Oxford English Corpus&lt;/a&gt;. Measuring by number of words, the
latter is around 20 times bigger. The strong point of the &lt;span class="caps"&gt;BNC&lt;/span&gt; is the
attention given to the mixing proportions of various domains. The &lt;span class="caps"&gt;OEC&lt;/span&gt;,
on the other hand, is larger and more recent. I have a feeling (but I
cannot strongly affirm) that the differences in the following results
arise from the inclusion in the &lt;span class="caps"&gt;OEC&lt;/span&gt; of blogs dating from the&amp;nbsp;mid-2000s.&lt;/p&gt;
&lt;h2&gt;Cognitive salience vs. social&amp;nbsp;salience&lt;/h2&gt;
&lt;p&gt;One of the key ideas that motivate corpus approaches is the mismatch
between these. The cognitive salience of something is the ease with
which we can recall it. An example often used in language is the fixed
expression &lt;em&gt;kicking the bucket&lt;/em&gt;. It is one of the standard examples of
fixed expressions that people give very often when asked. It is supposed
to mean &lt;em&gt;dying&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;However, big surprise: the &lt;span class="caps"&gt;BNC&lt;/span&gt; has only 18 instances of this phrase, out
of which only 3 are idiomatic, the rest being either literal or
metalinguistic. This is a nice example of the salience contrast, but we
mustn&amp;#8217;t hurry to conclusions. The &lt;span class="caps"&gt;OEC&lt;/span&gt; has 193 examples (still few,
relative to its size) but a lot more of them are idiomatic uses. To save
the time I didn&amp;#8217;t look at all the examples, but took a random sample of
size 18, to compare the relative frequencies to &lt;span class="caps"&gt;BNC&lt;/span&gt;. Here, 15 out of 18
instances are idiomatic and none are meta. Quite a&amp;nbsp;difference!&lt;/p&gt;
&lt;p&gt;This goes to show the importance of context when we draw conclusions
about language use. Now let&amp;#8217;s tackle the list with a similar&amp;nbsp;analysis.&lt;/p&gt;
&lt;h2&gt;The&amp;nbsp;idioms&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Piece of&amp;nbsp;cake&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In &lt;span class="caps"&gt;BNC&lt;/span&gt;, this phrase occurs 51 times. 29 of these occurrences,
however, the meaning is literal. In &lt;span class="caps"&gt;OEC&lt;/span&gt; we find 601 occurrences. In
a random sample of size 51 we find 12 literal&amp;nbsp;uses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Costing an arm and a&amp;nbsp;leg&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For flexibility we search for the phrase &lt;em&gt;an arm and a leg&lt;/em&gt;. In &lt;span class="caps"&gt;BNC&lt;/span&gt;
it can be found 29 times: one literal, four with the verb &lt;em&gt;to pay&lt;/em&gt;,
and 16 with &lt;em&gt;to cost&lt;/em&gt;. In &lt;span class="caps"&gt;OEC&lt;/span&gt; it appears 228 times. We take, again,
a sample of size 29 and find no literal uses, 16 with &lt;em&gt;to cost&lt;/em&gt;,
four with &lt;em&gt;to pay&lt;/em&gt;, three with &lt;em&gt;to charge&lt;/em&gt; and a few different uses.
The figurative meaning is the same in all cases: a lot of&amp;nbsp;money.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Break a leg&lt;/strong&gt;
    &lt;p&gt;
    &lt;span class="caps"&gt;BNC&lt;/span&gt;: 16, 13 of which are literal. &lt;span class="caps"&gt;OEC&lt;/span&gt;: 70 hits, 10/16&amp;nbsp;literal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hitting the&amp;nbsp;books&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;BNC&lt;/span&gt;: 1 occurrence of &lt;em&gt;hit the record books&lt;/em&gt;, which has a different
meaning. The idiom is never used. &lt;span class="caps"&gt;OEC&lt;/span&gt;: 135, one of which&amp;nbsp;literal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Letting the cat out of the&amp;nbsp;bag&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We just looked for cooccurrences of &lt;em&gt;cat&lt;/em&gt; in the context of the
phrase &lt;em&gt;out of the bag&lt;/em&gt;.&lt;br /&gt;
&lt;span class="caps"&gt;BNC&lt;/span&gt;: 19, out of which 3 metalinguistic/literal. &lt;span class="caps"&gt;OEC&lt;/span&gt;: 298, and out
of a sample of 19, all were&amp;nbsp;idiomatic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hitting the nail on the&amp;nbsp;head&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;BNC&lt;/span&gt;: 12 instances, all idiomatic. &lt;span class="caps"&gt;OEC&lt;/span&gt;: 484, and out of a sample of
12 all were&amp;nbsp;idiomatic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;When pigs fly&lt;/strong&gt;
    &lt;p&gt;
    We looked for the lemma &lt;em&gt;fly&lt;/em&gt; before the word &lt;em&gt;pigs&lt;/em&gt; therefore
    catching multiple variations.&lt;br /&gt;
    &lt;span class="caps"&gt;BNC&lt;/span&gt;: 17 hits, &lt;span class="caps"&gt;OEC&lt;/span&gt;:&amp;nbsp;240.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Judging a book by its&amp;nbsp;cover&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We looked for the fixed phrase &lt;em&gt;book by its cover&lt;/em&gt;, because the
leading verb might vary.&lt;br /&gt;
In the &lt;span class="caps"&gt;BNC&lt;/span&gt;, 11 instances (1 of them with tell instead of judge). In
&lt;span class="caps"&gt;OEC&lt;/span&gt;, 195 instances. Sampling 11, all were&amp;nbsp;idiomatic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Biting off more than one can&amp;nbsp;chew&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;BNC&lt;/span&gt;: 16 occurences, one of which with &amp;#8220;to take&amp;#8221; instead of &amp;#8220;to
bite&amp;#8221;. &lt;span class="caps"&gt;OEC&lt;/span&gt;: 231, all idiomatic after sampling&amp;nbsp;16.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scratching one&amp;#8217;s&amp;nbsp;back&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;BNC&lt;/span&gt;: 23, out of which only 5 idiomatic. &lt;span class="caps"&gt;OEC&lt;/span&gt;: 756, 5/23&amp;nbsp;idiomatic.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Recalculating the&amp;nbsp;rank&lt;/h2&gt;
&lt;p&gt;
We now have enough data to reorder the expressions and compare. The
result will be more approximate for the &lt;span class="caps"&gt;OEC&lt;/span&gt; because of our use of small
subsamples to estimate the frequencies, but hopefully it will still be
interesting. The way we are estimating the counts for the &lt;span class="caps"&gt;OEC&lt;/span&gt; is as
follows: take, for instance, *break a leg*. It was found 70 times, and
out of a sample of 16, 10 were literal. The expected number of idiomatic
uses is therefore:

&lt;center&gt;
[latex]n = &amp;#92;left ( 1 - &amp;#92;frac{10}{16} &amp;#92;right ) &amp;#92;cdot 70 =
26.25[/latex]

&lt;/center&gt;

Repeating this computation and skipping a ton of steps leads to the
following&amp;nbsp;rankings:

&lt;/p&gt;

&lt;div style="float: left; margin-left: 5em;"&gt;
**In the British National&amp;nbsp;Corpus:**

&lt;/p&gt;
1.  Costing an arm and a leg
2.  Piece of cake
3.  When pigs fly
4.  Letting the cat out of the bag
5.  Biting off more than one can chew
6.  Hitting the nail on the head
7.  Judging a book by its cover
8.  Scratching one’s back
9.  Break a leg
10. Hitting the books

&lt;/div&gt;

&lt;div style="float: right; margin-right: 5em;"&gt;
**In the Oxford English&amp;nbsp;Corpus:**

&lt;/p&gt;
1.  Hitting the nail on the head
2.  Piece of cake
3.  Letting the cat out of the bag
4.  When pigs fly
5.  Biting off more than one can chew
6.  Costing an arm and a leg
7.  Judging a book by its cover
8.  Scratching one’s back
9.  Hitting the books
10. Break a leg

&lt;/div&gt;

&lt;p&gt;We can see that apart from the apparent switching of &lt;em&gt;hitting the nail
on the head&lt;/em&gt; with &lt;em&gt;costing an arm and a leg&lt;/em&gt;, the rankings are not too
different. We can quantify this by using the &lt;strong&gt;Rank Distance&lt;/strong&gt;, a metric
introduced by Liviu P. Dinu [&lt;a href="#f3"&gt;3&lt;/a&gt;, &lt;a href="#f4"&gt;4&lt;/a&gt;]. Here, all our 3 rankings are
over the same domain: we are not looking for the most frequent idioms in
the corpora, this would be very hard. We are just reordering the
proposed rank according to the occurrences in &lt;span class="caps"&gt;BNC&lt;/span&gt; and &lt;span class="caps"&gt;OEC&lt;/span&gt;. In this
simple case, Rank Distance reduces to [latex]\ell_1[/latex] distance
over rank position vectors. The weighted Rank Distance, bounded on
[latex][0, 1][/latex] is in this case given by a scaling factor of
[latex]0.5k\^2[/latex] where &lt;em&gt;k&lt;/em&gt; is the length of the rankings (10 in
our&amp;nbsp;case).&lt;/p&gt;
&lt;p&gt;The computed distance between the original ranking and the &lt;span class="caps"&gt;BNC&lt;/span&gt;
reordering is 0.52. Between the original and the &lt;span class="caps"&gt;OEC&lt;/span&gt; reordering, it is
0.68. Our two reorderings are much closer: the distance is 0.28. This is
mostly because that the permutations between the two reorderings affect
the top position, and are therefore weighted&amp;nbsp;more.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s also interesting to look at the ratio of the counts. Interestingly,
they approximately differ by a constant factor not far from the relative
size difference of the two corpora, as would be&amp;nbsp;expected.&lt;/p&gt;
&lt;p&gt;We have to throw away &lt;em&gt;hitting the books&lt;/em&gt; because its &lt;span class="caps"&gt;BNC&lt;/span&gt; zero count
leads to divisions by zero. After this step, the average of the relative
counts of the idioms is 19.5, with a standard deviation of 10.1, while
&lt;span class="caps"&gt;OED&lt;/span&gt; is supposed to have around 20 times more words than the&amp;nbsp;&lt;span class="caps"&gt;BNC&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Well, it seems people don&amp;#8217;t say &lt;em&gt;break a leg&lt;/em&gt; and &lt;em&gt;let&amp;#8217;s hit the books&lt;/em&gt;
as often as the original author claims. The popularity of most of the
cited idioms seems supported by the data, but we have no easy way to
find other idioms that might turn out to be much more frequent. Corpus
linguistics is a reliable way to measure the social salience of language
patterns It should always be used to verify and back empty claims of the
form &lt;em&gt;X is correct&lt;/em&gt;, &lt;em&gt;Y is frequent&lt;/em&gt; or &lt;em&gt;Nobody says Z&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;span id="f1"&gt;1&lt;/span&gt;] Patrick Hanks, &lt;a href="http://www.patrickhanks.com/uploads/5/1/4/9/5149363/howpeopleusewordstomakemeanings.pdf"&gt;How people use words to make
meanings&lt;/a&gt;.&lt;br /&gt;
[&lt;span id="f2"&gt;2&lt;/span&gt;] Patrick Hanks, &lt;a href="http://www.amazon.com/Lexical-Analysis-Exploitations-Patrick-Hanks/dp/0262018578"&gt;Lexical Analysis: Norms and
Exploitations&lt;/a&gt;. The &lt;span class="caps"&gt;MIT&lt;/span&gt; Press (January 25, 2013)&lt;br /&gt;
[&lt;span id="f3"&gt;3&lt;/span&gt;] Liviu P. Dinu, Florin Manea. &lt;a href="http://dl.acm.org/citation.cfm?id=1167105"&gt;An efficient
approach for the rank aggregation problem&lt;/a&gt;. In: Theoretical Computer
Science, Volume 359 Issue 1, 14 August 2006. Pages 455 - 461.&lt;br /&gt;
[&lt;span id="f4"&gt;4&lt;/span&gt;] Liviu P. Dinu, [On the Classification and
Aggregation of Hierarchies with Different Constitutive Elements][].
Fundam. Inform. 55(1): 39-50&amp;nbsp;(2003)&lt;/p&gt;
&lt;p&gt;[On the Classification and Aggregation of Hierarchies with Different
  Constitutive Elements]:&amp;nbsp;http://dl.acm.org/citation.cfm?id=937465&lt;/p&gt;</summary><category term="bnc"></category><category term="british national corpus"></category><category term="corpus"></category><category term="fixed expression"></category><category term="fixed phrase"></category><category term="idioms"></category><category term="oec"></category><category term="oxford english corpus"></category><category term="corpus linguistics"></category><category term="nlp"></category></entry><entry><title>Scikit-learn-speed: An overview on the final day</title><link href="http://vene.ro/blog/scikit-learn-speed-an-overview-on-the-final-day.html" rel="alternate"></link><updated>2012-08-20T02:44:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-08-20:blog/scikit-learn-speed-an-overview-on-the-final-day.html</id><summary type="html">&lt;p&gt;This summer, I was granted the project called &lt;em&gt;scikit-learn-speed&lt;/em&gt;,
consisting of developing a benchmarking platform for &lt;em&gt;scikit-learn&lt;/em&gt; and
using it to find potential speedups, and in the end, make the library go
faster wherever I&amp;nbsp;can.&lt;/p&gt;
&lt;p&gt;On the official closing day of this work, I&amp;#8217;d like to take a moment and
recall the accomplishments and failures of this project, and all the
lessons to be&amp;nbsp;learned.&lt;/p&gt;
&lt;h2&gt;The &lt;em&gt;scikit-learn-speed&lt;/em&gt; benchmark&amp;nbsp;platform&lt;/h2&gt;
&lt;p&gt;[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/08/skl-speed-300x163.png" title="skl-speed" /&gt;][]&lt;br /&gt;
[&lt;em&gt;Scikit-learn-speed&lt;/em&gt;][&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/08/skl-speed-300x163.png" title="skl-speed" /&gt;] is a continuous benchmark suite for the
&lt;a href="http://scikit-learn.org"&gt;&lt;em&gt;scikit-learn&lt;/em&gt;&lt;/a&gt; library. It has the following&amp;nbsp;features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;vbench&lt;/em&gt;-powered integration with&amp;nbsp;Git&lt;/li&gt;
&lt;li&gt;Easily triggered build and report generation: just type &lt;code&gt;make&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Easily readable and writeable template for benchmarks:
    &lt;p&gt;
    [sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
    {&lt;br /&gt;
    &amp;#8216;obj&amp;#8217;: &amp;#8216;LogisticRegression&amp;#8217;,&lt;br /&gt;
    &amp;#8216;init_params&amp;#8217;: {&amp;#8216;C&amp;#8217;: 1e5},&lt;br /&gt;
    &amp;#8216;datasets&amp;#8217;: (&amp;#8216;arcene&amp;#8217;, &amp;#8216;madelon&amp;#8217;),&lt;br /&gt;
    &amp;#8216;statements&amp;#8217;: (&amp;#8216;fit&amp;#8217;, &amp;#8216;predict&amp;#8217;)&lt;br /&gt;
    }, &amp;#8230;&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/li&gt;
&lt;li&gt;Many attributes recorded: time (w/ estimated standard deviation),
    memory usage, cProfiler output, line_profiler output,&amp;nbsp;tracebacks&lt;/li&gt;
&lt;li&gt;Multi-step benchmarks: i.e. &lt;code&gt;fit&lt;/code&gt; followed by &lt;code&gt;predict&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What were the lessons I learned&amp;nbsp;here?&lt;/p&gt;
&lt;h3&gt;Make your work reusable: the trade-off between good design and&amp;nbsp;get-it-working-now&lt;/h3&gt;
&lt;p&gt;For the task of rolling out a continuous benchmarking platform, we
decided pretty early in the project to adopt Wes McKinney&amp;#8217;s &lt;em&gt;vbench&lt;/em&gt;. If
my goal would&amp;#8217;ve been to maintain &lt;em&gt;vbench&lt;/em&gt; and extend it into a
multi-purpose, reusable benchmarking framework, the work would&amp;#8217;ve been
structured differently. It also would have been very open-ended and
difficult to&amp;nbsp;quantify.&lt;/p&gt;
&lt;p&gt;The way things have been, I came up with features that we need in
&lt;em&gt;scikit-learn-speed&lt;/em&gt;, and tried to implement them in &lt;em&gt;vbench&lt;/em&gt; without
refactoring too much, but still by trying to make them as reusable as&amp;nbsp;possible.&lt;/p&gt;
&lt;p&gt;The result? I got all the features for &lt;em&gt;scikit-learn-speed&lt;/em&gt;, but the
implementation is not yet clean enough to be merged into &lt;em&gt;vbench&lt;/em&gt;. This
is fine for a project with a tight deadline such as this one: after it&amp;#8217;s
done, I will just spend another weekend on cleaning the work up and
making sure it&amp;#8217;s appreciated upstream. This will be easier because of
the constraint to keep compatibility with &lt;em&gt;scikit-learn-speed&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Never work quietly (unless you&amp;#8217;re a&amp;nbsp;ninja)&lt;/h3&gt;
&lt;p&gt;I know some students who prefer that the professor doesn&amp;#8217;t even know
they exist until the final, when they would score an A, and (supposedly)
leave the professor amazed. In real life, plenty of people would be
interested in what you are doing, as long as they know about it. The &lt;span class="caps"&gt;PSF&lt;/span&gt;
goes a long way to help this, with the &amp;#8220;blog weekly&amp;#8221; rule. In the end,
however, it&amp;#8217;s all up to you to make sure that everybody who should know
finds out about your work. It will spare the world the duplicated work,
the abandoned projects, but most importantly, those people could point
you to things you have missed. Try to mingle in real-life as well,
attend conferences, meetups, coding&amp;nbsp;sprints.&lt;/p&gt;
&lt;p&gt;I was able to slightly &amp;#8220;join forces&amp;#8221; with a couple of people who
contacted me about my new &lt;em&gt;vbench&lt;/em&gt; features (Hi Jon and Joel!), I have
shaped my design slightly towards their requirements as well, and
hopefully the result will be a more general &lt;em&gt;vbench&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;The&amp;nbsp;speedups&lt;/h2&gt;
&lt;p&gt;Once &lt;em&gt;scikit-learn-speed&lt;/em&gt; was up and running, I couldn&amp;#8217;t believe how
useful it is to be able to scroll, catch slow code and jump straight at
the profiler output with one click. I jumped on the following&amp;nbsp;speed-ups:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple outputs in linear models. (&lt;a href="https://github.com/scikit-learn/scikit-learn/pull/913"&gt;&lt;span class="caps"&gt;PR&lt;/span&gt;&lt;/a&gt;)
    &lt;p&gt;
    Some of them proved trickier than expected, so I didn&amp;#8217;t implement it
    for all the module yet, but it is ready for some&amp;nbsp;estimators.&lt;/li&gt;
&lt;li&gt;Less callable functions passed around in &lt;code&gt;FastICA&lt;/code&gt; (&lt;a href="https://github.com/scikit-learn/scikit-learn/pull/927"&gt;merged&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Speed up &lt;code&gt;euclidean_distances&lt;/code&gt; by rewriting in Cython. (&lt;a href="https://github.com/scikit-learn/scikit-learn/pull/1006"&gt;&lt;span class="caps"&gt;PR&lt;/span&gt;&lt;/a&gt;)
    &lt;p&gt;
    This meant making more operations support an &lt;code&gt;out&lt;/code&gt; argument, for
    passing preallocated memory. This touches many&lt;br /&gt;
    different objects in the codebase: clustering, manifold learning,
    nearest neighbour&amp;nbsp;methods.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://localhost:8001/2012/08/18/inverses-pseudoinverses-numerical-issues-speed-symmetry/" title="Inverses and pseudoinverses. Numerical issues, speed, symmetry."&gt;Insight into inverse and pseudoinverse computation&lt;/a&gt;, new &lt;code&gt;pinvh&lt;/code&gt;
    function for inverting symmetric/hermitian matrices. (&lt;a href="https://github.com/scikit-learn/scikit-learn/pull/1015"&gt;&lt;span class="caps"&gt;PR&lt;/span&gt;&lt;/a&gt;)
    &lt;p&gt;
    This speeds up the covariance module (especially &lt;code&gt;MinCovDet&lt;/code&gt;),
    &lt;code&gt;ARDRegression&lt;/code&gt; and the mixture models. It also lead to an [upstream
    contribution to&amp;nbsp;Scipy][]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;OrthogonalMatchingPursuit&lt;/code&gt; forward stepwise path for
    cross-validation (&lt;a href="https://github.com/scikit-learn/scikit-learn/pull/1042"&gt;&lt;span class="caps"&gt;PR&lt;/span&gt;&lt;/a&gt;)
    &lt;p&gt;
    This is only halfway finished, but it will lead to faster and easier
    optimization of the &lt;code&gt;OMP&lt;/code&gt; sparsity&amp;nbsp;parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lessons? These will be pretty&amp;nbsp;obvious.&lt;/p&gt;
&lt;h3&gt;Write tests, tests,&amp;nbsp;tests!&lt;/h3&gt;
&lt;p&gt;This is a no-brainer, but it still didn&amp;#8217;t stick. In that one case out of
10 that I didn&amp;#8217;t explicitly test, a bug was obviously hiding. When you
want to add a new feature, it&amp;#8217;s best to start by writing a failing test,
and then &lt;a href="http://c2.com/cgi/wiki?MakeItWorkMakeItRightMakeItFast"&gt;making it pass&lt;/a&gt;. Sure, you will miss tricky bugs, but you
will never have embarrassing, obvious bugs in your code&amp;nbsp;:)&lt;/p&gt;
&lt;h3&gt;Optimization doesn&amp;#8217;t have to be&amp;nbsp;ugly&lt;/h3&gt;
&lt;p&gt;Developers often shun optimization. It&amp;#8217;s true, you should profile first,
and you shouldn&amp;#8217;t focus on speeding up stuff that is dominated by other
computations that are orders of magnitude slower. However, there is an
elephant in the room: the assumption that making code faster invariably
makes it less clear, and takes a lot of&amp;nbsp;effort.&lt;/p&gt;
&lt;p&gt;The following code is a part of scipy&amp;#8217;s &lt;code&gt;pinv2&lt;/code&gt; function as it currently
is written:&lt;br /&gt;
[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
cutoff = cond*np.maximum.reduce(s)&lt;br /&gt;
psigma = np.zeros((m, n), t)&lt;br /&gt;
for i in range(len(s)):&lt;br /&gt;
if s[i] &gt; cutoff:&lt;br /&gt;
psigma[i,i] = 1.0/np.conjugate(s[i])&lt;br /&gt;
return np.transpose(np.conjugate(np.dot(np.dot(u,psigma),vh)))&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;&lt;code&gt;psigma&lt;/code&gt; is a diagonal matrix, and some time and memory can be saved
with simple vectorization. However, this part of the code dominated by
an above call to &lt;code&gt;svd&lt;/code&gt;. The profiler output would say that we shouldn&amp;#8217;t
bother, but is it really a bother? Look at Jake&amp;#8217;s new&amp;nbsp;version:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
above_cutoff = (s &gt; cond * np.max(s))&lt;br /&gt;
psigma_diag = np.zeros_like(s)&lt;br /&gt;
psigma_diag[above_cutoff] = 1.0 /&amp;nbsp;s[above_cutoff]&lt;/p&gt;
&lt;p&gt;return np.transpose(np.conjugate(np.dot(u * psigma_diag, vh)))&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s shorter, more elegant, easier to read, and nevertheless faster. I
would say it is worth&amp;nbsp;it.&lt;/p&gt;
&lt;h3&gt;Small speed-ups can&amp;nbsp;propagate&lt;/h3&gt;
&lt;p&gt;Sure, it&amp;#8217;s great if you can compute an inverse two times faster, say in
0.5s instead of 1s. But if some algorithm calls this function in a loop
that might iterate 100, 300, or 1000 times, this small speed-up seems
much more important, doesn&amp;#8217;t&amp;nbsp;it?&lt;/p&gt;
&lt;p&gt;What I&amp;#8217;m trying to say with this is that in a well-engineered system, a
performance improvement to a relatively small component (such as the
function that computes a pseudoinverse) can lead to multiple spread out
improvements. Be careful of the double edge of this sword, a bug
introduced in a small part can cause multiple failures downstream. But
you &lt;em&gt;are&lt;/em&gt; fully covered by your test suite, aren&amp;#8217;t&amp;nbsp;you?&lt;/p&gt;
&lt;p&gt;Overall it has been a fruitful project that may have not resulted in a
large number of speed-ups, but a few considerable ones nonetheless. And
I venture the claim that the &lt;em&gt;scikit-learn-speed&lt;/em&gt; tool will prove useful
over time, and that the efforts deployed during this project have
stretched beyond the boundary of the &lt;em&gt;scikit-learn&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/08/skl-speed-300x163.png" title="skl-speed" /&gt;]:&amp;nbsp;http://jenkins-scikit-learn.github.com/scikit-learn-speed/&lt;/p&gt;</summary><category term="gsoc"></category><category term="optimization"></category><category term="scikit-learn-speed"></category><category term="speedup"></category><category term="summary"></category><category term="vbench"></category><category term="benchmarking"></category><category term="python"></category><category term="scikit-learn"></category></entry><entry><title>Inverses and pseudoinverses. Numerical issues, speed, symmetry.</title><link href="http://vene.ro/blog/inverses-pseudoinverses-numerical-issues-speed-symmetry.html" rel="alternate"></link><updated>2012-08-18T19:41:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-08-18:blog/inverses-pseudoinverses-numerical-issues-speed-symmetry.html</id><summary type="html">&lt;p&gt;The matrix inverse is a cornerstone of linear algebra, taught, along
with its applications, since high school. The inverse of a matrix
\$latex A\$, if it exists, is the matrix \$latex A\^{-1}\$ such that
\$latex &lt;span class="caps"&gt;AA&lt;/span&gt;\^{-1} = A\^{-1}A = I_n\$. Based on the requirement that the
left and right multiplications should be equal, it follows that it only
makes sense to speak of inverting square matrices. But just the square
shape is not enough: for a matrix \$latex A\$ to have an inverse,
\$latex A\$ must be full&amp;nbsp;rank.&lt;/p&gt;
&lt;p&gt;The inverse provides an elegant (on paper) method of finding solutions
to systems of \$latex n\$ equations with \$latex n\$ unknowns, which
correspond to solving \$latex Ax = b\$ for \$latex x\$. If we&amp;#8217;re lucky
and \$latex A\^{-1}\$ exists, then we can find \$latex x = A\^{-1}b\$.
For this to work, it must be the case&amp;nbsp;that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We have exactly as many unknowns as&amp;nbsp;equations&lt;/li&gt;
&lt;li&gt;No equation is redundant, i.e. can be expressed as a linear
    combination of the&amp;nbsp;others&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this setting, there is a unique solution for \$latex&amp;nbsp;x\$.&lt;/p&gt;
&lt;h2&gt;The Moore-Penrose&amp;nbsp;pseudoinverse&lt;/h2&gt;
&lt;p&gt;What if we have more equations than unknowns? It is most likely the case
that we cannot satisfy all the equations perfectly, so let&amp;#8217;s settle for
a solution that best fits the constraints, in the sense of minimising
the sum of squared errors. We solve \$latex \operatorname{arg\,min}_x
||b -&amp;nbsp;Ax||\$.&lt;/p&gt;
&lt;p&gt;And how about the other extreme, where we have a lot of unknowns, but
just a few equations constraining them. We will probably have an
infinity of solutions, how can we choose one? A popular choice is to
take the one of least \$latex \ell_2\$ norm: \$latex
\operatorname{arg\,min}_x ||x|| \operatorname{s.t.} Ax = b\$. Is
there a way to generalize the idea of a matrix inverse for this&amp;nbsp;setting?&lt;/p&gt;
&lt;p&gt;The pseudoinverse of an arbitrary-shaped matrix \$latex A\$, written
\$latex A\^{+}\$, has the same shape as \$latex A\^{T}\$ and solves our
problem: the answer to both optimization methods above is given by
\$latex x =&amp;nbsp;A\^{+}y\$.&lt;/p&gt;
&lt;p&gt;The theoretical definition of the pseudoinverse is given by the
following conditions. The intuitive way to read them is as properties of
\$latex &lt;span class="caps"&gt;AA&lt;/span&gt;\^+\$ or \$latex&amp;nbsp;A\^+A\$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\$latex &lt;span class="caps"&gt;AA&lt;/span&gt;\^+A =&amp;nbsp;A\$&lt;/li&gt;
&lt;li&gt;\$latex A\^+&lt;span class="caps"&gt;AA&lt;/span&gt;\^+ =&amp;nbsp;A\^+\$&lt;/li&gt;
&lt;li&gt;\$latex (&lt;span class="caps"&gt;AA&lt;/span&gt;\^+)\^T =&amp;nbsp;&lt;span class="caps"&gt;AA&lt;/span&gt;\^+\$&lt;/li&gt;
&lt;li&gt;\$latex (A\^+A)\^T =&amp;nbsp;A\^+A\$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These conditions do not however give us a way to get our hands on a
pseudoinverse, so we need something&amp;nbsp;else.&lt;/p&gt;
&lt;h2&gt;How to compute the pseudoinverse on&amp;nbsp;paper&lt;/h2&gt;
&lt;p&gt;The first time I ran into the pseudoinverse, I didn&amp;#8217;t even know its
definition, only the expression of the closed-form solution of such a
problem, and given&amp;nbsp;as:&lt;/p&gt;
&lt;p&gt;\$latex A\^+ = (A\^T&amp;nbsp;A)\^{-1}A\^T\$&lt;/p&gt;
&lt;p&gt;What can we see from this&amp;nbsp;expression:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It gives us a way to compute the pseudoinverse, and hence to solve
    the&amp;nbsp;problem&lt;/li&gt;
&lt;li&gt;If \$latex A\$ is actually invertible, it means \$latex A\^T\$ is
    invertible, so we have \$latex A\^+ = A\^{-1}(A\^T)\^{-1}A\^T =&amp;nbsp;A\^{-1}\$&lt;/li&gt;
&lt;li&gt;Something bad happens if \$latex A\^&lt;span class="caps"&gt;TA&lt;/span&gt;\$ is not&amp;nbsp;invertible.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The pseudoinverse is still defined, and unique, when \$latex A\^&lt;span class="caps"&gt;TA&lt;/span&gt;\$ is
not invertible, but we cannot use the expression above to compute&amp;nbsp;it.&lt;/p&gt;
&lt;h2&gt;Numerical&amp;nbsp;issues&lt;/h2&gt;
&lt;p&gt;Before going on, we should clarify and demystify some of the urban
legends about numerical computation of least squares problems. You might
have heard the following unwritten&amp;nbsp;rules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Never compute \$latex A\^{-1}\$, solve the system&amp;nbsp;directly&lt;/li&gt;
&lt;li&gt;If you really need \$latex A\^{-1}\$, use &lt;code&gt;pinv&lt;/code&gt; and not &lt;code&gt;inv&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first of these rules is based on some misguided beliefs, but is
still good advice. If your goal is a one-shot answer to a system,
there&amp;#8217;s no use in explicitly computing a possibly large inverse, when
all you need is \$latex x\$. But &lt;a href="http://arxiv.org/abs/1201.6035"&gt;this paper&lt;/a&gt; shows that computing the
inverse is not necessarily a bad thing. The key to this is conditional
accuracy, and as long as the &lt;code&gt;inv&lt;/code&gt; function used has good conditional
bounds, you will get as good results as with a least squares&amp;nbsp;solver.&lt;/p&gt;
&lt;p&gt;The second rule comes from numerical stability, and will definitely bite
you if misunderstood. If \$latex A\$ is a square matrix with a row full
of zeros, it&amp;#8217;s clearly not invertible, so an algorithm attempting to
compute the inverse will fail and you will be able to catch that
failure. But what if the row is not exactly zero, but the sum of several
other rows, and a slight loss of precision is propagated at every&amp;nbsp;step?&lt;/p&gt;
&lt;h2&gt;Numerical rank vs. actual&amp;nbsp;rank&lt;/h2&gt;
&lt;p&gt;The rank of a matrix \$latex A\$ is defined as the number of linearly
independent rows (or equivalently, columns) in \$latex A\$. In other
words, the number of non-redundant equations in the system. We&amp;#8217;ve seen
before that if the rank is less than the total number of rows, the
system cannot have a unique solution anymore, so the matrix \$latex A\$
is not&amp;nbsp;invertible.&lt;/p&gt;
&lt;p&gt;The rank of a matrix is a computationally tricky problem. On paper, with
small matrices, you would look at minors of decreasing size, until you
find the first non-zero one. This is unfeasible to implement on a
computer, so numerical analysis has a different approach. Enter the
singular value&amp;nbsp;decomposition!&lt;/p&gt;
&lt;p&gt;The &lt;span class="caps"&gt;SVD&lt;/span&gt; of a matrix \$latex A\$ is \$latex A = &lt;span class="caps"&gt;USV&lt;/span&gt;\^{T}\$, where \$latex
S\$ is diagonal and \$latex U, V\$ are orthogonal. The elements on the
diagonal of \$latex S\$ are called the singular values of \$latex A\$.
It can be seen that to get a row full of zeros when multiplying three
such matrices, a singular value needs to be exactly&amp;nbsp;zero.&lt;/p&gt;
&lt;p&gt;The ugly thing that could happen is that one (or usually more) singular
values are not exactly zero, but very low values, due to propagated
imprecision. Why is this a problem? By looking at the &lt;span class="caps"&gt;SVD&lt;/span&gt; and noting its
properties, it becomes clear that \$latex A\^{-1} = &lt;span class="caps"&gt;VS&lt;/span&gt;\^{-1}U\^{T}\$ and
since \$latex S\$ is diagonal, its inverse is formed by taking the
inverse of all the elements on the diagonal. But if a singular value is
very small but not quite zero, its inverse is very large and it will
blow up the whole computation of the inverse. The right thing to do here
is either to tell the user that \$latex A\$ is numerically rank
deficient, or to return a pseudoinverse instead. A pseudoinverse would
mean: give up on trying to get \$latex &lt;span class="caps"&gt;AA&lt;/span&gt;\^+\$ to be the identity
matrix, simply aim for a diagonal matrix with approximately ones and
zeroes. In other words, when singular values are very low, set them to&amp;nbsp;0.&lt;/p&gt;
&lt;p&gt;How do you set the threshold? This is actually a delicate issue, being
discussed on &lt;a href="http://thread.gmane.org/gmane.comp.python.numeric.general/50396/focus=50912"&gt;the numeric Python mailing list&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Scipy&amp;nbsp;implementations&lt;/h2&gt;
&lt;p&gt;Scipy exposes &lt;code&gt;inv&lt;/code&gt;, &lt;code&gt;pinv&lt;/code&gt; and &lt;code&gt;pinv2&lt;/code&gt;. &lt;code&gt;inv&lt;/code&gt; secretly invokes &lt;span class="caps"&gt;LAPACK&lt;/span&gt;,
that ancient but crazy robust code that&amp;#8217;s been used since the 70s, to
first compute a pivoted &lt;span class="caps"&gt;LU&lt;/span&gt; decomposition that is then used to compute
the inverse. &lt;code&gt;pinv&lt;/code&gt; also uses &lt;span class="caps"&gt;LAPACK&lt;/span&gt;, but for computing the
least-squares solution to the system \$latex &lt;span class="caps"&gt;AX&lt;/span&gt; = I\$. &lt;code&gt;pinv2&lt;/code&gt; computes
the &lt;span class="caps"&gt;SVD&lt;/span&gt; and transposes everything like shown above. Both &lt;code&gt;pinv&lt;/code&gt; and
&lt;code&gt;pinv2&lt;/code&gt; expose &lt;code&gt;cond&lt;/code&gt; and &lt;code&gt;rcond&lt;/code&gt; arguments to handle the treatment of
very small singular values, but (&lt;em&gt;attention!&lt;/em&gt;) they behave&amp;nbsp;differently!&lt;/p&gt;
&lt;p&gt;The different implementations also lead to different speed. Let&amp;#8217;s look
at inverting a random square&amp;nbsp;matrix:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [1]: import numpy as&amp;nbsp;np&lt;/p&gt;
&lt;p&gt;In [2]: from scipy import&amp;nbsp;linalg&lt;/p&gt;
&lt;p&gt;In [3]: a = np.random.randn(1000,&amp;nbsp;1000)&lt;/p&gt;
&lt;p&gt;In [4]: timeit linalg.inv(a)&lt;br /&gt;
10 loops, best of 3: 132 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [5]: timeit linalg.pinv(a)&lt;br /&gt;
1 loops, best of 3: 18.8 s per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [6]: timeit linalg.pinv2(a)&lt;br /&gt;
1 loops, best of 3: 1.58 s per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Woah, huge difference! But do all three methods return the &amp;#8220;right&amp;#8221;&amp;nbsp;result?&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [7]: linalg.inv(a)[:3, :3]&lt;br /&gt;
Out[7]:&lt;br /&gt;
array([[ 0.03636918, 0.01641725, 0.00736503],&lt;br /&gt;
[-0.04575771, 0.03578062, 0.02937733],&lt;br /&gt;
[ 0.00542367, 0.01246306, 0.0122156&amp;nbsp;]])&lt;/p&gt;
&lt;p&gt;In [8]: linalg.pinv(a)[:3, :3]&lt;br /&gt;
Out[8]:&lt;br /&gt;
array([[ 0.03636918, 0.01641725, 0.00736503],&lt;br /&gt;
[-0.04575771, 0.03578062, 0.02937733],&lt;br /&gt;
[ 0.00542367, 0.01246306, 0.0122156&amp;nbsp;]])&lt;/p&gt;
&lt;p&gt;In [9]: linalg.pinv2(a)[:3, :3]&lt;br /&gt;
Out[9]:&lt;br /&gt;
array([[ 0.03636918, 0.01641725, 0.00736503],&lt;br /&gt;
[-0.04575771, 0.03578062, 0.02937733],&lt;br /&gt;
[ 0.00542367, 0.01246306, 0.0122156&amp;nbsp;]])&lt;/p&gt;
&lt;p&gt;In [10]: np.testing.assert_array_almost_equal(linalg.inv(a),&amp;nbsp;linalg.pinv(a))&lt;/p&gt;
&lt;p&gt;In [11]: np.testing.assert_array_almost_equal(linalg.inv(a),
linalg.pinv2(a))&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Looks good! This is because we got lucky, though, and &lt;code&gt;a&lt;/code&gt; was invertible
to start with. Let&amp;#8217;s look at its&amp;nbsp;spectrum:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [12]: _, s, _ =&amp;nbsp;linalg.svd(a)&lt;/p&gt;
&lt;p&gt;In [13]: np.min(s), np.max(s)&lt;br /&gt;
Out[13]: (0.029850235603382822, 62.949785645178906)&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;This is a lovely range for the singular values of a matrix, not too
small, not too large. But what if we built the matrix in a way that
would always pose problems? Specifically, let&amp;#8217;s look at the case of
covariance&amp;nbsp;matrices:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [14]: a = np.random.randn(1000,&amp;nbsp;50)&lt;/p&gt;
&lt;p&gt;In [15]: a = np.dot(a,&amp;nbsp;a.T)&lt;/p&gt;
&lt;p&gt;In [16]: _, s, _ =&amp;nbsp;linalg.svd(a)&lt;/p&gt;
&lt;p&gt;In [17]: s[-9:]&lt;br /&gt;
Out[17]:&lt;br /&gt;
array([ 7.40548924e-14, 6.48102455e-14, 5.75803505e-14,&lt;br /&gt;
5.44263048e-14, 4.51528730e-14, 3.55317976e-14,&lt;br /&gt;
2.46939141e-14, 1.54186776e-14,&amp;nbsp;5.08135874e-15])&lt;/p&gt;
&lt;p&gt;[/sourcecode]&lt;/p&gt;
&lt;p&gt;&lt;code&gt;a&lt;/code&gt; has at least 9 tiny singular values. Actually it&amp;#8217;s easy to see why
there are 950 of&amp;nbsp;them:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [18]: np.sum(s \&amp;lt; 1e-10)&lt;br /&gt;
Out[18]: 950&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;How do our functions behave in this case? Instead of just looking at a
corner, let&amp;#8217;s use our gift of sight:[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/08/pseudoinverses-300x218.png" title="Pseudoinverses" /&gt;][]&lt;/p&gt;
&lt;p&gt;The small eigenvalues are large enough that &lt;code&gt;inv&lt;/code&gt; thinks the matrix is
full rank. &lt;code&gt;pinv&lt;/code&gt; does better but it still fails, you can see a group of
high-amplitude noisy columns. &lt;code&gt;pinv2&lt;/code&gt; is faster and it also gives us a
useful result in this&amp;nbsp;case.&lt;/p&gt;
&lt;p&gt;Wait, does this mean that &lt;code&gt;pinv2&lt;/code&gt; is simply better, and &lt;code&gt;pinv&lt;/code&gt; is&amp;nbsp;useless?&lt;/p&gt;
&lt;p&gt;Not quite. Remember, we are now trying to actually invert matrices, and
degrade gracefully in case of rank deficiency. But what if we need the
pseudoinverse to solve an actual non-square, wide or tall&amp;nbsp;system?&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [19]: a = np.random.randn(1000,&amp;nbsp;50)&lt;/p&gt;
&lt;p&gt;In [20]: timeit linalg.pinv(a)&lt;br /&gt;
10 loops, best of 3: 104 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [21]: timeit linalg.pinv(a.T)&lt;br /&gt;
100 loops, best of 3: 7.08 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [22]: timeit linalg.pinv2(a)&lt;br /&gt;
10 loops, best of 3: 114 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [23]: timeit linalg.pinv2(a.T)&lt;br /&gt;
10 loops, best of 3: 126 ms per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Huge victory for &lt;code&gt;pinv&lt;/code&gt; in the wide case! Hurray! With all this insight,
we can draw a line and see what we&amp;nbsp;learned.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you are 100% sure that your matrix is invertible, use &lt;code&gt;inv&lt;/code&gt; for a
    huge speed gain. The implementation of &lt;code&gt;inv&lt;/code&gt; from Scipy is based on
    &lt;span class="caps"&gt;LAPACK&lt;/span&gt;&amp;#8217;s &lt;code&gt;*getrf&lt;/code&gt; + &lt;code&gt;*getri&lt;/code&gt;, known to have good&amp;nbsp;bounds.&lt;/li&gt;
&lt;li&gt;If you are trying to solve a tall or wide system, use &lt;code&gt;pinv&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If your matrix is square but might be rank deficient, use &lt;code&gt;pinv2&lt;/code&gt;
    for speed and numerical&amp;nbsp;gain.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Improving the symmetric&amp;nbsp;case&lt;/h2&gt;
&lt;p&gt;But wait a second, can&amp;#8217;t we do better? \$latex &lt;span class="caps"&gt;AA&lt;/span&gt;\^T\$ is symmetric,
can&amp;#8217;t we make use of that to speed up the computation even more?
Clearly, if \$latex A\$ is symmetric, in its &lt;span class="caps"&gt;SVD&lt;/span&gt; \$latex A = &lt;span class="caps"&gt;USV&lt;/span&gt;\^T\$,
we must have \$latex U = V\$. But this is exactly the eigendecomposition
of a symmetric matrix \$latex A\$. The eigendecomposition can be
computed cheaper than the &lt;span class="caps"&gt;SVD&lt;/span&gt; using Scipy &lt;code&gt;eigh&lt;/code&gt;, that uses &lt;span class="caps"&gt;LAPACK&lt;/span&gt;&amp;#8217;s
&lt;code&gt;*evr&lt;/code&gt;. As part of my GSoC this year, with help from &lt;a href="http://jakevdp.github.com/"&gt;Jake
VanderPlas&lt;/a&gt;, we made a &lt;a href="https://github.com/scipy/scipy/pull/289"&gt;pull request to Scipy&lt;/a&gt; containing a &lt;code&gt;pinvh&lt;/code&gt;
function that is equivalent to &lt;code&gt;pinv2&lt;/code&gt; but faster for symmetric&amp;nbsp;matrices.&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [24]: timeit linalg.pinv2(a)&lt;br /&gt;
1 loops, best of 3: 1.54 s per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [25]: timeit linalg.pinvh(a)&lt;br /&gt;
1 loops, best of 3: 621 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [26]: np.testing.assert_array_almost_equal(linalg.pinv2(a),
linalg.pinvh(a))&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/08/pseudoinverses-300x218.png" title="Pseudoinverses" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2012/08/pseudoinverses.png&lt;/p&gt;</summary><category term="inv"></category><category term="matrix inverse"></category><category term="numerical analysis"></category><category term="numerical methods"></category><category term="pinv"></category><category term="pinvh"></category><category term="positive semidefinite"></category><category term="pseudoinverse"></category><category term="symmetric"></category><category term="benchmarking"></category><category term="python"></category></entry><entry><title>The scikit-learn-speed ship has set sail! Faster than ever, with multi-step benchmarks!</title><link href="http://vene.ro/blog/the-scikit-learn-speed-ship-has-set-sail-faster-than-ever-with-multi-step-benchmarks.html" rel="alternate"></link><updated>2012-08-11T17:32:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-08-11:blog/the-scikit-learn-speed-ship-has-set-sail-faster-than-ever-with-multi-step-benchmarks.html</id><summary type="html">&lt;p&gt;I am pleased to announce that last night at 2:03 &lt;span class="caps"&gt;AM&lt;/span&gt;, the first fully
automated run of the scikit-learn-speed test suite has run on our
Jenkins instance! You can admire it at &lt;a href="http://jenkins-scikit-learn.github.com/scikit-learn-speed/"&gt;its temporary home&lt;/a&gt; for now.
As soon as we verify that everything is good, we will move this to the
official scikit-learn&amp;nbsp;page.&lt;/p&gt;
&lt;p&gt;I would like to take this opportunity to tell you about our latest
changeset. We made running the benchmark suite tons simpler by adding a
friendly Makefile. You can read more about its usage in the guide. But
by far, our coolest new toy&amp;nbsp;is:&lt;/p&gt;
&lt;h2&gt;Multi-step&amp;nbsp;benchmarks&lt;/h2&gt;
&lt;p&gt;A standard vbench benchmark has three units of code, represented as
strings: &lt;code&gt;code&lt;/code&gt;, &lt;code&gt;setup&lt;/code&gt; and &lt;code&gt;cleanup&lt;/code&gt;. With the original timeit-based
benchmarks, this means that for every run, the setup would be executed
once. Then, the main loop runs &lt;code&gt;repeat&lt;/code&gt; times, and within each
iteration, the &lt;code&gt;code&lt;/code&gt; is run &lt;code&gt;ncalls&lt;/code&gt; times. Then &lt;code&gt;cleanup&lt;/code&gt; happens, the
best time is returned, and everybody is&amp;nbsp;happy.&lt;/p&gt;
&lt;p&gt;In scikit-learn, most of our interesting objects go through a state
change called &lt;em&gt;fitting&lt;/em&gt;. This metaphor is right at home in the machine
learning field, where we separate the learning phase for the prediction
phase. The prediction step cannot be invoked on an object that hasn&amp;#8217;t
been&amp;nbsp;fitted.&lt;/p&gt;
&lt;p&gt;For some algorithms, one of these steps is trivial. A brute force
Nearest Neighbors classifier can be instantaneously fit, but prediction
takes a while. On the opposite end we have linear models, with tons of
complicated algorithms to fit them, but evaluation is a simple
matrix-vector product that Numpy handles&amp;nbsp;perfectly.&lt;/p&gt;
&lt;p&gt;But many of scikit-learn&amp;#8217;s estimators have both steps interesting. Let&amp;#8217;s
take Non-negative Matrix Factorization. It has three interesting
functions: The &lt;code&gt;fit&lt;/code&gt; that computes \$latex X = &lt;span class="caps"&gt;WH&lt;/span&gt; \$, the &lt;code&gt;transform&lt;/code&gt;
that computes a non-negative projection on the components learned in
&lt;code&gt;fit&lt;/code&gt;, and &lt;code&gt;fit_transform&lt;/code&gt; that takes advantage of the observation that
when fitting, we also get the transformed \$latex X \$ for&amp;nbsp;free.&lt;/p&gt;
&lt;p&gt;When benchmarking &lt;span class="caps"&gt;NMF&lt;/span&gt;, we initially had to design 3&amp;nbsp;benchmarks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;setup =&lt;/code&gt;standard, &lt;code&gt;code = obj.fit(X)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setup =&lt;/code&gt;standard, &lt;code&gt;code = obj.fit_transform(X)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setup =&lt;/code&gt;standard&lt;code&gt;+ obj.fit(X)&lt;/code&gt;, &lt;code&gt;code = obj.transform(X)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;How much time were we&amp;nbsp;wasting?&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s say it takes 10 seconds. For every benchmark, we time the code by
running it 3 times. We run it once more to measure memory usage, once
more for &lt;code&gt;cProfile&lt;/code&gt; and one last time for &lt;code&gt;line_profiler&lt;/code&gt;. This is a
total of 6 times per benchmark. We need to multiply this by 2 again for
running on two datasets. So when benchmarking &lt;code&gt;NMF&lt;/code&gt;, because we need to
fit before predicting, we do it 12 extra times. If a fit takes 5
seconds, this means one minute wasted on benchmarking just one
estimator. &lt;em&gt;Wouldn&amp;#8217;t it be nice to &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;fit_transform&lt;/code&gt; and
&lt;code&gt;transform&lt;/code&gt; in a&amp;nbsp;sequence?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Behind the&amp;nbsp;scenes&lt;/h2&gt;
&lt;p&gt;We made the &lt;code&gt;PythonBenchmark code&lt;/code&gt; parameter also support getting a
sequence of strings, instead of just a string. On the database side,
every benchmark result entry gets an extra component in the primary key,
the number of the step it&amp;nbsp;measures.&lt;/p&gt;
&lt;p&gt;In the benchmark description files, nothing is&amp;nbsp;changed:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
{&lt;br /&gt;
&amp;#8216;obj&amp;#8217;: &amp;#8216;&lt;span class="caps"&gt;NMF&lt;/span&gt;&amp;#8217;,&lt;br /&gt;
&amp;#8216;init_params&amp;#8217;: {&amp;#8216;n_components&amp;#8217;: 2},&lt;br /&gt;
&amp;#8216;datasets&amp;#8217;: (&amp;#8216;blobs&amp;#8217;,),&lt;br /&gt;
&amp;#8216;statements&amp;#8217;: (&amp;#8216;fit_unsup&amp;#8217;, &amp;#8216;transform_unsup&amp;#8217;, &amp;#8216;fit_transform&amp;#8217;)&lt;br /&gt;
},&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;But before, we would take the cartesian product of datasets and
statements, and build a &lt;code&gt;Benchmark&lt;/code&gt; object for every pairing. Now, we
just pass the tuple as it is, and vbench is smart enough to do the right
thing.&lt;br /&gt;
We avoided the extra calls to &lt;code&gt;fit&lt;/code&gt; in a lot of benchmarks. The whole
suite now takes almost half the time to&amp;nbsp;run!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; This trick is currently hosted in the
&lt;code&gt;abstract_multistep_benchmarks&lt;/code&gt; vbench branch in my&amp;nbsp;fork.&lt;/p&gt;</summary><category term="multi-step"></category><category term="multistep"></category><category term="vbench"></category><category term="benchmarking"></category><category term="python"></category><category term="scikit-learn"></category></entry><entry><title>Profiler output, benchmark standard deviation and other goodies in scikit-learn-speed</title><link href="http://vene.ro/blog/profiler-output-benchmark-standard-deviation-and-other-goodies-in-scikit-learn-speed.html" rel="alternate"></link><updated>2012-07-27T11:01:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-07-27:blog/profiler-output-benchmark-standard-deviation-and-other-goodies-in-scikit-learn-speed.html</id><summary type="html">&lt;p&gt;This post is about the &lt;a href="http://scikit-learn.org"&gt;scikit-learn&lt;/a&gt;benchmarking project that I am
working on, called &lt;a href="https://github.com/vene/scikit-learn-speed"&gt;scikit-learn-speed&lt;/a&gt;. This is a continuous
benchmarking suite that runs and generates &lt;span class="caps"&gt;HTML&lt;/span&gt; reports using Wes
McKinney&amp;#8217;s &lt;a href="http://wesmckinney.com/blog/?p=373"&gt;vbench&lt;/a&gt; framework, to which I had to make some (useful, I
hope)&amp;nbsp;additions.&lt;/p&gt;
&lt;h2&gt;What it looks like&amp;nbsp;now&lt;/h2&gt;
&lt;p&gt;You can check out a &lt;a href="http://vene.github.com/scikit-learn-speed"&gt;teaser/demo&lt;/a&gt; that was run on equidistant releases
from the last two months. What has changed since the last version?
Here&amp;#8217;s a list in order of&amp;nbsp;obviousness:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We now use the lovely scikit-learn&amp;nbsp;theme&lt;/li&gt;
&lt;li&gt;Timing graphs now show the ±1 standard deviation&amp;nbsp;range&lt;/li&gt;
&lt;li&gt;cProfile output is displayed for all the benchmarks, so we can
    easily see at a glance what&amp;#8217;s&amp;nbsp;up&lt;/li&gt;
&lt;li&gt;Said profiler output is collapsible using &lt;a href="http://www.jqueryui.com/demos/accordion/"&gt;JQueryUI&amp;nbsp;goodness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;There now is an improved &lt;a href="http://vene.github.com/scikit-learn-speed/quick_start.html"&gt;Quick Start guide&lt;/a&gt; to running vbench on
    your&amp;nbsp;machine&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;What made this&amp;nbsp;possible&lt;/h2&gt;
&lt;p&gt;I have done some more refactoring in my vbench fork, because I didn&amp;#8217;t
want to have a huge, monolithic &lt;code&gt;Benchmark&lt;/code&gt; class that was specific to
what we want in scikit-learn-speed. So on this branch, I set up a
mixin/multiple inheritance hierarchy of benchmark&amp;nbsp;classes.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Benchmark&lt;/code&gt; class in vbench is now an abstract base class, with some
common functionality and structure.&lt;br /&gt;
Our &lt;code&gt;SklBenchmark&lt;/code&gt; class is defined in scikit-learn-speed&amp;nbsp;as:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;class SklBenchmark(CProfileBenchmarkMixin,  MemoryBenchmarkMixin, PythonBenchmark):&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s read this from right to&amp;nbsp;left:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PythonBenchmark&lt;/code&gt;: This class stores &lt;code&gt;code&lt;/code&gt;, &lt;code&gt;setup&lt;/code&gt; and &lt;code&gt;cleanup&lt;/code&gt;
    Python code as strings, and implements simple timing mechanisms
    using the &lt;code&gt;time&lt;/code&gt; module.&lt;/li&gt;
&lt;li&gt;Bonus: &lt;code&gt;TimeitBenchmark&lt;/code&gt;: This class extends &lt;code&gt;PythonBenchmark&lt;/code&gt; with
    the &lt;code&gt;timeit&lt;/code&gt; micro-benchmark timing method previously used in
    vbench. We turned this off in&amp;nbsp;scikit-learn-speed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MemoryBenchmarkMixin&lt;/code&gt;: This adds memory benchmarking using&amp;nbsp;[memory_profiler][].&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CProfileBenchmarkMixin&lt;/code&gt;: This runs the code through &lt;a href="http://docs.python.org/library/profile.html#module-cProfile"&gt;cProfile&lt;/a&gt;
    and implements mechanisms to report the output.&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The database is not flexible enough to adapt to arbitrary benchmark
structure right now, so if anybody would like to help the effort, it
would be very&amp;nbsp;appreciated.&lt;/p&gt;</summary><category term="gsoc"></category><category term="memory_profiler"></category><category term="scikit-learn-speed"></category><category term="vbench"></category><category term="benchmarking"></category><category term="python"></category><category term="scikit-learn"></category></entry><entry><title>Scikit-learn-speed HTML reports teaser</title><link href="http://vene.ro/blog/scikit-learn-speed-html-reports-teaser.html" rel="alternate"></link><updated>2012-07-20T14:40:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-07-20:blog/scikit-learn-speed-html-reports-teaser.html</id><summary type="html">&lt;p&gt;&lt;span class="caps"&gt;EDIT&lt;/span&gt;: I made the plots a little more readable, check it&amp;nbsp;out!&lt;/p&gt;
&lt;p&gt;Last time, I teased you with a screenshot of local output. Now, I will
tease you with the benchmarks run on a couple of recent commits, along
with some from earlier this&amp;nbsp;year.&lt;/p&gt;
&lt;p&gt;After some effort and bugfixes, the project now reliably runs on
different machines, so the next step to host it on a remote server and
invoke it daily is getting closer. In the mean time, you can have a look
at &lt;a href="http://vene.github.com/scikit-learn-speed/" title="scikit-learn-speed"&gt;the sample output&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that just last time, the plots look jagged but the differences are
mostly minor and significant conclusions cannot be drawn yet, but as the
suite will start running daily, the plots will become much more
meaningful. I could waste time running the suite on more previous
commits, but the results wouldn&amp;#8217;t be comparable with the ones from the
deployed system, because of hardware&amp;nbsp;differences.&lt;/p&gt;
&lt;p&gt;Playing around with this makes me want a couple of features in vbench.
One is the possibility to overlay related benchmarks on the same plot
(for example, different parameters for the same algorithm and data):
this could be useful to spot patterns. A second one is some query /
sorting support: see what are the most expensive benchmarks, see what
benchmarks show the biggest jump in performance (but this could become a
historical wall of fame or&amp;nbsp;shame).&lt;/p&gt;</summary><category term="gsoc"></category><category term="scikit-learn-speed"></category><category term="vbench"></category><category term="benchmarking"></category><category term="python"></category><category term="scikit-learn"></category></entry><entry><title>Memory benchmarking with vbench</title><link href="http://vene.ro/blog/memory-benchmarking-with-vbench.html" rel="alternate"></link><updated>2012-07-05T12:38:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-07-05:blog/memory-benchmarking-with-vbench.html</id><summary type="html">&lt;p&gt;The &lt;a href="https://github.com/vene/scikit-learn-speed"&gt;scikit-learn-speed project&lt;/a&gt; now has memory usage&amp;nbsp;benchmarking!&lt;/p&gt;
&lt;p&gt;This was accomplished by building on what I described in my recent
posts, specifically the extensions to Fabian&amp;#8217;s [memory_profiler][] that
you can find in &lt;a href="https://github.com/vene/memory_profiler"&gt;my fork&lt;/a&gt;, but they will be merged upstream soon. The
key element is the &lt;code&gt;%magic_memit&lt;/code&gt; function whose development I blogged
about &lt;a href="http://localhost:8001/2012/06/30/quick-memory-usage-benchmarking-in-ipython/" title="Quick memory usage benchmarking in IPython"&gt;on&lt;/a&gt; &lt;a href="http://localhost:8001/2012/07/02/more-on-memory-benchmarking/" title="More on memory benchmarking"&gt;several&lt;/a&gt; &lt;a href="http://localhost:8001/2012/07/04/on-why-my-memit-fails-on-osx/" title="On why my %memit fails on OSX"&gt;occasions&lt;/a&gt;. I plugged this into &lt;a href="http://wesmckinney.com/blog/?p=373"&gt;vbench&lt;/a&gt;
in a similar way to how the timings are computed, all with great&amp;nbsp;success.&lt;/p&gt;
&lt;p&gt;Here is a screenshot of the way a simple benchmark looks now, with just
a few data&amp;nbsp;points.&lt;/p&gt;
&lt;p&gt;[caption id=&amp;#8221;attachment_464&amp;#8221; align=&amp;#8221;aligncenter&amp;#8221; width=&amp;#8221;600&amp;#8221;][![A
screenshot showing generated output from the scikit-learn-speed project,
illustrating memory usage benchmarking.][]][] Memory benchmarking in
scikit-learn-speed powered by&amp;nbsp;vbench.[/caption]&lt;/p&gt;
&lt;p&gt;You can check it out and use it yourself for your benchmarks, but you
need to use the vbench from the &lt;a href="https://github.com/vene/vbench/tree/memory"&gt;memory branch on my fork&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course, there are some important caveats. I am running this on my
laptop, which runs &lt;span class="caps"&gt;OS&lt;/span&gt; X Lion, so, under the effect of &lt;a href="http://localhost:8001/2012/07/04/on-why-my-memit-fails-on-osx/" title="On why my %memit fails on OSX"&gt;this
bug&lt;/a&gt;, I hardcoded the &amp;#8216;&lt;code&gt;-i&lt;/code&gt;&amp;#8216; so the memory benchmarks are not
realistic. Also, the y-range should probably be forced wider, because
the plots look erratic, showing the very small noise at a&amp;nbsp;large-scale.&lt;/p&gt;
&lt;p&gt;[A screenshot showing generated output from the scikit-learn-speed
  project, illustrating memory usage benchmarking.]: http://localhost:8001/wp-content/uploads/2012/07/vbench1.png
    &amp;#8220;Memory benchmarking in scikit-learn-speed powered by vbench.&amp;#8221;
  [![A screenshot showing generated output from the scikit-learn-speed
  project, illustrating memory usage benchmarking.][]]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2012/07/vbench1.png&lt;/p&gt;</summary><category term="memit"></category><category term="memory"></category><category term="vbench"></category><category term="python"></category><category term="scikit-learn"></category></entry><entry><title>On why my %memit fails on OSX</title><link href="http://vene.ro/blog/on-why-my-memit-fails-on-osx.html" rel="alternate"></link><updated>2012-07-04T12:49:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-07-04:blog/on-why-my-memit-fails-on-osx.html</id><summary type="html">&lt;p&gt;In my &lt;a href="http://localhost:8001/2012/07/02/more-on-memory-benchmarking/" title="More on memory benchmarking"&gt;last post&lt;/a&gt; I mentioned that I&amp;#8217;m not satisfied with the current
state of &lt;code&gt;%memit&lt;/code&gt;, because some more complicated numerical function
calls make it crash. I will start this post with a reminder of a pretty
important&amp;nbsp;bug:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[On MacOS X (10.7 but maybe more), after forking a new process, there
is a segfault in Grand Central Dispatch on the &lt;span class="caps"&gt;BLAS&lt;/span&gt; &lt;span class="caps"&gt;DGEMM&lt;/span&gt; function from
Accelerate.][]&lt;br /&gt;
&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;EDIT&lt;/span&gt; 1:&lt;/strong&gt; In a hurry, I forgot to mention how &lt;a href="http://twitter.com/ogrisel/"&gt;Olivier Grisel&lt;/a&gt; and
&lt;a href="https://github.com/cournape"&gt;David Cournapeau&lt;/a&gt; spent some time narrowing down this issue, starting
from an &lt;a href="https://github.com/scikit-learn/scikit-learn/issues/636"&gt;odd testing bug in scikit-learn&lt;/a&gt;. They reported it to Apple,
but there was, as of the date of this post, no&amp;nbsp;reaction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;EDIT&lt;/span&gt; 2:&lt;/strong&gt; MinRK &lt;a href="https://twitter.com/minrk/status/228265246819774464" title="Min's tweet"&gt;confirms&lt;/a&gt;, and I verified shortly after, that this
bug is fixed in Mountain Lion (10.8). Still not sure how far back it
goes, though, so feedback is&amp;nbsp;welcome.&lt;/p&gt;
&lt;p&gt;When I first tried to make the &lt;code&gt;%memit&lt;/code&gt; magic, I thought about simply
measuring the current memory, running the command, and measuring the
memory again. The problem is the results are not consistent, because
Python &lt;a href="http://effbot.org/pyfaq/why-doesnt-python-release-the-memory-when-i-delete-a-large-object.htm"&gt;tries to reuse already allocated memory whenever it can&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Using memory_profiler, here&amp;#8217;s an example illustrating this elastic
memory management:&lt;br /&gt;
[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
# mem_test.py&lt;br /&gt;
import numpy as&amp;nbsp;np&lt;/p&gt;
&lt;p&gt;def make_a_large_array():&lt;br /&gt;
return np.ones((1000,&amp;nbsp;1000))&lt;/p&gt;
&lt;p&gt;def main():&lt;br /&gt;
make_a_large_array()&lt;br /&gt;
make_a_large_array()&lt;br /&gt;&amp;nbsp;make_a_large_array()&lt;/p&gt;
&lt;p&gt;# in IPython:&lt;br /&gt;
In [1]: import&amp;nbsp;mem_test&lt;/p&gt;
&lt;p&gt;In [2]: %mprun -f mem_test.main mem_test.main()&lt;br /&gt;
Filename:&amp;nbsp;mem_test.py&lt;/p&gt;
&lt;h1&gt;Line # Mem usage Increment Line&amp;nbsp;Contents&lt;/h1&gt;
&lt;p&gt;8 24.8477 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0000 &lt;span class="caps"&gt;MB&lt;/span&gt; def main():&lt;br /&gt;
9 24.8633 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0156 &lt;span class="caps"&gt;MB&lt;/span&gt; make_a_large_array()&lt;br /&gt;
10 32.4688 &lt;span class="caps"&gt;MB&lt;/span&gt; 7.6055 &lt;span class="caps"&gt;MB&lt;/span&gt; make_a_large_array()&lt;br /&gt;
11 32.4688 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0000 &lt;span class="caps"&gt;MB&lt;/span&gt; make_a_large_array()&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;If this was in an IPython environment, and one would like to see how
much memory &lt;code&gt;make_a_large_array()&lt;/code&gt; uses, you could say we can simply run
it a few times and take the maximum. However, if you happened to
accidentally call &lt;code&gt;main()&lt;/code&gt; once before, you will no longer get a good&amp;nbsp;result:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [3]: %mprun -f mem_test.main mem_test.main()&lt;br /&gt;
Filename:&amp;nbsp;mem_test.py&lt;/p&gt;
&lt;h1&gt;Line # Mem usage Increment Line&amp;nbsp;Contents&lt;/h1&gt;
&lt;p&gt;8 32.4922 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0000 &lt;span class="caps"&gt;MB&lt;/span&gt; def main():&lt;br /&gt;
9 32.5234 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0312 &lt;span class="caps"&gt;MB&lt;/span&gt; make_a_large_array()&lt;br /&gt;
10 32.5234 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0000 &lt;span class="caps"&gt;MB&lt;/span&gt; make_a_large_array()&lt;br /&gt;
11 32.5234 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0000 &lt;span class="caps"&gt;MB&lt;/span&gt; make_a_large_array()&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;So how can we get consistent results for the memory usage of an
instruction? We could run it in a fresh, new process. I implemented this
in %memit and it&amp;nbsp;shows:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [5]: %memit mem_test.make_a_large_array()&lt;br /&gt;
maximum of 3: 8.039062 &lt;span class="caps"&gt;MB&lt;/span&gt; per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [6]: %memit mem_test.make_a_large_array()&lt;br /&gt;
maximum of 3: 8.035156 &lt;span class="caps"&gt;MB&lt;/span&gt; per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [7]: %memit mem_test.make_a_large_array()&lt;br /&gt;
maximum of 3: 8.042969 &lt;span class="caps"&gt;MB&lt;/span&gt; per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;This way you can also realistically benchmark&amp;nbsp;assignments:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [8]: %memit X = mem_test.make_a_large_array()&lt;br /&gt;
maximum of 3: 8.054688 &lt;span class="caps"&gt;MB&lt;/span&gt; per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [9]: %memit X = mem_test.make_a_large_array()&lt;br /&gt;
maximum of 3: 8.058594 &lt;span class="caps"&gt;MB&lt;/span&gt; per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [10]: %memit X = mem_test.make_a_large_array()&lt;br /&gt;
maximum of 3: 8.058594 &lt;span class="caps"&gt;MB&lt;/span&gt; per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;If we don&amp;#8217;t spawn a subprocess, &lt;code&gt;del&lt;/code&gt; doesn&amp;#8217;t help, but allocating new
variables does:&lt;br /&gt;
[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [11]: %memit -i X = mem_test.make_a_large_array()&lt;br /&gt;
maximum of 3: 7.632812 &lt;span class="caps"&gt;MB&lt;/span&gt; per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [12]: del&amp;nbsp;X&lt;/p&gt;
&lt;p&gt;In [13]: %memit -i X = mem_test.make_a_large_array()&lt;br /&gt;
maximum of 3: 0.000000 &lt;span class="caps"&gt;MB&lt;/span&gt; per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [14]: %memit -i Y = mem_test.make_a_large_array()&lt;br /&gt;
maximum of 3: 7.632812 &lt;span class="caps"&gt;MB&lt;/span&gt; per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [15]: %memit -i Z = mem_test.make_a_large_array()&lt;br /&gt;
maximum of 3: 7.632812 &lt;span class="caps"&gt;MB&lt;/span&gt; per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Now, the problem is that when the function that you are benchmarking
contains calls to &lt;code&gt;np.dot&lt;/code&gt; (matrix multiplication), the subprocess will
consistently fail with &lt;span class="caps"&gt;SIGSEGV&lt;/span&gt; on affected &lt;span class="caps"&gt;OS&lt;/span&gt; X systems. These are
actually pretty much all the functions that I intended &lt;code&gt;%memit&lt;/code&gt; for:
numerical applications. For that reason, I have made &lt;code&gt;%memit&lt;/code&gt; notify the
user when all subprocesses fail, and to suggest the usage of the &lt;code&gt;-i&lt;/code&gt;
flag.&lt;/p&gt;
&lt;p&gt;I think that, with this update, &lt;code&gt;%memit&lt;/code&gt; is flexible and usable enough
for actual use, and therefore for merging into&amp;nbsp;memory_profiler.&lt;/p&gt;
&lt;p&gt;[On MacOS X (10.7 but maybe more), after forking a new process, there
  is a segfault in Grand Central Dispatch on the &lt;span class="caps"&gt;BLAS&lt;/span&gt; &lt;span class="caps"&gt;DGEMM&lt;/span&gt; function
  from Accelerate.]:&amp;nbsp;https://gist.github.com/2027412&lt;/p&gt;</summary><category term="IPython"></category><category term="magic"></category><category term="memit"></category><category term="mprun"></category><category term="benchmarking"></category><category term="python"></category></entry><entry><title>More on memory benchmarking</title><link href="http://vene.ro/blog/more-on-memory-benchmarking.html" rel="alternate"></link><updated>2012-07-02T11:27:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-07-02:blog/more-on-memory-benchmarking.html</id><summary type="html">&lt;p&gt;Following up on my task to make it easier to benchmark memory usage in
Python, I updated Fabian&amp;#8217;s [memory_profiler][] to include a couple of
useful IPython magics. While in my &lt;a href="http://localhost:8001/2012/06/30/quick-memory-usage-benchmarking-in-ipython/" title="Quick memory usage benchmarking in IPython"&gt;last post&lt;/a&gt;, I used the new IPython
0.13 syntax for defining magics, this time I used the
backwards-compatible one from the previous&amp;nbsp;version.&lt;/p&gt;
&lt;p&gt;You can find this work-in-progress as a [pull request on
memory_profiler][] from where you can trace it to my GitHub repo.
Here&amp;#8217;s what you can do with&amp;nbsp;it:&lt;/p&gt;
&lt;h2&gt;%mprun&lt;/h2&gt;
&lt;p&gt;Copying the spirit of &lt;code&gt;%lprun&lt;/code&gt;, since imitation is the most sincere form
of flattery, you can use %mprun to easily view line-by-line memory usage
reports, without having to go in and add the &lt;code&gt;@profile&lt;/code&gt; decorator.&lt;/p&gt;
&lt;p&gt;For&amp;nbsp;example:&lt;/p&gt;
&lt;p&gt;[sourcecode&amp;nbsp;lang=&amp;#8221;python&amp;#8221;]&lt;/p&gt;
&lt;p&gt;In [1]: import numpy as&amp;nbsp;np&lt;/p&gt;
&lt;p&gt;In [2]: from sklearn.linear_model import&amp;nbsp;ridge_regression&lt;/p&gt;
&lt;p&gt;In [3]: X, y = np.array([[1, 2], [3, 4], [5, 6]]), np.array([2, 4,&amp;nbsp;6])&lt;/p&gt;
&lt;p&gt;In [4]: %mprun -f ridge_regression ridge_regression(X, y,&amp;nbsp;1.0)&lt;/p&gt;
&lt;p&gt;(&amp;#8230;)&lt;/p&gt;
&lt;p&gt;109 41.6406 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0000 &lt;span class="caps"&gt;MB&lt;/span&gt; if n_features &gt; n_samples or \&lt;br /&gt;
110 41.6406 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0000 &lt;span class="caps"&gt;MB&lt;/span&gt; isinstance(sample_weight, np.ndarray) or \&lt;br /&gt;
111 41.6406 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0000 &lt;span class="caps"&gt;MB&lt;/span&gt; sample_weight != 1.0:&lt;br /&gt;
112&lt;br /&gt;
113 # kernel ridge&lt;br /&gt;
114 # w = X.T * inv(X X\^t + alpha*Id) y&lt;br /&gt;
115 A = np.dot(X, X.T)&lt;br /&gt;
116 A.flat[::n_samples + 1] += alpha * sample_weight&lt;br /&gt;
117 coef = np.dot(X.T, _solve(A, y, solver, tol))&lt;br /&gt;
118 else:&lt;br /&gt;
119 # ridge&lt;br /&gt;
120 # w = inv(X\^t X + alpha*Id) * X.T y&lt;br /&gt;
121 41.6484 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0078 &lt;span class="caps"&gt;MB&lt;/span&gt; A = np.dot(X.T, X)&lt;br /&gt;
122 41.6875 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0391 &lt;span class="caps"&gt;MB&lt;/span&gt; A.flat[::n_features + 1] += alpha&lt;br /&gt;
123 41.7344 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0469 &lt;span class="caps"&gt;MB&lt;/span&gt; coef = _solve(A, np.dot(X.T, y), solver,
tol)&lt;br /&gt;
124&lt;br /&gt;
125 41.7344 &lt;span class="caps"&gt;MB&lt;/span&gt; 0.0000 &lt;span class="caps"&gt;MB&lt;/span&gt; return&amp;nbsp;coef.T&lt;/p&gt;
&lt;p&gt;[/sourcecode]&lt;/p&gt;
&lt;h2&gt;%memit&lt;/h2&gt;
&lt;p&gt;As described in my previous post, this is a &lt;code&gt;%timeit&lt;/code&gt;-like magic for
quickly seeing how much memory a Python command uses.&lt;br /&gt;
Unlike %timeit, however, the command needs to be executed in a fresh
process. I have to dig in some more to debug this, but if the command is
run in the current process, very often the difference in memory usage
will be insignificant, I assume because preallocated memory is used. The
problem is that when running in a new process, some functions that I
tried to bench crash with &lt;code&gt;SIGSEGV&lt;/code&gt;. For a lot of stuff, though,
&lt;code&gt;%memit&lt;/code&gt; is currently&amp;nbsp;usable:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [1]: import numpy as&amp;nbsp;np&lt;/p&gt;
&lt;p&gt;In [2]: X = np.ones((1000,&amp;nbsp;1000))&lt;/p&gt;
&lt;p&gt;In [3]: %memit X.T&lt;br /&gt;
worst of 3: 0.242188 &lt;span class="caps"&gt;MB&lt;/span&gt; per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [4]: %memit np.asfortranarray(X)&lt;br /&gt;
worst of 3: 15.687500 &lt;span class="caps"&gt;MB&lt;/span&gt; per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [5]: Y =&amp;nbsp;X.copy(&amp;#8216;F&amp;#8217;)&lt;/p&gt;
&lt;p&gt;In [6]: %memit np.asfortranarray(Y)&lt;br /&gt;
worst of 3: 0.324219 &lt;span class="caps"&gt;MB&lt;/span&gt; per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;It is very easy, using this small tool, to see what forces memory
copying and what does&amp;nbsp;not.&lt;/p&gt;
&lt;h2&gt;Installation&amp;nbsp;instructions&lt;/h2&gt;
&lt;p&gt;First, you have to get the source code of this version of
memory_profiler. Then, it depends on your version of IPython. If you
have 0.10, you have to edit &lt;code&gt;~/.ipython/ipy_user_conf.py&lt;/code&gt; like this:
(once again, instructions &lt;em&gt;borrowed&lt;/em&gt; from&amp;nbsp;[line_profiler][])&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
# These two lines are standard and probably already there.&lt;br /&gt;
import IPython.ipapi&lt;br /&gt;
ip =&amp;nbsp;IPython.ipapi.get()&lt;/p&gt;
&lt;p&gt;# These two are the important ones.&lt;br /&gt;
import memory_profiler&lt;br /&gt;
ip.expose_magic(&amp;#8216;mprun&amp;#8217;, memory_profiler.magic_mprun)&lt;br /&gt;
ip.expose_magic(&amp;#8216;memit&amp;#8217;, memory_profiler.magic_memit)&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;re using IPython 0.11 or newer, the steps are different. First
create a configuration profile:&lt;br /&gt;
[sourcecode lang=&amp;#8221;bash&amp;#8221;]&lt;br /&gt;
\$ ipython profile create&lt;br /&gt;
[/sourcecode]&lt;br /&gt;
Then create a file named &lt;code&gt;~/.ipython/extensions/memory_profiler_ext.py&lt;/code&gt;
with the following&amp;nbsp;content:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
import&amp;nbsp;memory_profiler&lt;/p&gt;
&lt;p&gt;def load_ipython_extension(ip):&lt;br /&gt;
ip.define_magic(&amp;#8216;mprun&amp;#8217;, memory_profiler.magic_mprun)&lt;br /&gt;
ip.define_magic(&amp;#8216;memit&amp;#8217;, memory_profiler.magic_memit)&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Then register it in &lt;code&gt;~/.ipython/profile_default/ipython_config.py&lt;/code&gt;, like
this. Of course, if you already have other extensions such as
&lt;code&gt;line_profiler_ext&lt;/code&gt;, just add the new one to the&amp;nbsp;list.&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
c.TerminalIPythonApp.extensions = [&lt;br /&gt;
&amp;#8216;memory_profiler_ext&amp;#8217;,&lt;br /&gt;
]&lt;br /&gt;
c.InteractiveShellApp.extensions = [&lt;br /&gt;
&amp;#8216;memory_profiler_ext&amp;#8217;,&lt;br /&gt;
]&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Now launch IPython and you can use the new magics like in the examples&amp;nbsp;above.&lt;/p&gt;</summary><category term="IPython"></category><category term="magic"></category><category term="memit"></category><category term="memory"></category><category term="memory_profiler"></category><category term="mprun"></category><category term="benchmarking"></category><category term="python"></category></entry><entry><title>Quick memory usage benchmarking in IPython</title><link href="http://vene.ro/blog/quick-memory-usage-benchmarking-in-ipython.html" rel="alternate"></link><updated>2012-06-30T08:53:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-06-30:blog/quick-memory-usage-benchmarking-in-ipython.html</id><summary type="html">&lt;p&gt;Everybody loves &lt;code&gt;%timeit&lt;/code&gt;, there&amp;#8217;s no doubt about it. So why not have
something like that, but for measuring how much memory your line takes?
Well, now you can; grab a hold of the script in the following gist and
run it like in the&amp;nbsp;example.&lt;/p&gt;
&lt;p&gt;[gist&amp;nbsp;id=3022718]&lt;/p&gt;
&lt;p&gt;Instead of taking care of the dirty process inspection stuff myself, I
decided to delegate this to Fabian&amp;#8217;s simple but very good
[&lt;code&gt;memory_profiler&lt;/code&gt;][]. There is also &lt;a href="http://guppy-pe.sourceforge.net/"&gt;Guppy&lt;/a&gt; available, but its design
seems a bit and overkill for this&amp;nbsp;task.&lt;/p&gt;
&lt;p&gt;Please contact me if you find problems with this implementation, this is
a preliminary, quick hack-y version.&amp;nbsp;:)&lt;/p&gt;</summary><category term="benchmark"></category><category term="IPython"></category><category term="magic"></category><category term="memory"></category><category term="memory_profiler"></category><category term="profiling"></category><category term="benchmarking"></category><category term="python"></category></entry><entry><title>Compiling and Installing GLARF and the bundled Charniak parser on MacOS X</title><link href="http://vene.ro/blog/compiling-and-installing-glarf-and-the-bundled-charniak-parser-on-macos-x.html" rel="alternate"></link><updated>2012-06-21T12:32:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-06-21:blog/compiling-and-installing-glarf-and-the-bundled-charniak-parser-on-macos-x.html</id><summary type="html">&lt;p&gt;It seems that I keep getting handed buggy code to install. These are
cases of research software where the developers didn&amp;#8217;t make the effort
to make sure their tool works on the platforms it&amp;nbsp;should.&lt;/p&gt;
&lt;p&gt;[&lt;span class="caps"&gt;GLARF&lt;/span&gt;][] (Grammatical and Logical Argument Representation Framework)
is, in their words, &amp;#8220;a typed feature structure framework for
representing regularizations of parse trees&amp;#8221;. It is a processing
pipeline from &lt;span class="caps"&gt;NYU&lt;/span&gt; with rich output including many types of structure in
the given text. However, it is clearly a case of software whose
maintenance was abandoned when it &amp;#8220;worked&amp;#8221; for them. The whole install
and run procedure is pretty messy, but at least it&amp;#8217;s documented. The
problem is, following it step by step doesn&amp;#8217;t work on my MacBook. As
usual, I needed to hack through it a&amp;nbsp;bit.&lt;/p&gt;
&lt;p&gt;The Charniak parser distributed with &lt;span class="caps"&gt;GLARF&lt;/span&gt; has now been superseded by
the [&lt;span class="caps"&gt;BLLIP&lt;/span&gt; parser][]. The new one is tricky to compile as well, but I
have yet to see if it plugs into &lt;span class="caps"&gt;GLARF&lt;/span&gt;, so I leave this for a future&amp;nbsp;post.&lt;/p&gt;
&lt;p&gt;Here are the steps I needed to take to make &lt;span class="caps"&gt;GLARF&lt;/span&gt;&amp;nbsp;work:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
Download and unzip the &lt;span class="caps"&gt;GLARF&lt;/span&gt;&amp;nbsp;package.

&lt;/li&gt;
&lt;li&gt;
Make sure you have `sbcl` in your path, and your `perl` is in (or linked
from)&amp;nbsp;`/usr/bin/perl`

&lt;/li&gt;
&lt;li&gt;
Set the environment variables. I like to make a shell script to set them
so I don&amp;#8217;t have to do it every time. So I write something looking like
this:

[sourcecode lang=&amp;#8221;bash&amp;#8221;]  
\# glarf\_env.sh

export &lt;span class="caps"&gt;GLARF&lt;/span&gt;=/Users/vene/code/&lt;span class="caps"&gt;GLARF&lt;/span&gt;  
export &lt;span class="caps"&gt;GLARF&lt;/span&gt;\_JET=\${&lt;span class="caps"&gt;GLARF&lt;/span&gt;}/&lt;span class="caps"&gt;JET&lt;/span&gt;  
export &lt;span class="caps"&gt;PATH&lt;/span&gt;=\$&lt;span class="caps"&gt;PATH&lt;/span&gt;:.  
[/sourcecode]

&lt;p&gt;
Then for every session when I want to use &lt;span class="caps"&gt;GLARF&lt;/span&gt;, I do
`source&amp;nbsp;glarf_env.sh`.

&lt;/li&gt;
&lt;li&gt;
Compile &lt;span class="caps"&gt;GLARF&lt;/span&gt; by running `$&lt;span class="caps"&gt;GLARF&lt;/span&gt;/commands-2010/compile-glarf`. *Note:*
this only compiles the pipeline lisp&amp;nbsp;code.

&lt;/li&gt;
&lt;li&gt;
Now, according to their instructions, you&amp;#8217;re done. However, if you try
to run it, you&amp;#8217;d notice the output is incomplete. (It goes through the
named entity extraction part, but it doesn&amp;#8217;t run the parser.) The reason
for this is that they distribute the Charniak parser with a precompiled
binary that runs on Linux, but not on the Mac. So we need to recompile
it. So go to `$&lt;span class="caps"&gt;GLARF&lt;/span&gt;/charniak-parser-2005/parser05Aug16-static/&lt;span class="caps"&gt;PARSE2&lt;/span&gt; `,
run `make clean` and roll up your&amp;nbsp;sleeves.

&lt;/li&gt;
&lt;li&gt;
Obviously, simply running `make` doesn&amp;#8217;t work. [As documented by Pawel
Mazur][], we need to edit `BchartSm.C` to add the line
`#include&amp;nbsp;&amp;#8220;GotIter.h&amp;#8221;`

&lt;/li&gt;
&lt;li&gt;
On my system this still isn&amp;#8217;t enough, and I get some linker errors. By
poking through the Makefile, I noticed I could fix it by commenting out
the 5th line:&amp;nbsp;`&lt;span class="caps"&gt;LDFLAGS&lt;/span&gt;=-static`.

&lt;/li&gt;
&lt;li&gt;
Now run make and watch it work, hurrah!&amp;nbsp;&amp;#92;o/

&lt;/li&gt;
&lt;li&gt;
To see if &lt;span class="caps"&gt;GLARF&lt;/span&gt; itself works now, go to
`$&lt;span class="caps"&gt;GLARF&lt;/span&gt;/commands-2010/run-glarf/` and run
`make-all-glarf-a sample-files-a N`. You should get beautiful, beautiful
&lt;span class="caps"&gt;GLARF&lt;/span&gt; output&amp;nbsp;files.

&lt;/li&gt;
&lt;/ul&gt;
Phew, now that was quite an effort!

  [&lt;span class="caps"&gt;GLARF&lt;/span&gt;]: http://nlp.cs.nyu.edu/meyers/&lt;span class="caps"&gt;GLARF&lt;/span&gt;.html
  [&lt;span class="caps"&gt;BLLIP&lt;/span&gt; parser]: https://github.com/&lt;span class="caps"&gt;BLLIP&lt;/span&gt;/bllip-parser
  [As documented by Pawel Mazur]:&amp;nbsp;http://web.science.mq.edu.au/~mpawel/resources/notes/compilingCharniakJohnson.htm</summary><category term="bllip"></category><category term="charniak"></category><category term="glarf"></category><category term="installation"></category><category term="parser"></category><category term="nlp"></category></entry><entry><title>Compiling MegaM on MacOS X</title><link href="http://vene.ro/blog/compiling-megam-on-macos-x.html" rel="alternate"></link><updated>2012-06-08T11:45:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-06-08:blog/compiling-megam-on-macos-x.html</id><summary type="html">&lt;p&gt;&lt;a href="http://hal3.name/megam"&gt;MegaM&lt;/a&gt; is Hal Daumé &lt;span class="caps"&gt;III&lt;/span&gt;&amp;#8217;s maxent (logistic regression, and much more)
modeling software written in OCaml. It is feature-packed and seems to be
used a lot, despite being slightly dated. &lt;a href="http://nltk.org" title="Natural Language Toolkit"&gt;&lt;span class="caps"&gt;NLTK&lt;/span&gt;&lt;/a&gt; is able to use&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;In order to compile it as of 2012, with the current version of OCaml, I
had to do some tricks that I would like to document here. It&amp;#8217;s no big
deal but it could save somebody precious&amp;nbsp;minutes.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download and unpack the gzip&amp;nbsp;archive.&lt;/li&gt;
&lt;li&gt;Install ocaml using macports: &lt;code&gt;sudo port install ocaml&lt;/code&gt;. &lt;em&gt;Note:&lt;/em&gt;
    this installed version 3.12.1_5, &lt;span class="caps"&gt;YMMV&lt;/span&gt; with newer versions&amp;nbsp;later.&lt;/li&gt;
&lt;li&gt;Point the compiler to the correct headers. First run &lt;code&gt;ocamlc -where&lt;/code&gt;
    to find out the correct path. On my system it&amp;#8217;s
    &lt;code&gt;/opt/local/lib/ocaml/caml&lt;/code&gt;. Change the &lt;code&gt;WITHCLIBS&lt;/code&gt; line (#73) in
    the Makefile to point&amp;nbsp;there.&lt;/li&gt;
&lt;li&gt;As of OCaml 3.12.0, the &lt;code&gt;-lstr&lt;/code&gt; compiler flag should be replaced
    with &lt;code&gt;-lcamlstr&lt;/code&gt;. It occurs on line #62 within the definition of
    &lt;code&gt;WITHSTR&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;make&lt;/code&gt; or &lt;code&gt;make opt&lt;/code&gt; and&amp;nbsp;enjoy.&lt;/li&gt;
&lt;/ol&gt;</summary><category term="compile"></category><category term="install"></category><category term="maxent"></category><category term="megam"></category><category term="nlp"></category></entry><entry><title>Dynamically generated benchmarks with vbench</title><link href="http://vene.ro/blog/dynamically-generated-benchmarks-with-vbench.html" rel="alternate"></link><updated>2012-06-07T01:57:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-06-07:blog/dynamically-generated-benchmarks-with-vbench.html</id><summary type="html">&lt;p&gt;To construct a &lt;code&gt;vbench&lt;/code&gt; benchmark you need a setup string and a code
string. The constructor&amp;#8217;s signature&amp;nbsp;is:  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;Benchmark(self, code, setup, ncalls=None, repeat=3, cleanup=None, name=None, description=None, start_date=None, logy=False)&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Why generate benchmarks&amp;nbsp;dynamically?&lt;/h2&gt;
&lt;p&gt;For most &lt;code&gt;scikit-learn&lt;/code&gt; purposes, the &lt;code&gt;code&lt;/code&gt; string will be very close
to &lt;code&gt;"algorithm.fit(X, y)"&lt;/code&gt;, &lt;code&gt;"algorithm.transform(X)"&lt;/code&gt; or
&lt;code&gt;"algorithm.predict(X)"&lt;/code&gt;. We can generate a lot of benchmarks by
changing what the algorithm is, and changing what the data is or the way
it is&amp;nbsp;generated.&lt;/p&gt;
&lt;p&gt;A possible idea would be to create a
&lt;abbr title="domain-specific language" lang="en"&gt;&lt;span class="caps"&gt;DSL&lt;/span&gt;&lt;/abbr&gt; in which to
specify scikit-learn tests and create benchmarks from them. However,
before engineering such a solution, I wanted to test out how to generate
three related benchmarks using different arguments for the dataset
generation&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;This is what I came up&amp;nbsp;with:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
from vbench.benchmark import&amp;nbsp;Benchmark&lt;/p&gt;
&lt;p&gt;_setup = &amp;#8220;&amp;#8221;&amp;#8220;&lt;br /&gt;
from deps import&amp;nbsp;*&lt;/p&gt;
&lt;p&gt;kwargs = %s&lt;br /&gt;
X, y = make_regression(random_state=0, **kwargs)&lt;br /&gt;
lr = LinearRegression()&lt;br /&gt;&amp;nbsp;&amp;#8221;&amp;#8220;&amp;#8221;&lt;/p&gt;
&lt;p&gt;_configurations = [&lt;br /&gt;
(&amp;#8216;linear_regression_many_samples&amp;#8217;,&lt;br /&gt;
{&amp;#8216;n_samples&amp;#8217;: 10000, &amp;#8216;n_features&amp;#8217;: 100}),&lt;br /&gt;
(&amp;#8216;linear_regression_many_features&amp;#8217;,&lt;br /&gt;
{&amp;#8216;n_samples&amp;#8217;: 100, &amp;#8216;n_features&amp;#8217;: 10000}),&lt;br /&gt;
(&amp;#8216;linear_regression_many_targets&amp;#8217;,&lt;br /&gt;
{&amp;#8216;n_samples&amp;#8217;: 1000, &amp;#8216;n_features&amp;#8217;: 100, &amp;#8216;n_targets&amp;#8217;: 100})&lt;br /&gt;&amp;nbsp;]&lt;/p&gt;
&lt;p&gt;_statement = &amp;#8220;lr.fit(X,&amp;nbsp;y)&amp;#8221;&lt;/p&gt;
&lt;p&gt;_globs = globals()&lt;br /&gt;
_globs.update({name: Benchmark(_statement, _setup % str(kwargs),
name=name)&lt;br /&gt;
for name, kwargs in&amp;nbsp;_configurations})&lt;/p&gt;
&lt;p&gt;[/sourcecode]&lt;/p&gt;
&lt;p&gt;It works perfectly, but I don&amp;#8217;t like having to hack the globals to make
the benchmarks detectable. This is because of the way the vbench suite
gathers benchmarks. In &lt;code&gt;__init__.py&lt;/code&gt; we have to do
&lt;code&gt;from linear_regression import *&lt;/code&gt;. With a small update to the detection
method, we could replace the hacky part with a public lists of Benchmark&amp;nbsp;objects.&lt;/p&gt;
&lt;h2&gt;Exposed&amp;nbsp;issues&lt;/h2&gt;
&lt;p&gt;While working on this, after my first attempt, I was surprised to see
that there were no results added to the database, and output plots were
empty. It turns out that the generated benchmarks weren&amp;#8217;t running, even
though if I copied and pasted their source code from the generated html,
it would run. Vbench was not issuing any sort of message to let me know
that anything was&amp;nbsp;wrong.&lt;/p&gt;
&lt;p&gt;So what was the problem? My fault, of course, whitespace. But in all
fairness, we should add better&amp;nbsp;feedback.&lt;/p&gt;
&lt;p&gt;This is what I was doing to generate the setup&amp;nbsp;string:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
def _make_setup(kwargs):&lt;br /&gt;
return &amp;#8220;&amp;#8221;&amp;#8220;&lt;br /&gt;
from deps import&amp;nbsp;*&lt;/p&gt;
&lt;p&gt;kwargs = %s&lt;br /&gt;
X, y = make_regression(random_state=0, **kwargs)&lt;br /&gt;
lr = LinearRegression()&lt;br /&gt;
&amp;#8220;&amp;#8221;&amp;#8221; % str(kwargs)&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s clear as daylight now that I overzealously indented the multiline
string. But man, was it hard to debug! Also, in this example, the bug
led to a refactoring that made the whole thing nicer and more direct.
Hopefully, my experience with vbench will lead to some improvements to
this cool and highly useful piece of&amp;nbsp;software.&lt;/p&gt;</summary><category term="gsoc"></category><category term="vbench"></category><category term="benchmarking"></category><category term="python"></category></entry><entry><title>First contact with vbench</title><link href="http://vene.ro/blog/first-contact-with-vbench.html" rel="alternate"></link><updated>2012-05-29T12:57:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-05-29:blog/first-contact-with-vbench.html</id><summary type="html">&lt;p&gt;With a slight delay caused by going to lovely lovely Istanbul for the
&lt;span class="caps"&gt;LREC&lt;/span&gt; conference where I presented a &lt;a href="http://vene.ro/papers/lrec12-poster.pdf"&gt;poster&lt;/a&gt;, I am back to work on the
Google Summer of Code project. By the way, this year&amp;#8217;s logo and swag
looks a lot nicer than last year&amp;#8217;s, thank you Google!&lt;br /&gt;
[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/05/P5280194-300x225.jpg" title="GSoC swag" /&gt;][]&lt;br /&gt;
The backbone of my GSoC consists of putting together a continuous
benchmark platform. I took a good look at &lt;a href="https://github.com/pydata/vbench"&gt;vbench&lt;/a&gt; and spent an
evening hacking Wes&amp;#8217;s benchmarks suite config into something that will
run on my machine. These are the key points I got from this&amp;nbsp;experience.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;vbench is, at least for the moment, very specific to [Wes&amp;#8217; and
    Pandas&amp;#8217; needs][]. This is also because there weren&amp;#8217;t so many other
    users that could have brought&amp;nbsp;contributions.&lt;/li&gt;
&lt;li&gt;Even though it has support for some configuration and automation,
    vbench seems largely suited for running on a local machine.
    Specifically, it is &lt;span class="caps"&gt;NOT&lt;/span&gt; designed to run continuously but in one-off
    runs, going back in git history and getting the last commit for each
    day, and running the benchmark with it. Of course, it is trivial to
    patch it into getting just one&amp;nbsp;commit.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;code-as-strings&lt;/em&gt; approach is not ideal. The first thought is
    that it should be replaced with reading &lt;code&gt;.py&lt;/code&gt; files into strings,
    but there are two issues with this:&lt;ol&gt;
&lt;li&gt;One benchmark file can have a lot of setup code and several key
    lines that need to actually be benched. This can be fixed using
    convensions (ie. setup functions and &lt;code&gt;bench_*&lt;/code&gt; functions) in the
    spirit of testing suites, or using&amp;nbsp;decorators.&lt;/li&gt;
&lt;li&gt;I would like to be able to run bench files as python scripts,
    but the vbench import system breaks this. This can be fixed by
    hijacking the imports when reading the&amp;nbsp;file.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our project has different dynamics than Pandas, so it&amp;#8217;s important that
the published results run on an independent machine, but it would be
great if an individual developer can run the benchmark himself while
coding but before pushing his changes upstream. Of course, his numbers
would only be comparable to the numbers he gets on his own machine
before his changes, but a developer shouldn&amp;#8217;t wait for the daily
benchmark for knowing if he made an&amp;nbsp;improvement.&lt;/p&gt;
&lt;p&gt;On the other hand there is &lt;a href="http://code.google.com/p/unladen-swallow/"&gt;unladen-swallow&lt;/a&gt;&amp;#8216;s &lt;a href="http://code.google.com/p/unladen-swallow/wiki/Benchmarks"&gt;benchmark system&lt;/a&gt;
using the [&lt;code&gt;perf.py&lt;/code&gt;][] file. I didn&amp;#8217;t try it out yet, so I would like
feedback, but there are some key things that can be taken from&amp;nbsp;them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Memory usage&amp;nbsp;benchmarking&lt;/li&gt;
&lt;li&gt;Python scripts as benchmarks, with a simple but efficient Benchmark
    object&amp;nbsp;hierarchy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What&amp;#8217;s missing&amp;nbsp;is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A system to remember previous results and compare them, similar to
    vbench&amp;#8217;s&amp;nbsp;database&lt;/li&gt;
&lt;li&gt;The ability to bench only an area of the code without rerunning the
    setup. (Not really sure whether vbench&amp;#8217;s way is actually&amp;nbsp;better)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At a first glance, it seems that a very good system can be obtained by
combining these two excellent projects (or rather, improving vbench with
features from &lt;code&gt;perf.py&lt;/code&gt;). While I continue exploring this, I would like
to hear feedback from people who had to do with similar issues. As for
the GSoC timeline, I plan to join forces with Immanuel and design a
solid benchmark suite for the linear models over the next 2&amp;nbsp;weeks.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/05/P5280194-300x225.jpg" title="GSoC swag" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2012/05/P5280194.jpg&lt;/p&gt;</summary><category term="benchmarks"></category><category term="perf.py"></category><category term="performance"></category><category term="vbench"></category><category term="scikit-learn"></category></entry><entry><title>Support vector regression on Anscombe’s dataset</title><link href="http://vene.ro/blog/support-vector-regression-on-anscombes-dataset.html" rel="alternate"></link><updated>2012-05-27T21:59:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-05-27:blog/support-vector-regression-on-anscombes-dataset.html</id><summary type="html">&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Anscombe's_quartet" title="Anscombe's quartet"&gt;Anscombe&amp;#8217;s quartet&lt;/a&gt; is a set of four toy datasets that look very
different, but many of their statistics coincide. They were developed by
Francis Anscombe as a striking visual to show that even for small
datasets, blindly examining their statistical properties without
considering their structure can&amp;nbsp;mislead.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Anscombe's datasets" src="http://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/640px-Anscombe%27s_quartet_3.svg.png" /&gt;&lt;/p&gt;
&lt;p&gt;Particularly, the four datasets have the same &lt;a href="http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares" title="Ordinary least squares regression"&gt;least squares regression
line&lt;/a&gt;. While the second dataset is a clear example of a nonlinear
correlation which cannot be accurately captured by any linear model, the
third dataset is actually perfectly linear, with no noise, but just a
single outlier that shifts the regression line&amp;nbsp;considerably.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://scikit-learn.org/stable/modules/svm.html#regression" title="Support vector regression"&gt;Support vector regression&lt;/a&gt; is an extension of the support vector
machine idea to tackle the regression problem. It is based on the
observation that a &lt;span class="caps"&gt;SVM&lt;/span&gt; classifier builds its decision boundary as a
function of a (small) subset of training points. For regression, &lt;span class="caps"&gt;SVR&lt;/span&gt;
fits a &lt;em&gt;tube&lt;/em&gt; that is robust to noise within a width
[latex]\epsilon[/latex]. For this particular example, using a small
width makes the solution robust to the obvious outlier. For very small
but non-zero [latex]\epsilon[/latex], the solution is a combination of
the outlier and on two other points. For [latex]\epsilon=0[/latex], you
can see that every point except a non-outlier is highlighted. This is
actually the perfect solution but very&amp;nbsp;dense.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Support vector regression on Anscombe's third dataset with no
noise" src="" /&gt;][]&lt;/p&gt;
&lt;p&gt;Every frame displays the global mean squared error and the true mean
squared error, &lt;em&gt;i.e.&lt;/em&gt; over the inlying points. If the epsilon size is
well chosen, &lt;span class="caps"&gt;SVR&lt;/span&gt; can perform robustly with a sparse solution. Since our
interest was in avoiding the outlier, we assumed no noise in the inlying
data, so a very small epsilon is perfect. For real data a larger epsilon
is often useful because of variability in the data. When adding noise,
&lt;span class="caps"&gt;SVR&lt;/span&gt; still manages to avoid the outlier, but when the tube width becomes
zero, the solution is again very dense, very&amp;nbsp;non-parametric.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Support vector regression on Anscombe's third dataset with
noise" src="http://localhost:8001/wp-content/uploads/2012/05/svr_noise.gif" title="SVR on Anscombe's dataset with noise" /&gt;][]&lt;/p&gt;
&lt;p&gt;Here is the code you can use to play around with&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;[gist&amp;nbsp;id=2815589]&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nl"&gt;http:&lt;/span&gt;&lt;span class="c1"&gt;//localhost:8001/wp-content/uploads/2012/05/svr.gif&lt;/span&gt;
&lt;span class="s"&gt;&amp;quot;&lt;span class="caps"&gt;SVR&lt;/span&gt; on Anscombe&amp;#39;s dataset with no noise&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;[![Support vector regression on Anscombe&amp;#8217;s third dataset with no
  noise][]]: http://localhost:8001/wp-content/uploads/2012/05/svr.gif
  [![Support vector regression on Anscombe&amp;#8217;s third dataset with
  noise][]]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2012/05/svr_noise.gif&lt;/p&gt;</summary><category term="anscombe"></category><category term="outlier"></category><category term="robust regression"></category><category term="support vector regression"></category><category term="svm"></category><category term="svr"></category><category term="python"></category><category term="scikit-learn"></category></entry><entry><title>GSoC 2012 proposal: Need for scikit-learn speed</title><link href="http://vene.ro/blog/gsoc-2012-proposal-need-for-scikit-learn-speed.html" rel="alternate"></link><updated>2012-04-16T00:37:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-04-16:blog/gsoc-2012-proposal-need-for-scikit-learn-speed.html</id><summary type="html">&lt;p&gt;This summer I hope to be able to put in another full-time amount of
effort into scikit-learn. After a successful Google Summer of Code
project last year on dictionary learning, I now plan to do some
low-level work. The title of my proposal is: &amp;#8220;Need for scikit-learn
speed&amp;#8221; and, in a nutshell, will make the scikit go faster and will help
it stay that&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;Scikit-learn has always enforced standards of quality that kept all
implementations at a non-trivial level (i.e. faster than using &lt;a href="http://docs.scipy.org/doc/scipy/reference/optimize.html"&gt;the
generic optimizers in scipy&lt;/a&gt;). However, not all modules are equal:
some have received more attention for speed than others (for example the
&lt;span class="caps"&gt;SGD&lt;/span&gt; classes). I intend to raise the bar towards a more uniform&amp;nbsp;level.&lt;/p&gt;
&lt;h2&gt;Are you crazy, can you really do&amp;nbsp;this?&lt;/h2&gt;
&lt;p&gt;Well, of course. This might not the usual GSoC proposal, but I can show
how I can do it and how it&amp;#8217;s easily quantifiable. Actually, a very
important part of the work will be to make scikit-learn&amp;#8217;s speed easily&amp;nbsp;measurable.&lt;/p&gt;
&lt;p&gt;As for the specific speed-ups, I have shown &lt;a href="http://localhost:8001/2011/08/07/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1/" title="Optimizing Orthogonal Matching Pursuit code in Numpy, part 1"&gt;in&lt;/a&gt; &lt;a href="http://localhost:8001/2011/08/11/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2/" title="Optimizing Orthogonal Matching Pursuit code in Numpy, part 2"&gt;the&lt;/a&gt; &lt;a href="http://localhost:8001/2011/11/18/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code/" title="The nasty bug crawling in my Orthogonal Matching Pursuit code"&gt;past&lt;/a&gt; that
I can do algorithmic and memory layout optimizations in numerical code.
There are parts in the scikit-learn that can benefit from such work: for
example only recently Peter merged this &lt;a href="https://github.com/scikit-learn/scikit-learn/pull/545"&gt;pull request&lt;/a&gt; significantly
improving SGDClassifier&amp;#8217;s test time performance by switching the memory
layout of the coefficients: they were laid out optimally for the
training phase, not for the prediction&amp;nbsp;phase.&lt;/p&gt;
&lt;p&gt;There are certainly more opportunities for such speed improvements in
the scikit. Of course there is a lot of code that can&amp;#8217;t reasonably be
made any faster (I have a feeling that SGDClassifier is at the moment
such a case, but we can&amp;#8217;t know for sure without heavy profiling). But
generally there are many speed fixes that could weigh a lot: for
example, a &lt;a href="http://cython.org/"&gt;Cython&lt;/a&gt; implementation of the &lt;code&gt;euclidean_distances&lt;/code&gt;
function that is able to use preallocated memory will improve the
performance of raw NearestNeighbours queries as well as of the KMeans
and hierarchical clustering&amp;nbsp;algorithms.&lt;/p&gt;
&lt;h2&gt;How will we be able to tell if you&amp;nbsp;succeed?&lt;/h2&gt;
&lt;p&gt;A key part of the GSoC project is setting up a
&lt;abbr title="Continuous Integration"&gt;&lt;span class="caps"&gt;CI&lt;/span&gt;&lt;/abbr&gt;-style benchmark platform.
The point is to be able to track how the speed of certain operations
evolves in time. For such purposes, Wes McKinney developed the
&lt;a href="https://github.com/pydata/vbench"&gt;vbench&lt;/a&gt; project, introduced in &lt;a href="http://wesmckinney.com/blog/?p=373"&gt;this blog post&lt;/a&gt;. The goal is for
every scikit-learn module to have several such benchmarks, for
differently shaped and structured&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;Having such a benchmark suite available is the equivalent of a test
suite, in terms of performance. It makes developers be extra conscious
of the effect of their changes. It also makes it more fun to chase speed
improvements, thanks to the positive reinforcement it&amp;nbsp;gives.&lt;/p&gt;
&lt;p&gt;There are some static benchmarks comparing the performance of
scikit-learn algorithms with other well-known libraries in the
&lt;a href="http://scikit-learn.sourceforge.net/ml-benchmarks/"&gt;ml-benchmarks&lt;/a&gt; project. It would be very helpful to have such a
benchmark suite that automatically keeps&amp;nbsp;up-to-date.&lt;/p&gt;
&lt;h2&gt;Side&amp;nbsp;effects&lt;/h2&gt;
&lt;p&gt;The cool thing about such a project is that it should raise the overall
quality of the scikit. The refactoring will lead to an increase in test
coverage, because the low-coverage modules are expected to be less
optimized as well. Also, the benchmarks will lead to well-backed
summaries in the documentation, such as &lt;a href="http://scikit-learn.org/dev/modules/clustering.html#overview-of-clustering-methods"&gt;the one recently added in the
clustering section&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since the scikit is reaching a state where many well-known algorithms
are available, the &lt;strong&gt;1.0&lt;/strong&gt; release is slowly approaching. My Google
Summer of Code project should bring the scikit significantly closer to
that&amp;nbsp;milestone.&lt;/p&gt;</summary><category term="gsoc"></category><category term="proposal"></category><category term="scikit-learn"></category></entry><entry><title>Romanian people and coffee</title><link href="http://vene.ro/blog/romanian-people-and-coffee.html" rel="alternate"></link><updated>2012-04-13T21:20:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-04-13:blog/romanian-people-and-coffee.html</id><summary type="html">&lt;p&gt;So I got my hands of the &lt;a href="http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html" title="Google Research"&gt;Google N-gram data&lt;/a&gt; for the Romanian
language. It&amp;#8217;s noisy as hell, has some other subtle issues too, but
here&amp;#8217;s the first thing I&amp;nbsp;noticed:&lt;/p&gt;
&lt;p&gt;The Romanian word for coffee is &lt;em&gt;cafea&lt;/em&gt;, and the more you crave it, the
longer you pronunce the final &lt;em&gt;a&lt;/em&gt;: I really need some &lt;em&gt;cafeaaaa&lt;/em&gt; right&amp;nbsp;now.&lt;/p&gt;
&lt;p&gt;Thanks to Google, here are the&amp;nbsp;numbers:&lt;/p&gt;
&lt;p&gt;[![Distribution of the length of the final letter in the Romanian word
for&amp;nbsp;coffee.][]][]&lt;/p&gt;
&lt;p&gt;Post scriptum: I hope you like the theme: I installed the &lt;a href="http://www.huyng.com/posts/sane-color-scheme-for-matplotlib/" title="www.huyng.com/posts/sane-color-scheme-for-matplotlib"&gt;sane
matplotlib color scheme&lt;/a&gt; from Huy&amp;nbsp;Nguyen.&lt;/p&gt;
&lt;p&gt;[Distribution of the length of the final letter in the Romanian word
  for coffee.]: http://localhost:8001/wp-content/uploads/2012/04/cafeaaa-300x218.png
    &amp;#8220;cafeaaa&amp;#8221;
  [![Distribution of the length of the final letter in the Romanian word
  for coffee.][]]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2012/04/cafeaaa.png&lt;/p&gt;</summary><category term="Uncategorized"></category><category term="coffee"></category><category term="ngram"></category></entry><entry><title>Nash-Williams theorem on the Hamiltonian property of some regular graphs</title><link href="http://vene.ro/blog/nash-williams-theorem-on-the-hamiltonian-property-of-some-regular-graphs.html" rel="alternate"></link><updated>2012-01-29T22:31:00+01:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-01-29:blog/nash-williams-theorem-on-the-hamiltonian-property-of-some-regular-graphs.html</id><summary type="html">&lt;p&gt;I have been digging on the internet for the proof of this theorem for
the last couple of days without success. The result was published by Sir
Crispin Nash-Williams as &lt;em&gt;Valency Sequences which force graphs to have
Hamiltonian Circuits&lt;/em&gt;. Interim Rep, University of Waterloo Res Rep.,
Waterloo, Ontario, 1969. However, this old paper is unavailable online
but I have a proof in some lecture notes from my class, that I want to
share&amp;nbsp;here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let [latex]G=(V, E)[/latex] be an [latex]n[/latex]-regular
graph with [latex]|V| = 2n + 1[/latex]. Then, [latex]G[/latex] is&amp;nbsp;Hamiltonian.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We first remark that [latex]n[/latex] must be even, since
\$\$\sum_{x \in V} d(x) = n(2n + 1) = 2|E|\$\$ We might try to apply
Dirac&amp;#8217;s theorem, which would give us a Hamiltonian cycle if [latex]
\forall x \in V, d(x) \geq \frac{|V|}{2}[/latex]. But in the current
case, [latex]\forall x \in V, d(x) = n \&amp;lt;&amp;nbsp;\frac{2n+1}{2}[/latex].&lt;/p&gt;
&lt;p&gt;So we force Dirac by adding an extra vertex [latex]w[/latex] and
connecting it to all [latex] x \in V [/latex]. In this new graph
[latex]G&amp;#8217;[/latex], [latex]d(x) = n + 1 \forall x \in V[/latex] and
[latex]d(w) = 2n + 1[/latex]. Therefore we have a Hamiltonian cycle that
passes through [latex]w[/latex] and in which, [latex]w[/latex] is
adjacent to two vertices [latex]x[/latex] and [latex]y \in V[/latex].
Therefore this cycle induces a Hamiltonian path in [latex]G[/latex]:
\$\$P = [x = v_0, v_1, &amp;#8230;, v_{2n-1}, v_{2n}=y]&amp;nbsp;\$\$&lt;/p&gt;
&lt;p&gt;Suppose that [latex]G[/latex] is not Hamiltonian. It follows that if
[latex] v_0v_i \in E [/latex], then [latex] v_{i-1}v_{2n} \notin
E[/latex] and also that if [latex] v_0v_i \notin E [/latex], then
[latex] v_{i-1}v_{2n} \in&amp;nbsp;E[/latex].&lt;/p&gt;
&lt;p&gt;We have two cases. If [latex]v_0[/latex] is adjacent to [latex]v_1,
&amp;#8230;, v_n[/latex] then it follows that [latex]v_{2n}[/latex] is
adjacent to [latex]v_n, v_{n+1}, &amp;#8230;, v_{2n-1}[/latex], since it
cannot be adjacent to any [latex]v_i, i \&amp;lt; n[/latex] without creating a
Hamiltonian cycle. But in this case, in the graph induced by the first
half [latex]G[\{v_0, v_1, &amp;#8230; v_n\}][/latex], [latex]v_n[/latex]
cannot be adjacent to all the others, since in [latex]G[/latex] it has
degree [latex]n[/latex] and it already has [latex]2[/latex] outgoing
edges. So there is at least one vertex [latex]v_i, i \&amp;lt; n[/latex] that
isn&amp;#8217;t adjacent to it, which means [latex]v_i[/latex] is adjacent to
some [latex]v_j, j &gt; n[/latex], thus forming a Hamiltonian&amp;nbsp;cycle.&lt;/p&gt;
&lt;p&gt;In the second case, we have a vertex [latex]v_i, 2 \leq i \leq 2n -
1[/latex] such that [latex]v_0v_i \notin E[/latex] and
[latex]v_0v_{i+1} \in E[/latex]. This also means that
[latex]v_{i-1}v_{2n} \in&amp;nbsp;E[/latex].&lt;/p&gt;
&lt;p&gt;We therefore have a cycle of length [latex]2n[/latex] in
[latex]G[/latex] that excludes [latex]v_i[/latex]. Let&amp;#8217;s rename this
cycle [latex]C=[y_1, y_2, &amp;#8230;, y_{2n}, y_1][/latex] and&amp;nbsp;[latex]v_i=y_0[/latex].&lt;/p&gt;
&lt;p&gt;[latex]y_0[/latex] cannot be adjacent to two consecutive vertices
[latex]y_i[/latex] and [latex]y_{i+1}[/latex] because this will give a
Hamiltonian cycle. But we know that [latex]deg(y_0) = n[/latex]. It
follows that it&amp;#8217;s adjacent to all of the even or odd numbered vertices.
We assume the latter, without loss of generality. Let [latex]2k[/latex]
be some even index. Notice that we have [latex]\{y_0y_{2k-1},
y_0y_{2k+1}\} \subset E[/latex] and we can follow the cycle
[latex]C[/latex] from [latex]y_{2k+1}[/latex] all the way back to
[latex]y_{2n-1}[/latex] giving us a new cycle [latex]C&amp;#8217; = [y_1, y_2,
&amp;#8230;, y_{2n-1}, y_0, y_{2k+1}, &amp;#8230;, y_{2n}, y_1][/latex] also of
length [latex]2n[/latex]. So by repeating the same reasoning for every
even vertex, by placing it in the middle and building a cycle around it,
it follows that every even vertex is adjacent to all the odd vertices.
But there are [latex]n+1[/latex] even indices, so it follows that the
degree of any odd vertex is at least [latex]n+1[/latex], contradicting
the initial conditions of the theorem.&amp;nbsp;[latex]\square[/latex]&lt;/p&gt;</summary><category term="graph"></category><category term="graph theory"></category><category term="hamiltonian"></category><category term="nash-williams"></category><category term="Uncategorized"></category></entry><entry><title>Moving out</title><link href="http://vene.ro/blog/moving-out.html" rel="alternate"></link><updated>2012-01-06T00:16:00+01:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-01-06:blog/moving-out.html</id><summary type="html">&lt;p&gt;Happy new year,&amp;nbsp;friends!&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve made a New Year&amp;#8217;s resolution to build a better web presence and
make better use of the domain that I previously only used for&amp;nbsp;mail.&lt;/p&gt;
&lt;p&gt;This has prompted me to move my blog over to http://localhost:8001 which
hopefully is shorter, better, faster and&amp;nbsp;stronger!&lt;/p&gt;</summary><category term="Uncategorized"></category></entry><entry><title>The nasty bug crawling in my Orthogonal Matching Pursuit code</title><link href="http://vene.ro/blog/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code.html" rel="alternate"></link><updated>2011-11-18T20:51:00+01:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-11-18:blog/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code.html</id><summary type="html">&lt;p&gt;A while back, Bob L. Sturm blogged about a &lt;a href="http://media.aau.dk/null_space_pursuits/2011/10/efficient-omp.html"&gt;similar implementation of
&lt;span class="caps"&gt;OMP&lt;/span&gt;&lt;/a&gt; to the one in scikit-learn. Instead of using the Cholesky
decomposition like we did, his Matlab code uses the &lt;span class="caps"&gt;QR&lt;/span&gt; decomposition, to
a similar (or maybe even identical) outcome, in theory. So lucky that
Alejandro pointed out to him the existence of the scikit-learn
implementation, and that Bob&amp;#8217;s code &lt;a href="http://media.aau.dk/null_space_pursuits/2011/10/omp-in-python-strange-results.html"&gt;exposed a bug&lt;/a&gt; that all the test
coverage didn&amp;#8217;t catch! This plot should increase, certainly not
decrease! Something is clearly wrong here.&lt;br /&gt;
&lt;a href="http://media.aau.dk/null_space_pursuits/2011/10/omp-in-python-strange-results.html"&gt;&lt;img alt="OMP buggy phase transition, decreasing instead of
increasing" src="http://media.aau.dk/null_space_pursuits/2011/10/17/OMPscikit.png" title="OMP buggy phase transition" /&gt;&lt;/a&gt;&lt;br /&gt;
Luckily we were able to find it and &lt;a href="http://media.aau.dk/null_space_pursuits/2011/10/to-the-rescue.html"&gt;fix it&lt;/a&gt; very quickly. I have
updated the old entries I wrote on the &lt;span class="caps"&gt;OMP&lt;/span&gt; optimizations, so they no
longer include the bug. But I take this opportunity to explain what
exactly went&amp;nbsp;wrong.&lt;/p&gt;
&lt;p&gt;A key part of the optimization was that slicing out arbitrary columns
out of an array is slow when they are passed to &lt;span class="caps"&gt;BLAS&lt;/span&gt; functions like
matrix multiplication. In order to make the most out of your code, the
data should have a contiguous layout. We achieved this by swapping
active dictionary atoms (columns) to the beginning of the&amp;nbsp;array.&lt;/p&gt;
&lt;p&gt;Something that can happen, but won&amp;#8217;t happen very often, is that after an
atom is selected as active, the atom that takes its place after swapping
needs to be selected. This is rare because dictionaries have many
columns, out of which only very very few will be active. But when it
happens, because the code didn&amp;#8217;t keep track of swapped indices, the
corresponding coefficient of the solution would get updated twice,
leading to more zero entries than we should have. A keen eye could have
noticed that the first `n_nonzero_coefs` entries in &lt;span class="caps"&gt;OMP&lt;/span&gt; solution
vectors were never non-zero. But alas, my eye was not a keen one at&amp;nbsp;all.&lt;/p&gt;
&lt;p&gt;In other words, the following test (that was written after the bug was
found, unfortunately) was failing:&lt;br /&gt;
[sourcecode lang=&amp;#8221;Python&amp;#8221;]&lt;br /&gt;
def test_swapped_regressors():&lt;br /&gt;
gamma = np.zeros(n_features)&lt;br /&gt;
# X[:, 21] should be selected first, then X[:, 0] selected second,&lt;br /&gt;
# which will take X[:, 21]&amp;#8217;s place in case the algorithm does&lt;br /&gt;
# column swapping for optimization (which is the case at the moment)&lt;br /&gt;
gamma[21] = 1.0&lt;br /&gt;
gamma[0] = 0.5&lt;br /&gt;
new_y = np.dot(X, gamma)&lt;br /&gt;
new_Xy = np.dot(X.T, new_y)&lt;br /&gt;
gamma_hat = orthogonal_mp(X, new_y, 2)&lt;br /&gt;
gamma_hat_gram = orthogonal_mp_gram(G, new_Xy, 2)&lt;br /&gt;
# active indices should be [0, 21], but prior to the bugfix&lt;br /&gt;
# the algorithm would update only [21] but twice&lt;br /&gt;
assert_equal(np.flatnonzero(gamma_hat), [0, 21])&lt;br /&gt;
assert_equal(np.flatnonzero(gamma_hat_gram), [0, 21])&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Note that this bug has been fixed for a while, but I didn&amp;#8217;t get the free
time to write this post until now. Good news is: we fixed it, and did so
very quickly after the report. So you can still trust me, I&amp;nbsp;guess!&lt;/p&gt;</summary><category term="bug"></category><category term="omp"></category><category term="orthogonal matching pursuit"></category><category term="dictionary learning"></category><category term="scikit-learn"></category></entry><entry><title>Sampling Gamma random variates through the ratio-of-uniforms method</title><link href="http://vene.ro/blog/sampling-gamma-random-variates-through-the-ratio-of-uniforms-method.html" rel="alternate"></link><updated>2011-10-09T15:40:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-10-09:blog/sampling-gamma-random-variates-through-the-ratio-of-uniforms-method.html</id><summary type="html">&lt;p&gt;One year ago I had the chance to take a class on Monte Carlo simulation
with prof. Ion Văduva, and my assignment for the class was to implement
exactly what it says in the title of the blog post. I am going to walk
you through the idea behind&amp;nbsp;this.&lt;/p&gt;
&lt;h3&gt;General&amp;nbsp;formulation&lt;/h3&gt;
&lt;p&gt;The ratio-of-uniforms is a method that can be applied to many density
functions. Essentially, given a density function over [latex]
\mathbb{R}\^m[/latex], [latex] f(x) = \frac{h(x)}{H}[/latex] where
[latex]H[/latex] is a normalization constant (ie. [latex] h(x) \geq
0[/latex], [latex] H = \int h(x)dX[/latex]). Given a parameter [latex]
c &gt; 0 [/latex] and a parametrization [latex]\phi[/latex] from [latex]
\mathbb{R}\^{m+1}[/latex] to [latex] \mathbb{R}\^{m}[/latex] expressed
as: \$\$ \phi(v_0, v_1, &amp;#8230;, v_m) = \left ( \frac{v_1}{v_0\^c},
\frac{v_2}{v_0\^c}, &amp;#8230;, \frac{v_m}{v_0\^c} \right )\$\$&lt;br /&gt;
Define the set [latex] \mathcal{C} = \{\mathbf{v} \big |
\gamma(\mathbf{v}) \leq 0, v_0 &gt; 0\} \in
\mathbb{R}\^{m+1}[/latex] where&lt;br /&gt;
\$\$\gamma(\mathbf{v}) = \log v_0 - \frac{\log h(\phi(v_0,
v_1, &amp;#8230;, v_m))}{mc + 1}\$\$ If [latex] \mathcal{C}[/latex] is
bounded and we sample a uniform vector [latex] \mathbf{V} \sim
\text{Uniform}(\mathcal{C})[/latex] then [latex] \phi(\mathbf{V})
\sim f(x)[/latex]. Also note that the measure (volume) of the set
[latex] \mathcal{C}[/latex] is [latex] \frac{H}{mc + 1}[/latex]. I do
not have any references for the proof, except for a book in Romanian,
but if you are interested, just leave me a comment and I&amp;#8217;ll do a
follow-up post with the&amp;nbsp;proofs.&lt;/p&gt;
&lt;h3&gt;Univariate&amp;nbsp;scenario&lt;/h3&gt;
&lt;p&gt;For the univariate case, all the above simplifies to \$\$ \mathcal{C} =
\left \{(u, v) \Big | 0 \&amp;lt; u \&amp;lt; \sqrt&lt;br /&gt;
{h\left (\frac{v}{u\^c}\right )} \right \} \$\$. We generate
[latex] (U, V) \sim \text{Uniform}(\mathcal{C})[/latex] and take
[latex] \frac{V}{U\^c} \sim f(x)[/latex].&lt;br /&gt;
Since we are looking at the (univariate) Gamma distribution, described
by: \$\$ f(x; \nu, \theta) = \frac{x\^{\mu - 1}
\exp(\frac{-x}{\theta})}{\theta\^k\Gamma(k)}\$\$ [latex]
\nu[/latex] is the shape parameter and [latex] \theta[/latex] is the
scale parameter.&lt;br /&gt;
But because of the property that if [latex] X \sim \text{Gamma}(\nu,
\theta)[/latex], then for any [latex] k &gt; 0[/latex], [latex] kX \sim
\text{Gamma}(\nu, k\theta)[/latex], we conclude that we can fix
[latex] \theta[/latex] to 1 without loss of generality. Replacing in
the style of the definition in the previous section, we have [latex]
h(x; \nu) = x\^{\nu-1}e\^{-x}[/latex] and [latex] H_\nu =
\Gamma(\nu)[/latex].&lt;br /&gt;
This allows us to compute the equation of the boundary of the set
[latex] \mathcal{C}[/latex] which ends up being described by
[latex]\gamma(u, v) = \log{u} - \frac{\nu - 1}{c + 1}
\log{\left(\frac{v}{u\^c}\right)} + \frac{1}{c+1}
\frac{v}{u\^c}[/latex]. For visualisation purposes, here is how it
would look like for [latex] \nu=6, c=1[/latex] (plotted using &lt;a href="http://www.wolframalpha.com/" title="Wolfram Alpha"&gt;Wolfram
Alpha&lt;/a&gt;):[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2011/10/regiunea.png" title="The accepting set C" /&gt;][]&lt;/p&gt;
&lt;h3&gt;Sampling&amp;nbsp;algorithm&lt;/h3&gt;
&lt;p&gt;In order to uniformly sample from this set, we can apply basic rejection
sampling: just uniformly sample from a rectangular region surrounding
the set, and reject the points that do not satisfy the condition. In
order to do this as efficiently as possible, we need to compute the
minimal bounding box, which can be done by solving a couple of
optimization problems using Lagrange multipliers and the &lt;span class="caps"&gt;KKT&lt;/span&gt; conditions.
Also by looking closely at the image, you can see that the lower left
corner is exactly the origin: this turns out not to be a coincidence. I
won&amp;#8217;t go into detail here, but here are the bounds I derived:&lt;br /&gt;
\$\$ 0 \&amp;lt; u \&amp;lt; (\nu - 1)\^\frac{\nu - 1}{c + 1} e \^ {-\frac{\nu -
1}{c + 1}} \text{ and } 0\&amp;lt; v \&amp;lt; \left(\frac{c\nu +
1}{c}\right)\^{\frac{c\nu + 1}{c + 1}} e \^ {- \frac {c\nu +&amp;nbsp;1}{c+1}}\$\$&lt;/p&gt;
&lt;p&gt;The probability of acceptance (which can be seen as the efficiency) of
the rejection sampling method is given by the ratio of the areas of the
set [latex] \mathcal{C}[/latex] and the bounding box. The larger this
probability, the less points we throw away and the more efficient the
algorithm is. Using the values derived above, this probability is: \$\$
p(\nu, c) = \frac{\Gamma(\nu)e\^{\nu}}{(c+1) (\nu -
1)\^{\frac{\nu - 1}{c + 1}} \left(\frac{c\nu +
1}{c}\right)\^{\frac{c\nu + 1}{c +&amp;nbsp;1}}}\$\$&lt;/p&gt;
&lt;p&gt;Personally I got stumped here. The idea would be to determine the ideal
[latex] c[/latex] for a given [latex] \nu[/latex] in order to maximize
the probability, but I didn&amp;#8217;t manage to do it (I leave it as an exercise
for the reader ;)). Anyway, this is enough to proceed with an
implementation, so I&amp;#8217;m gonna give the Python code for it. Note that I
used the name k for the shape parameter instead of [latex] \nu[/latex].
Also note that the case when [latex] 0 \&amp;lt; \nu \&amp;lt; 1[/latex] needed to be
treated separately, which I did using the following property: Let
[latex] \nu \in (0, 1)[/latex]. If [latex] X&amp;#8217; \sim
\text{Gamma}(1+\nu, 1), U \sim \text{Uniform}(0, 1)[/latex] then
\$\$ X = X&amp;#8217; \cdot \sqrt[\nu]{U} \sim \text{Gamma}(\nu, 1)\$\$ For
a proof of this fact, see [&lt;a href="#footnote-1"&gt;1&lt;/a&gt;], which is a great article on
generating Gamma&amp;nbsp;variates.&lt;/p&gt;
&lt;h3&gt;Implementation&lt;/h3&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
from import numpy as&amp;nbsp;np&lt;/p&gt;
&lt;p&gt;def _cond(u, v, k, c):&lt;br /&gt;
&amp;#8220;&amp;#8221;&amp;#8220;Identity function describing the acceptance region&amp;#8221;&amp;#8220;&amp;#8221;&lt;br /&gt;
x = v / u ** c&lt;br /&gt;
return (c + 1) * np.log(u) \&amp;lt;= (k - 1) * np.log(x) -&amp;nbsp;x&lt;/p&gt;
&lt;p&gt;def vn_standard_gamma(k, c=1.0, rng=np.random):&lt;br /&gt;
&amp;#8220;&amp;#8221;&amp;#8220;Generates a single standard gamma random variate&amp;#8221;&amp;#8220;&amp;#8221;&lt;br /&gt;
if k \&amp;lt;= 0:&lt;br /&gt;
raise ValueError(&amp;#8220;Gamma shape should be positive&amp;#8221;)&lt;br /&gt;
elif k \&amp;lt; 1:&lt;br /&gt;
return vn_standard_gamma(1 + k, c, rng) * rng.uniform() ** (1 /
k)&lt;br /&gt;
elif k == 1:&lt;br /&gt;
return rng.standard_exponential()&lt;br /&gt;
else:&lt;br /&gt;
a, b = get_bounds(k, c)&lt;br /&gt;
while True:&lt;br /&gt;
u, v = rng.uniform(0, a), rng.uniform(0, b)&lt;br /&gt;
if _cond(u, v, k, c):&lt;br /&gt;
break;&lt;br /&gt;
return v / u **&amp;nbsp;c&lt;/p&gt;
&lt;p&gt;def vn_gamma(k, t, shape=1, c=1.0, rng=np.random):&lt;br /&gt;
&amp;#8220;&amp;#8221;&amp;#8220;Vectorized function to generate multiple gamma variates&amp;#8221;&amp;#8220;&amp;#8221;&lt;br /&gt;
generator = lambda x: t * vn_standard_gamma(k, c, rng)&lt;br /&gt;
generator = np.vectorize(generator)&lt;br /&gt;
return&amp;nbsp;generator(np.empty(shape))&lt;/p&gt;
&lt;p&gt;def get_bounds(k, c=1.0):&lt;br /&gt;
&amp;#8220;&amp;#8221;&amp;#8220;Computes the minimal upper bounds surrounding the acceptance
region&amp;#8221;&amp;#8220;&amp;#8221;&lt;br /&gt;
a = ((k - 1) / np.e) ** ((k - 1) / (c + 1))&lt;br /&gt;
b = ((c * k + 1) / (c * np.e)) ** ((c * k + 1) / (c + 1))&lt;br /&gt;
return a,&amp;nbsp;b&lt;/p&gt;
&lt;p&gt;def prob_acc(k, c=1.0):&lt;br /&gt;
&amp;#8220;&amp;#8221;&amp;#8220;Calculates the probability of acceptance for the given
parameters&amp;#8221;&amp;#8220;&amp;#8221;&lt;br /&gt;
from scipy.special import gamma&lt;br /&gt;
a, b = get_bounds(k, c)&lt;br /&gt;
return gamma(k) / ((c + 1) * a * b)&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;And of course I should show you that it works. Here are some histograms
for various values of [latex] \nu[/latex], with the theoretical density
plotted in dotted red, after sampling [latex] 10\^5[/latex] values. The
y-axis is the frequency (sorry for labeling in Romanian), and for the
red dotted line it can be interpreted as the theoretical probability.
You can clearly see the goodness of fit is&amp;nbsp;excellent.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2011/10/hist1.png" title="Histogram for nu=6" /&gt;][][&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2011/10/hist2.png" title="Histogram for nu=100" /&gt;][][&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2011/10/hist3.png" title="Histogram for nu=0.66" /&gt;][]&lt;/p&gt;
&lt;p&gt;&lt;span id="footnote-1"&gt;&lt;a href="#footnote-1"&gt;1&lt;/a&gt;&lt;/span&gt;: George Marsaglia and Wai Wan Tsang.
1998. &lt;a href="http://www.jstatsoft.org/v03/i03/paper"&gt;The Monty Python method for generating random variables&lt;/a&gt;. &lt;span class="caps"&gt;ACM&lt;/span&gt;
Trans. Math. Softw. 24, 3 (September 1998), 341-350.
&lt;a href="http://doi.acm.org/10.1145/292395.292453"&gt;&lt;span class="caps"&gt;DOI&lt;/span&gt;=10.1145/292395.292453&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2011/10/regiunea.png" title="The accepting set C" /&gt;]: http://localhost:8001/wp-content/uploads/2011/10/regiunea.png
  [&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2011/10/hist1.png" title="Histogram for nu=6" /&gt;]: http://localhost:8001/wp-content/uploads/2011/10/hist1.png
  [&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2011/10/hist2.png" title="Histogram for nu=100" /&gt;]: http://localhost:8001/wp-content/uploads/2011/10/hist2.png
  [&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2011/10/hist3.png" title="Histogram for nu=0.66" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/10/hist3.png&lt;/p&gt;</summary><category term="monte carlo"></category><category term="numpy"></category><category term="random sampling"></category><category term="ratio-of-uniforms"></category><category term="scipy"></category><category term="python"></category></entry><entry><title>RANLP 2011 in Hissar, BG</title><link href="http://vene.ro/blog/ranlp-2011-in-hissar-bg.html" rel="alternate"></link><updated>2011-09-20T14:17:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-09-20:blog/ranlp-2011-in-hissar-bg.html</id><summary type="html">&lt;p&gt;Last week was marked by the international &lt;span class="caps"&gt;RANLP&lt;/span&gt; (Recent Advances in
Natural Language Processing) conference, taking place in a nice spa in
Hissar, Bulgaria. The excellent folks from the &lt;a href="http://clg.wlv.ac.uk/"&gt;computational
linguistics group&lt;/a&gt; at the University of Wolverhampton were behind it,
together with the &lt;a href="http://www.iict.bas.bg/EN/index.html"&gt;Institute of Information and Communication
Technologies&lt;/a&gt; from the Bulgarian Academy of Sciences.&lt;br /&gt;
[caption id=&amp;#8221;attachment_247&amp;#8221; align=&amp;#8221;alignleft&amp;#8221; width=&amp;#8221;225&amp;#8221; caption=&amp;#8221;
&amp;#8220;][&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2011/09/319030_10150303323328171_677848170_8151131_1046590975_n1.jpg?w=225" title="Warm spring in Hissar" /&gt;][]&lt;/p&gt;
&lt;p&gt;A fountain with warm mineral spring water in Hissarya. &lt;br /&gt;
&lt;small&gt;Picture by &lt;a href="http://pers-www.wlv.ac.uk/~ex0233/" title="Miranda Chong"&gt;Miranda&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;[/caption]&lt;br /&gt;
I must begin by thanking them: the organization was impeccable! I&amp;#8217;m not
sure, but I think that at one point Ivelina was even running around
buying routers to improve wifi coverage (which is already spectacular in
Bulgaria &amp;#8212; I&amp;#8217;ve received reports from &lt;a href="http://pers-www.wlv.ac.uk/~ex0233/" title="Miranda Chong"&gt;Miranda&lt;/a&gt; that you can get wifi
in the&amp;nbsp;mountains!)&lt;/p&gt;
&lt;p&gt;The schedule was busy, with three tracks going in parallel, in order to
cover a wide range of topics in computational linguistics. The student
workshop should also be noted for the excellent quality of the works&amp;nbsp;there.&lt;/p&gt;
&lt;p&gt;Of course it would be infeasible to write about all the great people I
met and their high quality work. And if I were to write about all the
fun we had, it would probably make this post look unprofessional :).
This doesn&amp;#8217;t mean I forgot about any of you, and as soon as I get the
chance to work on something related, I will most certainly write about
it, and&amp;nbsp;you.&lt;/p&gt;
&lt;p&gt;So, if I would have to summarize the trends and the ideas stated during
the conference and especially during the keynotes, I would&amp;nbsp;say:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When talking about word sense disambiguation, it&amp;#8217;s wrong to speak
    about the different meanings of a word, but rather about the
    potential a word has for bringing a certain meaning in a certain
    context. See &lt;a href="http://www.patrickhanks.com/"&gt;Patrick Hanks&lt;/a&gt;&amp;#8216; &lt;a href="http://nlp.fi.muni.cz/projects/cpa/"&gt;Corpus Pattern Analysis&lt;/a&gt;. Without
    something like this, to have good &lt;span class="caps"&gt;WSD&lt;/span&gt; you need to heavily adjust the
    overlapping meanings from a Wordnet-style&amp;nbsp;ontology.&lt;/li&gt;
&lt;li&gt;Certain relations, such as temporal and spacial ones, can naturally
    be modeled by complex domain-specific logics (see [Inderjeet
    Mani][]&amp;#8217;s new book, Interpreting Motion: Grounded Representations
    for Spatial Language, that is due for publishing). But these only
    appear in a small subset of human communication. The attempt to map
    human language to a complete logic in which to do general-purpose
    inference seems futile: &lt;a href="http://u.cs.biu.ac.il/~dagan/"&gt;Ido Dagan&lt;/a&gt; suggests textual entailment:
    reasoning directly in natural language, and only abstracting away to
    a formal logic system when need&amp;nbsp;arises.&lt;/li&gt;
&lt;li&gt;If you have a large enough sample of n-gram frequency data, you can
    eventually beat the performance you can get with a limited amount of
    labeled data, and most importantly: it generalizes much better when
    going out of the domain you trained on. Apparently the best tool for
    this at the moment is the &lt;a href="http://ngrams.googlelabs.com/datasets"&gt;Google n-gram data&lt;/a&gt;, which has some
    limitations. In time, we can easily extend this data by huge amounts
    by mining n-grams from Wikipedia (which allegedly has a higher count
    of distinct n-grams than the Google dataset), and more importantly,
    by aligning multi-language data, making use of transliterations and
    cognate&amp;nbsp;identification.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please note that I may be (or most probably, am) ignorant of older
instances of similar ideas, and I may have misunderstood certain claims.
Please feel free to discuss in the comments whether you think I forgot
about something important, or whether I am plain wrong about something.
In particular, I seem to have been completely ignorant of the existence
of the Google n-gram data, which has been around for quite a while, so I
must have missed other important things as&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;Take care, kind readers, and express your opinion!&lt;br /&gt;&amp;nbsp;V&lt;/p&gt;
&lt;p&gt;[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2011/09/319030_10150303323328171_677848170_8151131_1046590975_n1.jpg?w=225" title="Warm spring in Hissar" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/09/319030_10150303323328171_677848170_8151131_1046590975_n1.jpg&lt;/p&gt;</summary><category term="conferences"></category><category term="nlp"></category></entry><entry><title>Dictionary learning in scikit-learn 0.9</title><link href="http://vene.ro/blog/dictionary-learning-in-scikit-learn-0-9.html" rel="alternate"></link><updated>2011-09-19T19:15:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-09-19:blog/dictionary-learning-in-scikit-learn-0-9.html</id><summary type="html">&lt;p&gt;Thanks to Olivier, Gaël and Alex, who reviewed the code heavily the last
couple of days, and with apologies for my lack of activity during a
sequence of conferences, Dictionary learning has officially been merged
into scikit-learn master, and just in time for the new scikit-learn 0.9
release. Here are some glimpses of the examples you can run for&amp;nbsp;yourself:&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Dictionary learned from Lena patches" src="http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_1.png" title="plot_image_denoising_1" /&gt;][]&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Noisy image for denoising" src="http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_24.png" title="plot_image_denoising_24" /&gt;][]&lt;/p&gt;
&lt;p&gt;[![Image denoising with Dictionary learning and Orthogonal matching&amp;nbsp;pursuit][]][]&lt;/p&gt;
&lt;p&gt;The stars of this new release are: the manifold learning module by Jake
Vanderplas and Fabian Pedregosa, the Dirichlet process gaussian mixture
model by Alexandre Passos, and many others, as you can see from the
&lt;a href="http://scikit-learn.sourceforge.net/dev/whats_new.html" title="scikit-learn development changelog"&gt;development changelog&lt;/a&gt; (as soon as the release is made, I will update
this post with permanent&amp;nbsp;links).&lt;/p&gt;
&lt;p&gt;The release is due tomorrow. I will also be in charge with building the
Windows installers for this release, let&amp;#8217;s hope I do a good job and you
can think of me and smile when&amp;nbsp;installing!&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Dictionary learned from Lena patches" src="http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_1.png" title="plot_image_denoising_1" /&gt;]: http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_1.png
  [&lt;img alt="Noisy image for denoising" src="http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_24.png" title="plot_image_denoising_24" /&gt;]: http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_24.png
  [Image denoising with Dictionary learning and Orthogonal matching
  pursuit]: http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_44.png
    &amp;#8220;Image denoising with Dictionary learning and Orthogonal matching pursuit&amp;#8221;
  [![Image denoising with Dictionary learning and Orthogonal matching
  pursuit][]]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_44.png&lt;/p&gt;</summary><category term="dictionary learning"></category><category term="scikit-learn"></category></entry><entry><title>Long overdue update. EuroScipy and SSLST 2011</title><link href="http://vene.ro/blog/long-overdue-update-euroscipy-and-sslst-2011.html" rel="alternate"></link><updated>2011-09-05T00:08:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-09-05:blog/long-overdue-update-euroscipy-and-sslst-2011.html</id><summary type="html">&lt;p&gt;Anybody reading my blog should have expected me to blog about the end of
my GSoC. Sorry to disappoint, but I simply did not experience anything
similar to an ending. On the contrary, I feel like things have barely
started. Also, I apologize for one of the few posts here without pretty
pictures!&amp;nbsp;:)&lt;/p&gt;
&lt;p&gt;For the last two weeks, I&amp;#8217;ve been traveling. I attended the EuroScipy
conference thanks to Fabian, who offered me a place to sleep during the
week. We sprinted hard, we discussed tricky APIs, we drank a lot of
coffee, beer, and ate well in lovely Paris. It was great to meet all of
the celebrities, the people who keep the scientific Python globe&amp;nbsp;turning.&lt;/p&gt;
&lt;p&gt;Many thanks to Gael and Emmanuelle, who worked very, very hard on
organizing everything, so they weren&amp;#8217;t around and I didn&amp;#8217;t get to say
goodbye when I ran to catch my plane last&amp;nbsp;Sunday.&lt;/p&gt;
&lt;p&gt;I was in a hurry, heading to Tarragona, a beautiful city on the Catalan
coast, where the public university organized the 2011 summer school in
linguistics and speech technologies (&lt;span class="caps"&gt;SSLST&lt;/span&gt;). This was a great
opportunity to meet many fellow young researchers working in
computational linguistics. I will not go into details now, because I
plan expand on this, but I would like to state a couple of things.
Firsty, even though &lt;span class="caps"&gt;NLP&lt;/span&gt; seems to be mostly a Java-dominated affair (note
for example &lt;a href="http://nlp.stanford.edu/software/index.shtml"&gt;Stanford&amp;#8217;s &lt;span class="caps"&gt;NLP&lt;/span&gt; toolkit&lt;/a&gt; and &lt;a href="http://gate.ac.uk/"&gt;Sheffield&amp;#8217;s &lt;span class="caps"&gt;GATE&lt;/span&gt;&lt;/a&gt;), the
[computational linguistics and psycholinguistics research center
(CLiPS)][] at the University of Antwerp actually briefly manifested its
devotion to Python and &lt;span class="caps"&gt;NLTK&lt;/span&gt; via its research director, Walter&amp;nbsp;Daelemans.&lt;/p&gt;
&lt;p&gt;It was good to see a little love for Python in this field. &lt;span class="caps"&gt;NLTK&lt;/span&gt; is very
underrepresented in the SciPy community, I couldn&amp;#8217;t find anybody at the
EuroScipy conference knowing too much about it or about the people
behind&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;Another lab that has done a lot of cool work is &lt;a href="http://www.uclouvain.be/cental"&gt;Cental&lt;/a&gt; at the
Catholic University of Louvain, and they also use Python for natural
language processing. Maybe in the coming years, we will see a Python for
Computational Linguistics sattelite, along with Physics and
Neuroscience. Doesn&amp;#8217;t it sound more fun?&amp;nbsp;:P&lt;/p&gt;
&lt;p&gt;Secondly, I wish &lt;span class="caps"&gt;SSLST&lt;/span&gt; were organized by someone like Gael! As the
discussion at dinner regarding who will organize next year&amp;#8217;s EuroScipy
went, it is imperative that the organizers be actively involved in the
community, and generally passionate about it. Even though I&amp;#8217;m comparing
apples and oranges, Carlos Martin-Vide behaved in this context like a
old, tired, emotionless academic, not taking into account even lunch
breaks for the whole group, not to mention any sort of getting together
or even a group photo (which, alas, we were not able to take, apart from
small groups.) They said it couldn&amp;#8217;t be done. Of course it could, they
just didn&amp;#8217;t want it hard&amp;nbsp;enough.&lt;/p&gt;
&lt;p&gt;Finally, before signing off, I would like to announce that because the
Romanian Ministry of Education failed to specify the allocated number of
public positions for masters&amp;#8217; programmes, the admission exam at the
University of Bucharest will be delayed by a couple of weeks. Luckily,
this will allow me to attend &lt;a href="http://lml.bas.bg/ranlp2011/start3.php"&gt;&lt;span class="caps"&gt;RANLP&lt;/span&gt; 2011&lt;/a&gt; in Hissar, Bulgaria a week
from now, where I will present my poster entitled:&lt;br /&gt;
&amp;#8220;Can alternations be learned? A machine learning approach to Romanian
verb conjugation&amp;#8221; by Liviu P. Dinu, Emil Ionescu, Vlad Niculae and
Octavia-Maria Sulea. See you in&amp;nbsp;Hissar!&lt;/p&gt;
&lt;p&gt;[computational linguistics and psycholinguistics research center
  (CLiPS)]:&amp;nbsp;http://www.clips.ua.ac.be/&lt;/p&gt;</summary><category term="Uncategorized"></category></entry><entry><title>Optimizing Orthogonal Matching Pursuit code in Numpy, part 2</title><link href="http://vene.ro/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2.html" rel="alternate"></link><updated>2011-08-11T19:39:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-08-11:blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2.html</id><summary type="html">&lt;p&gt;&lt;span class="caps"&gt;EDIT&lt;/span&gt;: There was a bug in the final version of the code presented here.
It is fixed now, for its backstory, check out &lt;a href="http://venefrombucharest.wordpress.com/2011/11/18/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code/" title="The nasty bug crawling in my Orthogonal Matching Pursuit code"&gt;my blog post on it&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we last saw our hero, he was fighting with the dreaded
implementation of least-angle regression, knowing full well that it was
his destiny to be&amp;nbsp;faster.&lt;/p&gt;
&lt;p&gt;We had come up with a more robust implementation, catching malformed
cases that would have broken the naive implementation, and also it was
orders of magnitude faster than said implementation. However, our
benchmark [&lt;a href="#footnote-1"&gt;1&lt;/a&gt;] showed that it was a couple of times slower than
least-angle&amp;nbsp;regression.&lt;/p&gt;
&lt;p&gt;By poking around the &lt;code&gt;scikits.learn&lt;/code&gt; codebase, I noticed that there is a
triangular system solver in &lt;code&gt;scikits.learn.utils.arrayfuncs&lt;/code&gt;. Unlike the
&lt;code&gt;scipy.linalg&lt;/code&gt; one, this one only works with lower triangular arrays,
and it forcefully overwrites &lt;code&gt;b&lt;/code&gt;. Even though if weren&amp;#8217;t faster, it
should still be used: &lt;code&gt;scikits.learn&lt;/code&gt; aims to be as backwards-compatible
with SciPy as possible, and &lt;code&gt;linalg.solve_triangular&lt;/code&gt; was added in
0.9.0. Anyway, let&amp;#8217;s just see whether it&amp;#8217;s&amp;nbsp;faster:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In &lt;a href="#footnote-1"&gt;1&lt;/a&gt;: import numpy as&amp;nbsp;np&lt;/p&gt;
&lt;p&gt;In &lt;a href="#footnote-2"&gt;2&lt;/a&gt;: from scipy import&amp;nbsp;linalg&lt;/p&gt;
&lt;p&gt;In [3]: from scikits.learn.datasets import&amp;nbsp;make_spd_matrix&lt;/p&gt;
&lt;p&gt;In [4]: from scikits.learn.utils.arrayfuncs import&amp;nbsp;solve_triangular&lt;/p&gt;
&lt;p&gt;In [5]: G =&amp;nbsp;make_spd_matrix(1000)&lt;/p&gt;
&lt;p&gt;In [6]: L = linalg.cholesky(G,&amp;nbsp;lower=True)&lt;/p&gt;
&lt;p&gt;In [7]: x =&amp;nbsp;np.random.randn(1000)&lt;/p&gt;
&lt;p&gt;In [8]: y =&amp;nbsp;x.copy()&lt;/p&gt;
&lt;p&gt;In [9]: timeit solve_triangular(L, x)&lt;br /&gt;
100 loops, best of 3: 3.45 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [10]: timeit linalg.solve_triangular(L, y, lower=True,
overwrite_b=True)&lt;br /&gt;
10 loops, best of 3: 134 ms per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Wow! That&amp;#8217;s 40x faster. We&amp;#8217;re catching two rabbits with one stone here,
let&amp;#8217;s do the change! Notice that we can just copy [latex]
\mathbf{v}[/latex] into the appropriate place in [latex] L[/latex] and
then solve in&amp;nbsp;place.&lt;/p&gt;
&lt;p&gt;But whoops! When solving the [latex] &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;[/latex] system, we take
advantage of the &lt;code&gt;transpose&lt;/code&gt; attribute in &lt;code&gt;linalg.solve_triangular&lt;/code&gt;,
which the &lt;code&gt;scikits.learn&lt;/code&gt; version does not expose. We could think of a
solution, but here&amp;#8217;s a better idea: Shouldn&amp;#8217;t there be some way to
directly solve the entire system in one&amp;nbsp;go?&lt;/p&gt;
&lt;p&gt;Well, there is. It is an &lt;span class="caps"&gt;LAPACK&lt;/span&gt; function by the name of &lt;code&gt;potrs&lt;/code&gt;. If you
are not aware, &lt;span class="caps"&gt;LAPACK&lt;/span&gt; is a Fortran library with solvers for various
types of linear systems and eigenproblems. &lt;span class="caps"&gt;LAPACK&lt;/span&gt; along with &lt;span class="caps"&gt;BLAS&lt;/span&gt; (on
which it is based) pretty much powers all the scientific computation
that happens. &lt;span class="caps"&gt;BLAS&lt;/span&gt; is an &lt;span class="caps"&gt;API&lt;/span&gt; with multiple implementations dating from
1979, while &lt;span class="caps"&gt;LAPACK&lt;/span&gt; dates from 1992. If you ever used Matlab, this is
what was called behind the scenes. SciPy, again, provides a high-level
wrapper around this, the &lt;code&gt;linalg.cho_solve&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;But SciPy also gives us the possibility to import functions directly
from &lt;span class="caps"&gt;LAPACK&lt;/span&gt;, through the use of &lt;code&gt;linalg.lapack.get_lapack_funcs&lt;/code&gt;. Let&amp;#8217;s
see how the low-level &lt;span class="caps"&gt;LAPACK&lt;/span&gt; function compares to the SciPy wrapper, for
our use&amp;nbsp;case:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [11]: x =&amp;nbsp;np.random.randn(1000)&lt;/p&gt;
&lt;p&gt;In [12]: y =&amp;nbsp;x.copy()&lt;/p&gt;
&lt;p&gt;In [13]: timeit linalg.cho_solve((L, True), x)&lt;br /&gt;
1 loops, best of 3: 95.4 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [14]: potrs, = linalg.lapack.get_lapack_funcs((&amp;#8216;potrs&amp;#8217;,),&amp;nbsp;(G,))&lt;/p&gt;
&lt;p&gt;In [15]: potrs&lt;br /&gt;
Out[15]: &amp;lt;fortran&amp;nbsp;object&amp;gt;&lt;/p&gt;
&lt;p&gt;In [16]: timeit potrs(L, y)&lt;br /&gt;
100 loops, best of 3: 9.49 ms per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;That&amp;#8217;s 10 times faster! So now we found an obvious way to optimize the&amp;nbsp;code:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
def cholesky_omp(X, y, n_nonzero_coefs, eps=None):&lt;br /&gt;
min_float = np.finfo(X.dtype).eps&lt;br /&gt;
potrs, = get_lapack_funcs((&amp;#8216;potrs&amp;#8217;,), (X,))&lt;br /&gt;
alpha = np.dot(X.T, y)&lt;br /&gt;
residual = y&lt;br /&gt;
n_active = 0&lt;br /&gt;
idx =&amp;nbsp;[]&lt;/p&gt;
&lt;p&gt;max_features = X.shape&lt;a href="#footnote-1"&gt;1&lt;/a&gt; if eps is not None else n_nonzero_coefs&lt;br /&gt;
L = np.empty((max_features, max_features), dtype=X.dtype)&lt;br /&gt;
L[0, 0] =&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;while 1:&lt;br /&gt;
lam = np.abs(np.dot(X.T, residual)).argmax()&lt;br /&gt;
if lam &amp;lt; n_active or alpha[lam] ** 2 &amp;lt; min_float:&lt;br /&gt;
# atom already selected or inner product too small&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
if n_active &amp;gt; 0:&lt;br /&gt;
# Updates the Cholesky decomposition of X&amp;#8217; X&lt;br /&gt;
L[n_active, :n_active] = np.dot(X[:, idx].T, X[:, lam]&lt;br /&gt;
solve_triangular(L[:n_active, :n_active], L[n_active, :n_active])&lt;br /&gt;
d = np.dot(L[n_active, :n_active].T, L[n_active, :n_active])&lt;br /&gt;
if 1 - d &amp;lt;= min_float: # selected atoms are dependent&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
L[n_active, n_active] = np.sqrt(1 - d)&lt;br /&gt;
idx.append(lam)&lt;br /&gt;
# solve &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;x = y in two steps:&lt;br /&gt;
gamma, _ = potrs(L[:n_active, :n_active], alpha[idx], lower=True,&lt;br /&gt;
overwrite_b=False)&lt;br /&gt;
residual = y - np.dot(X[:, idx], gamma)&lt;br /&gt;
if eps is not None and np.dot(residual.T, residual) &amp;lt;= eps:&lt;br /&gt;
break&lt;br /&gt;
elif n_active == max_features:&lt;br /&gt;
break&lt;br /&gt;
return gamma, idx&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Woohoo! But we still lag behind. Now that we delegated the trickiest
parts of the code to fast and reliable solvers, it&amp;#8217;s time to use a
profiler and see what the bottleneck is now. Python has excellent tools
for this purpose. What solved the problem in this case was
&lt;code&gt;line_profiler&lt;/code&gt; [&lt;a href="#footnote-2"&gt;2&lt;/a&gt;]. There is a great article by Olivier Grisel here
&lt;a href="#footnote-2"&gt;2&lt;/a&gt; regarding how to use these profilers. I&amp;#8217;m just going to say that
&lt;code&gt;line_profiler&lt;/code&gt;&amp;#8216;s output is very helpful, basically printing the time
taken by each line of code next to that&amp;nbsp;line.&lt;/p&gt;
&lt;p&gt;Running the profiler on this code, we found that 58% of the time is
spent on line 14, 20.5% on line 21, and 20.5% on line 32, with the rest
being insignificant (&lt;code&gt;potrs&lt;/code&gt; takes 0.1%!). The code is clearly dominated
by the matrix multiplications. By running some more timings with
IPython, I found that multiplying such column-wise views of the data as
&lt;code&gt;X[:, idx]&lt;/code&gt; is considerably slower then multiplying a contiguous array.
The least-angle regression code in &lt;code&gt;scikits.learn&lt;/code&gt; avoids this by
swapping columns towards the front of the array as they are chosen, so
we can replace &lt;code&gt;X[:, idx]&lt;/code&gt; with &lt;code&gt;X[:, :n_active]&lt;/code&gt;. The nice part is that
if the array is stored in Fortran-contiguous order (ie. column
contiguous order, as opposed to row contiguous order, as in C), swapping
two columns is a very fast operation!. Let&amp;#8217;s see some more&amp;nbsp;benchmarks!&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [17]: X = np.random.randn(5000,&amp;nbsp;5000)&lt;/p&gt;
&lt;p&gt;In [18]: Y = X.copy(&amp;#8216;F&amp;#8217;) #&amp;nbsp;fortran-ordered&lt;/p&gt;
&lt;p&gt;In [19]: a, b = 1000,&amp;nbsp;2500&lt;/p&gt;
&lt;p&gt;In [20]: swap, = linalg.get_blas_funcs((&amp;#8216;swap&amp;#8217;,),&amp;nbsp;(X,))&lt;/p&gt;
&lt;p&gt;In [21]: timeit X[:, a], X[:, b] = swap(X[:, a], X[:, b])&lt;br /&gt;
100 loops, best of 3: 6.29 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [22]: timeit Y[:, a], Y[:, b] = swap(Y[:, a], Y[:, b])&lt;br /&gt;
10000 loops, best of 3: 111 us per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;We can see that using Fortran-order takes us from the order of
miliseconds to the order of&amp;nbsp;microseconds!&lt;/p&gt;
&lt;p&gt;Side note: I almost fell into the trap of swapping columns the pythonic
way. That doesn&amp;#8217;t work:&lt;br /&gt;
[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [23]: X[:, a], X[:, b] = X[:, b], X[:,&amp;nbsp;a]&lt;/p&gt;
&lt;p&gt;In [24]: np.testing.assert_array_equal(X[:, a], X[:,&amp;nbsp;b])&lt;/p&gt;
&lt;p&gt;In [25]:&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;However this trick works great for swapping elements of one-dimensional&amp;nbsp;arrays.&lt;/p&gt;
&lt;p&gt;Another small optimization that we can do: I found that on my system,
it&amp;#8217;s slightly faster to compute the norm using the &lt;span class="caps"&gt;BLAS&lt;/span&gt; function &lt;code&gt;nrm2&lt;/code&gt;.
So by putting all of these together, we end up with the final version of
our&amp;nbsp;code:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
def cholesky_omp(X, y, n_nonzero_coefs, eps=None,
overwrite_X=False):&lt;br /&gt;
if not overwrite_X:&lt;br /&gt;
X = X.copy(&amp;#8216;F&amp;#8217;)&lt;br /&gt;
else: # even if we are allowed to overwrite, still copy it if bad
order&lt;br /&gt;
X =&amp;nbsp;np.asfortranarray(X)&lt;/p&gt;
&lt;p&gt;min_float = np.finfo(X.dtype).eps&lt;br /&gt;
nrm2, swap = linalg.get_blas_funcs((&amp;#8216;nrm2&amp;#8217;, &amp;#8216;swap&amp;#8217;), (X,))&lt;br /&gt;
potrs, = get_lapack_funcs((&amp;#8216;potrs&amp;#8217;,),&amp;nbsp;(X,))&lt;/p&gt;
&lt;p&gt;indices = range(len(Gram)) # keeping track of swapping&lt;br /&gt;
alpha = np.dot(X.T, y)&lt;br /&gt;
residual = y&lt;br /&gt;
n_active =&amp;nbsp;0&lt;/p&gt;
&lt;p&gt;max_features = X.shape&lt;a href="#footnote-1"&gt;1&lt;/a&gt; if eps is not None else n_nonzero_coefs&lt;br /&gt;
L = np.empty((max_features, max_features), dtype=X.dtype)&lt;br /&gt;
L[0, 0] =&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;while True:&lt;br /&gt;
lam = np.abs(np.dot(X.T, residual)).argmax()&lt;br /&gt;
if lam &amp;lt; n_active or alpha[lam] ** 2 &amp;lt; min_float:&lt;br /&gt;
# atom already selected or inner product too small&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
if n_active &amp;gt; 0:&lt;br /&gt;
# Updates the Cholesky decomposition of X&amp;#8217; X&lt;br /&gt;
L[n_active, :n_active] = np.dot(X[:, :n_active].T, X[:, lam])&lt;br /&gt;
solve_triangular(L[:n_active, :n_active], L[n_active, :n_active])&lt;br /&gt;
v = nrm2(L[n_active, :n_active]) ** 2&lt;br /&gt;
if 1 - v &amp;lt;= min_float: # selected atoms are dependent&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
L[n_active, n_active] = np.sqrt(1 - v)&lt;br /&gt;
X.T[n_active], X.T[lam] = swap(X.T[n_active], X.T[lam])&lt;br /&gt;
alpha[n_active], alpha[lam] = alpha[lam], alpha[n_active]&lt;br /&gt;
indices[n_active], indices[lam] = indices[lam], indices[n_active]&lt;br /&gt;
n_active += 1&lt;br /&gt;
# solves &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;x = y as a composition of two triangular systems&lt;br /&gt;
gamma, _ = potrs(L[:n_active, :n_active], alpha[:n_active],
lower=True,&lt;br /&gt;&amp;nbsp;overwrite_b=False)&lt;/p&gt;
&lt;p&gt;residual = y - np.dot(X[:, :n_active], gamma)&lt;br /&gt;
if eps is not None and nrm2(residual) ** 2 &amp;lt;= eps:&lt;br /&gt;
break&lt;br /&gt;
elif n_active == max_features:&lt;br /&gt;&amp;nbsp;break&lt;/p&gt;
&lt;p&gt;return gamma, indices[:n_active]&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Now, the benchmark at [&lt;a href="#footnote-1"&gt;1&lt;/a&gt;] indicates victory over least-angle
regression! I hope you have enjoyed this short tour. See you next&amp;nbsp;time!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_plot_omp_lars.py" title="Orthogonal matching pursuit versus least-angle regression"&gt;&lt;span id="footnote-1"&gt;1&lt;/span&gt;&lt;/a&gt;[]&lt;br /&gt;
&lt;a href="http://scikit-learn.sourceforge.net/dev/developers/performance.html#profiling-python-code" title="Profiling Python code"&gt;&lt;span id="footnote-2"&gt;2&lt;/span&gt;&lt;/a&gt;[]&lt;/p&gt;</summary><category term="blas"></category><category term="efficient"></category><category term="lapack"></category><category term="numpy"></category><category term="omp"></category><category term="orthogonal matching pursuit"></category><category term="potrs"></category><category term="scipy"></category><category term="dictionary learning"></category><category term="python"></category><category term="scikit-learn"></category></entry><entry><title>Optimizing Orthogonal Matching Pursuit code in Numpy, part 1</title><link href="http://vene.ro/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1.html" rel="alternate"></link><updated>2011-08-07T20:50:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-08-07:blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1.html</id><summary type="html">&lt;p&gt;After intense code optimization work, my implementation of &lt;span class="caps"&gt;OMP&lt;/span&gt; finally
beat least-angle regression! This was the primary issue discussed during
the pull request, so once performance was taken care of, the code was
ready for merge. Orthogonal matching pursuit is now available in
scikits.learn as a sparse linear regression model. &lt;span class="caps"&gt;OMP&lt;/span&gt; is a key building
block of the dictionary learning code that we are working on&amp;nbsp;merging.&lt;/p&gt;
&lt;p&gt;I will go through the process of developing this particular piece of
code as an example of code refining and iterative improvements, as well
as for the useful notes it will provide on optimizing numerical Python
code. In the first part we will see how the code got from pseudocode
state to a reasonably efficient code with smart memory allocation. In
the next part we will see how to make it blazing fast by leveraging
[&lt;a href="#footnote-1"&gt;1&lt;/a&gt;] lower level &lt;span class="caps"&gt;BLAS&lt;/span&gt; and &lt;span class="caps"&gt;LAPACK&lt;/span&gt; routines, and how to use profiling
to find hot&amp;nbsp;spots.&lt;/p&gt;
&lt;p&gt;As stated before, orthogonal matching pursuit is a greedy algorithm for
finding a sparse solution [latex] \gamma[/latex] to a linear regression
problem [latex] X\gamma = y[/latex]. Mathematically, it approximates
the solution of the optimization&amp;nbsp;problem:&lt;/p&gt;
&lt;p&gt;\$\$ \text{argmin} {\big|\big|} \gamma {\big|\big|} _0 \text{
subject to }{\big |\big|}y-X\gamma{\big|\big|}_2\^2 \leq
\epsilon \$\$&lt;br /&gt;
or (under a different parametrization):&lt;br /&gt;
\$\$\text{argmin} {\big |\big|}y - X\gamma{\big |\big|}_2\^2
\text{ subject to } {\big|\big|}\gamma{\big|\big|}_0 \leq
n_{\text{nonzero&amp;nbsp;coefs}}\$\$&lt;/p&gt;
&lt;p&gt;In the code samples in this post I will omit the docstrings, but I will
follow the notation in the formulas&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important note:&lt;/strong&gt; The regressors/dictionary atoms (the columns of
[latex] X[/latex]) are assumed to be normalized throughout this post (as
well as usually any discussion of &lt;span class="caps"&gt;OMP&lt;/span&gt;). We also assume the following
imports:&lt;br /&gt;
[sourcecode language=&amp;#8221;Python&amp;#8221;]&lt;br /&gt;
import numpy as np&lt;br /&gt;
from scipy import linalg&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Orthogonal matching pursuit is a very simple algorithm in pseudocode,
and as I stated before, it almost writes itself in Numpy. For this
reason, instead of stating the pseudocode here, I will start with how
naively implemented &lt;span class="caps"&gt;OMP&lt;/span&gt; looks like in&amp;nbsp;Python:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;Python&amp;#8221;]&lt;br /&gt;
def orthogonal_mp(X, y, n_nonzero_coefs, eps=None):&lt;br /&gt;
residual = y&lt;br /&gt;
idx = []&lt;br /&gt;
if eps == None:&lt;br /&gt;
stopping_condition = lambda: len(idx) == n_nonzero_coefs&lt;br /&gt;
else:&lt;br /&gt;
stopping_condition = lambda: np.inner(residual, residual) &amp;lt;= eps&lt;br /&gt;
while not stopping_condition():&lt;br /&gt;
lam = np.abs(np.dot(residual, X)).argmax()&lt;br /&gt;
idx.append(lam)&lt;br /&gt;
gamma, _, _, _ = linalg.lstsq(X[:, idx], y)&lt;br /&gt;
residual = y - np.dot(X[:, idx], gamma)&lt;br /&gt;
return gamma, idx&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Using lambda expressions as stopping conditions never looked like a
brilliant idea, but it seems to me like the most elegant way to specify
such a variable stopping condition. However, the biggest slowdown in
this is the need for solving a least squares problem at each iteration,
while least-angle regression is known to produce the entire
regularization path for the cost of a single least squares problem. We
will also see that this implementation is more vulnerable to numerical
stability&amp;nbsp;issues.&lt;/p&gt;
&lt;p&gt;In [&lt;a href="#footnote-2"&gt;2&lt;/a&gt;], Rubinstein et al. described the Cholesky-&lt;span class="caps"&gt;OMP&lt;/span&gt; algorithm, an
implementation of &lt;span class="caps"&gt;OMP&lt;/span&gt; that avoids solving a new least squares problem at
each iteration by keeping a Cholesky decomposition [latex] &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;[/latex]
of the Gram matrix [latex] G =
X_{\text{idx}}&amp;#8217;X_{\text{idx}}[/latex]. Because [latex]
X_{\text{idx}}[/latex] grows by exactly one column at each iteration,
[latex] L[/latex] can be updated according to the following rule: Given
[latex] A = \begin{pmatrix} \tilde{A} &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; \mathbf{v}&amp;#8217; &amp;#92; \mathbf{v}
&lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; c \end{pmatrix}[/latex], and knowing the decomposition of [latex]
\tilde{A} = \tilde{L}\tilde{L}&amp;#8217;[/latex], the Cholesky decomposition
[latex] A = &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;[/latex] is given by \$\$ L = \begin{pmatrix}\tilde{L}
&lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; \mathbf{0} &amp;#92; \mathbf{w}&amp;#8217; &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; \sqrt{c - \mathbf{w}&amp;#8217;\mathbf{w}}
\end{pmatrix}, \text{ where } \tilde{L}\mathbf{w} =&amp;nbsp;\mathbf{v}\$\$&lt;/p&gt;
&lt;p&gt;Even if you are unfamiliar with the mathematical properties of the
Cholesky decomposition, you can see from the construction detailed above
that [latex] L[/latex] is always going to be a lower triangular matrix
(it will only have null elements above the main diagonal). Actually, the
letter L stands for lower. We have therefore replaced the step where we
needed to solve the least-squares problem [latex]
X_{\text{idx}}\gamma = y[/latex] with two much simpler computations:
solving [latex] \tilde{L}\mathbf{w} = \mathbf{v}[/latex] and solving
[latex] &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;\gamma = X_{\text{idx}}&amp;#8217;y[/latex]. Due to the [latex]
L[/latex]&amp;#8217;s structure, these are much quicker operations than a least
squares projection.&lt;br /&gt;
Here is the initial way I implemented&amp;nbsp;this:&lt;/p&gt;
&lt;p&gt;[sourcecode&amp;nbsp;language=&amp;#8221;Python&amp;#8221;]&lt;/p&gt;
&lt;p&gt;def cholesky_omp(X, y, n_nonzero_coefs, eps=None):&lt;br /&gt;
if eps == None:&lt;br /&gt;
stopping_condition = lambda: it == n_nonzero_coefs&lt;br /&gt;
else:&lt;br /&gt;
stopping_condition = lambda: np.inner(residual, residual) \&amp;lt;=&amp;nbsp;eps&lt;/p&gt;
&lt;p&gt;alpha = np.dot(X.T, y)&lt;br /&gt;
residual = y&lt;br /&gt;
idx = []&lt;br /&gt;
L =&amp;nbsp;np.ones((1,1))&lt;/p&gt;
&lt;p&gt;while not stopping_condition():&lt;br /&gt;
lam = np.abs(np.dot(residual, X)).argmax()&lt;br /&gt;
if len(idx) &amp;gt; 0:&lt;br /&gt;
w = linalg.solve_triangular(L, np.dot(X[:, idx].T, X[:, lam]),&lt;br /&gt;
lower=True)&lt;br /&gt;
L = np.r_[np.c_[L, np.zeros(len(L))],&lt;br /&gt;
np.atleast_2d(np.append(w, np.sqrt(1 - np.dot(w.T, w))))]&lt;br /&gt;
idx.append(lam)&lt;br /&gt;
# solve &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;x = y in two steps:&lt;br /&gt;
Ltx = linalg.solve_triangular(L, alpha[idx], lower=True)&lt;br /&gt;
gamma = linalg.solve_triangular(L, Ltx, trans=1, lower=True)&lt;br /&gt;
residual = y - np.dot(X[:, idx],&amp;nbsp;gamma)&lt;/p&gt;
&lt;p&gt;return gamma, idx&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Note that a lot of the code remained unchanged, this is the same
algorithm as before, only the Cholesky trick is used to improve
performance. According to the plot in [&lt;a href="#footnote-3"&gt;3&lt;/a&gt;], we can see that the naive
implementation has oscillations of the reconstruction error due to
numerical instability, while this Cholesky implementation is&amp;nbsp;well-behaved.&lt;/p&gt;
&lt;p&gt;Along with this I also implemented the Gram-based version of this
algorithm, which only needs [latex] X&amp;#8217;X[/latex] and [latex] X&amp;#8217;y[/latex]
(and [latex] {\big|\big|}y{\big|\big|}_2\^2[/latex], in case the
epsilon-parametrization is desired). This is called &lt;strong&gt;Batch &lt;span class="caps"&gt;OMP&lt;/span&gt;&lt;/strong&gt; in
[&lt;a href="#footnote-2"&gt;2&lt;/a&gt;], because it offers speed gains when many signals need to be
sparse coded against the same dictionary [latex] X[/latex]. A lot of
speed is gained because two large matrix multiplications are avoided at
each iteration, but for many datasets, the cost of the precomputations
dominates the procedure. I will not insist on Gram &lt;span class="caps"&gt;OMP&lt;/span&gt; in this post, it
can be found in the &lt;code&gt;scikits.learn&lt;/code&gt; repository [&lt;a href="#footnote-4"&gt;4&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Now, the problems with this are a bit more subtle. At this point, I
moved on to code other things, since &lt;span class="caps"&gt;OMP&lt;/span&gt; was passing tests and the
signal recovery example was working. The following issues popped up
during&amp;nbsp;review:&lt;/p&gt;
&lt;p&gt;​1. The lambda stopping condition does not pickle.&lt;br /&gt;
2. For well-constructed signals and data matrices, assuming normal
atoms, [latex] \mathbf{w}[/latex] on line 14 will never have norm
greater than or equal to zero, unless the chosen feature happens to be
dependent of the already chosen set. In theory, this cannot happen,
since we do an orthogonal projection at each step. However, if the
matrix [latex] X[/latex] is not well-behaved (for example, if it has two
identical columns, and [latex] y[/latex] is built using non-zero
coefficients for those columns), then we end up with the square root of
a negative value on line 17.&lt;br /&gt;
3. It was orders of magnitude slower than least-angle regression, given
the same number of nonzero&amp;nbsp;coefficients.&lt;/p&gt;
&lt;p&gt;1 was an easy fix. 2 was a bit tricky since it was a little hidden: the
first time I encountered such an error, I wrongfully assumed that given
that the diagonal of [latex] X_\text{idx}&amp;#8217;X_\text{idx}[/latex] was
unit, then [latex] L[/latex] should also have a unit diagonal, so I
passed the parameter &lt;code&gt;unit_diagonal=True&lt;/code&gt; to &lt;code&gt;linalg.solve_triangular&lt;/code&gt;,
and the plethora of NaN&amp;#8217;s along the diagonal were simply ignored. Let
this show what happens when you don&amp;#8217;t pay attention when&amp;nbsp;coding.&lt;/p&gt;
&lt;p&gt;When I realized my mistake, I first did something I saw in &lt;code&gt;lars_path&lt;/code&gt;
from the scikit: take the absolute value of the argument of &lt;code&gt;sqrt&lt;/code&gt;, and
also ensure it is practically larger than zero. However, tests started
failing randomly. Confusion ensued until the nature of the issue,
discussed above, was discovered. It&amp;#8217;s just not right to take the &lt;code&gt;abs&lt;/code&gt;:
if that argument ends up less than zero, &lt;span class="caps"&gt;OMP&lt;/span&gt; simply cannot proceed and
must stop due to malformed data. The reference implementation from the
website of the authors of [&lt;a href="#footnote-2"&gt;2&lt;/a&gt;] includes explicit &lt;em&gt;early stopping&lt;/em&gt;
conditions for this, along with some other&amp;nbsp;cases.&lt;/p&gt;
&lt;p&gt;At the same time, I started to try a couple of optimizations. The most
obvious thing was the way I was building the matrix [latex] L[/latex]
was clearly suboptimal, reallocating it at each&amp;nbsp;iteration.&lt;/p&gt;
&lt;p&gt;This leads to the following&amp;nbsp;code:&lt;/p&gt;
&lt;p&gt;[sourcecode&amp;nbsp;language=&amp;#8221;Python&amp;#8221;]&lt;/p&gt;
&lt;p&gt;def cholesky_omp(X, y, n_nonzero_coefs, eps=None):&lt;br /&gt;
min_float = np.finfo(X.dtype).eps&lt;br /&gt;
alpha = np.dot(X.T, y)&lt;br /&gt;
residual = y&lt;br /&gt;
n_active = 0&lt;br /&gt;
idx =&amp;nbsp;[]&lt;/p&gt;
&lt;p&gt;max_features = X.shape&lt;a href="#footnote-1"&gt;1&lt;/a&gt; if eps is not None else n_nonzero_coefs&lt;br /&gt;
L = np.empty((max_features, max_features), dtype=X.dtype)&lt;br /&gt;
L[0, 0] =&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;while 1:&lt;br /&gt;
lam = np.abs(np.dot(X.T, residual)).argmax()&lt;br /&gt;
if lam \&amp;lt; n_active or alpha[lam] ** 2 &gt; min_float:&lt;br /&gt;
# atom already selected or inner product too small&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
if n_active &amp;gt; 0:&lt;br /&gt;
# Updates the Cholesky decomposition of X&amp;#8217; X&lt;br /&gt;
w = linalg.solve_triangular(L[:n_active, :n_active],&lt;br /&gt;
np.dot(X[:, idx].T, X[:, lam]),&lt;br /&gt;
lower=True)&lt;br /&gt;
L[n_active, :n_active] = w&lt;br /&gt;
d = np.dot(w.T, w)&lt;br /&gt;
if 1 - d &amp;lt;= min_float: # selected atoms are dependent&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
L[n_active, n_active] = np.sqrt(1 - d)&lt;br /&gt;
idx.append(lam)&lt;br /&gt;
# solve &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;x = y in two steps:&lt;br /&gt;
Ltx = linalg.solve_triangular(L[:n_active, :n_active], alpha[idx],
lower=True)&lt;br /&gt;
gamma = linalg.solve_triangular(L[:n_active, :n_active], Ltx,
trans=1, lower=True)&lt;br /&gt;
residual = y - np.dot(X[:, idx], gamma)&lt;br /&gt;
if eps is not None and np.dot(residual.T, residual) &amp;lt;= eps:&lt;br /&gt;
break&lt;br /&gt;
elif n_active == max_features:&lt;br /&gt;
break&lt;br /&gt;
return gamma, idx&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;What should be noted here, apart from the obvious fix for #1, are the
early stopping conditions. It is natural to stop if the same feature
gets picked twice: the residual is always orthogonalized with respect to
the chosen basis, so the only way this could happen is if there would be
no more unused independent regressors. This would either lead to this,
or to the stopping criterion on line 25, depending on which equally
insignificant vector gets picked. The other criterion for early stopping
is if the chosen atom is orthogonal to y, which would make it
uninformative and would again mean that there are no better ones left,
so we might as well quit&amp;nbsp;looking.&lt;/p&gt;
&lt;p&gt;Also, we now make sure that [latex] L[/latex] is preallocated. Note that
&lt;code&gt;np.empty&lt;/code&gt; is marginally faster than &lt;code&gt;np.zeros&lt;/code&gt; because it does not
initialize the array to zero after allocating, so the untouched parts of
the array will contain whatever happened to be in memory before. In our
case, this means only the values above the main diagonal: everything on
and beneath is initialized before access. Luckily, the
&lt;code&gt;linalg.solve_triangular&lt;/code&gt; function ignores what it doesn&amp;#8217;t&amp;nbsp;need.&lt;/p&gt;
&lt;p&gt;This is a robust implementation, but still a couple of times slower than
least-angle regression. In the next part of the article we will see how
we can make it beat&amp;nbsp;&lt;span class="caps"&gt;LARS&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span id="footnote-1"&gt;&lt;a href="#footnote-1"&gt;1&lt;/a&gt;&lt;/span&gt; I always wanted to use this word in a
serious context :P&lt;br /&gt;
&lt;span id="footnote-2"&gt;&lt;a href="#footnote-2"&gt;2&lt;/a&gt;&lt;/span&gt; Rubinstein, R., Zibulevsky, M. and
Elad, M., [Efficient Implementation of the K-&lt;span class="caps"&gt;SVD&lt;/span&gt; Algorithm using Batch
Orthogonal Matching Pursuit][] Technical Report - &lt;span class="caps"&gt;CS&lt;/span&gt; Technion, April
2008.&lt;br /&gt;
&lt;span id="footnote-3"&gt;&lt;a href="#footnote-3"&gt;3&lt;/a&gt;&lt;/span&gt; &lt;a href="http://venefrombucharest.wordpress.com/2011/05/30/first-thoughts-on-orthogonal-matching-pursuit/" title="First thoughts on Orthogonal Matching Pursuit"&gt;First thoughts on Orthogonal
Matching Pursuit&lt;/a&gt; on this blog.&lt;br /&gt;
&lt;span id="footnote-4"&gt;&lt;a href="#footnote-4"&gt;4&lt;/a&gt;&lt;/span&gt; &lt;a href="https://github.com/scikit-learn/scikit-learn/blob/master/scikits/learn/linear_model/omp.py"&gt;omp.py&lt;/a&gt; on&amp;nbsp;github.&lt;/p&gt;
&lt;p&gt;[Efficient Implementation of the K-&lt;span class="caps"&gt;SVD&lt;/span&gt; Algorithm using Batch
  Orthogonal Matching Pursuit]:&amp;nbsp;http://www.cs.technion.ac.il/~ronrubin/Publications/&lt;span class="caps"&gt;KSVD&lt;/span&gt;-&lt;span class="caps"&gt;OMP&lt;/span&gt;-v2.pdf&lt;/p&gt;</summary><category term="efficient"></category><category term="numpy"></category><category term="omp"></category><category term="orthogonal matching pursuit"></category><category term="scipy"></category><category term="dictionary learning"></category><category term="scikit-learn"></category><category term="python"></category></entry><entry><title>Progress on Orthogonal Matching Pursuit</title><link href="http://vene.ro/blog/progress-on-orthogonal-matching-pursuit.html" rel="alternate"></link><updated>2011-08-02T16:56:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-08-02:blog/progress-on-orthogonal-matching-pursuit.html</id><summary type="html">&lt;p&gt;Since orthogonal matching pursuit (&lt;span class="caps"&gt;OMP&lt;/span&gt;) is an important part of signal
processing and therefore crucial to the image processing aspect of
dictionary learning, I am currently focusing on optimizing the &lt;span class="caps"&gt;OMP&lt;/span&gt; code
and making sure it is stable. &lt;span class="caps"&gt;OMP&lt;/span&gt; is a forward method like least-angle
regression, so it is natural to bench them against one&amp;nbsp;another.&lt;/p&gt;
&lt;p&gt;This has helped find a couple of bottlenecks. Time has been gained by
preallocating the array to store the Cholesky decomposition. Also, using
the &lt;span class="caps"&gt;LAPACK&lt;/span&gt; &lt;code&gt;potrs&lt;/code&gt; function in order to solve a system of the shape
\$latex &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;x=y\$ is faster than using &lt;code&gt;solve_triangular&lt;/code&gt; twice.&lt;/p&gt;
&lt;p&gt;I am still trying to optimize the code. We are working hard to make sure
that scikits.learn contributions are up to standards before&amp;nbsp;merging.&lt;/p&gt;</summary><category term="omp"></category><category term="orthogonal matching pursuit"></category><category term="scikit-learn"></category></entry><entry><title>SparsePCA in scikits.learn-git</title><link href="http://vene.ro/blog/sparsepca-in-scikits-learn-git.html" rel="alternate"></link><updated>2011-07-19T12:01:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-07-19:blog/sparsepca-in-scikits-learn-git.html</id><summary type="html">&lt;p&gt;I am happy to announce that the Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; code has been reviewed and
merged into the main &lt;code&gt;scikits.learn&lt;/code&gt; repository.&lt;/p&gt;
&lt;p&gt;You can use it if you install the bleeding edge &lt;code&gt;scikits.learn&lt;/code&gt; git
version, by first downloading the source code as explained in the
&lt;a href="http://scikit-learn.sourceforge.net/stable/developers/index.html#git-repo" title="installation user's guide"&gt;user&amp;#8217;s guide&lt;/a&gt;, and then running &lt;code&gt;python setup.py install&lt;/code&gt;.&lt;br /&gt;
[caption id=&amp;#8221;&amp;#8221; align=&amp;#8221;aligncenter&amp;#8221; width=&amp;#8221;400&amp;#8221; caption=&amp;#8221;Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; on
images of the digit 3&amp;#8221;][&lt;img alt="" src="http://scikit-learn.sourceforge.net/dev/_images/plot_digits_decomposition_4.png" title="Sparse PCA on images of the digit 3" /&gt;][][/caption]&lt;br /&gt;
To see what code is needed to produce an image such as the one above,
using &lt;code&gt;scikits.learn&lt;/code&gt;. check out this cool &lt;a href="http://scikit-learn.sourceforge.net/dev/auto_examples/decomposition/plot_digits_decomposition.html" title="decomposition example"&gt;decomposition example&lt;/a&gt;
that compares the results of most matrix decomposition models
implemented at the&amp;nbsp;moment.&lt;/p&gt;
&lt;p&gt;There are other new cool things that have been recently merged by other
contributors, such as support for sparse matrices in &lt;a href="http://scikit-learn.sourceforge.net/dev/modules/clustering.html#mini-batch-k-means" title="minibatch K-means"&gt;minibatch
K-means&lt;/a&gt;, and the &lt;a href="http://scikit-learn.sourceforge.net/dev/modules/mixture.html#infinite-gaussian-mixtures-dpgmm-classifier" title="variational infinite gaussian mixture model"&gt;variational infinite gaussian mixture model&lt;/a&gt;, so
I invite you to take a&amp;nbsp;look!&lt;/p&gt;
&lt;p&gt;[&lt;img alt="" src="http://scikit-learn.sourceforge.net/dev/_images/plot_digits_decomposition_4.png" title="Sparse PCA on images of the digit 3" /&gt;]:&amp;nbsp;http://scikit-learn.sourceforge.net/dev/_images/plot_digits_decomposition_4.png&lt;/p&gt;</summary><category term="pca"></category><category term="principal components analysis"></category><category term="scikit-learn"></category><category term="sparse pca"></category><category term="SparsePCA"></category><category term="spca"></category></entry><entry><title>K-Means for dictionary learning</title><link href="http://vene.ro/blog/k-means-for-dictionary-learning.html" rel="alternate"></link><updated>2011-07-10T14:27:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-07-10:blog/k-means-for-dictionary-learning.html</id><summary type="html">&lt;p&gt;[![Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset with whitening
&lt;span class="caps"&gt;PCA&lt;/span&gt;][]][][![Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset without
whitening&amp;nbsp;&lt;span class="caps"&gt;PCA&lt;/span&gt;][]][]&lt;/p&gt;
&lt;p&gt;One of the simplest, and yet most heavily constrained form of matrix
factorization, is vector quantization (&lt;span class="caps"&gt;VQ&lt;/span&gt;). Heavily used in image/video
compression, the &lt;span class="caps"&gt;VQ&lt;/span&gt; problem is a factorization [latex] X=&lt;span class="caps"&gt;WH&lt;/span&gt;[/latex]
where [latex] H[/latex] (our dictionary) is called the codebook and is
designed to cover the cloud of data points effectively, and each line of
[latex] W[/latex] is a unit&amp;nbsp;vector.&lt;/p&gt;
&lt;p&gt;This means that each each data point [latex] x_i[/latex] is
approximated as [latex] x_i \approx h_{k} = \sum_{j=1}\^{r}
\delta_{kj}h_{j}[/latex]. In other words, the closest row vector
(codeword/dictionary atom) [latex] h_k[/latex] of [latex] H[/latex] is
chosen as an approximation, and this is encoded as a unit vector [latex]
(\delta_{k1}, &amp;#8230;, \delta_{kr})[/latex]. The data representation
[latex] W[/latex] is composed of such&amp;nbsp;vectors.&lt;/p&gt;
&lt;p&gt;There is a variation called gain-shape &lt;span class="caps"&gt;VQ&lt;/span&gt; where instead of approximating
each point as one of the codewords, we allow a scalar multiplication
invariance: [latex] x_i \approx \alpha_ih_k[/latex]. This model
requires considerably more storage (each data point needs a floating
point number and an unsigned index, as opposed to just the index), but
it leads to a much better approximation.&lt;br /&gt;
Gain-shape &lt;span class="caps"&gt;VQ&lt;/span&gt; can equivalently be accomplished by normalizing each data
vector prior to fitting the&amp;nbsp;codebook.&lt;/p&gt;
&lt;p&gt;In order to fit a codebook [latex] H[/latex] for efficient &lt;span class="caps"&gt;VQ&lt;/span&gt; use, the
K-Means Clustering [&lt;a href="#footnote-1"&gt;1&lt;/a&gt;] algorithm is a natural thought. K-means is an
iterative algorithm that incrementally improves the dispersion of k
cluster centers in the data space until convergence. The cluster centers
are initialized in a random or procedural fashion, then, at each
iteration, the data points are assigned to the closest cluster center,
which is subsequently moved to the center of the points assigned to&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;scikits.learn.decomposition.KMeansCoder&lt;/code&gt; object from our
work-in-progress dictionary learning toolkit can learn a dictionary from
image patches using the K-Means algorithm, with optional local contrast
normalization and a &lt;span class="caps"&gt;PCA&lt;/span&gt; whitening transform. Using a trained object to
transform data points with orthogonal matching pursuit, with the
parameter &lt;code&gt;n_atoms=1&lt;/code&gt; is equivalent to gain-shape &lt;span class="caps"&gt;VQ&lt;/span&gt;. Of course you are
free to use any method of sparse coding such as &lt;span class="caps"&gt;LARS&lt;/span&gt;. The code used to
produce the example images on top of this post can be found in [&lt;a href="#footnote-2"&gt;2&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Using K-Means for learning the dictionary does not optimize over linear
combinations of dictionary atoms, like standard dictionary learning
methods do. However, it&amp;#8217;s considerably faster, and Adam Coates and
Andrew Ng suggest in [&lt;a href="#footnote-3"&gt;3&lt;/a&gt;] that as long as the dictionary is filled
with a large enough number of atoms and it covers well enough the cloud
of data (and of future test data) points, then K-Means, or even random
sampling of image patches, can perform remarkably well for some&amp;nbsp;tasks.&lt;/p&gt;
&lt;div id="footnote-1"&gt;
[1] [Wikipedia article on K-Means clustering][]

&lt;/div&gt;

&lt;div id="footnote-2"&gt;
[2] [K-Means Coder example][]

&lt;/div&gt;

&lt;div id="footnote-3"&gt;
[3] [**The importance of encoding versus training with sparse coding and
vector quantization**, Adam Coates and Andrew Y. Ng. In Proceedings of
the Twenty-Eighth International Conference on Machine Learning, 2011.][]

&lt;/div&gt;

&lt;p&gt;[Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset with whitening
  &lt;span class="caps"&gt;PCA&lt;/span&gt;]: http://localhost:8001/wp-content/uploads/2011/07/kmeans_w.png?w=250
    &amp;#8220;K-Means dictionary with whitening &lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;#8221;
  [![Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset with whitening
  &lt;span class="caps"&gt;PCA&lt;/span&gt;][]]: http://localhost:8001/wp-content/uploads/2011/07/kmeans_w.png
  [Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset without whitening
  &lt;span class="caps"&gt;PCA&lt;/span&gt;]: http://localhost:8001/wp-content/uploads/2011/07/kmeans_no_w.png?w=250
    &amp;#8220;K-Means dictionary without whitening &lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;#8221;
  [![Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset without
  whitening &lt;span class="caps"&gt;PCA&lt;/span&gt;][]]: http://localhost:8001/wp-content/uploads/2011/07/kmeans_no_w.png
  [&lt;strong&gt;The importance of encoding versus training with sparse coding and
  vector quantization&lt;/strong&gt;, Adam Coates and Andrew Y. Ng. In Proceedings of
  the Twenty-Eighth International Conference on Machine Learning,
  2011.]:&amp;nbsp;http://ai.stanford.edu/~ang/papers/icml11-EncodingVsTraining.pdf&lt;/p&gt;</summary><category term="dictionary learning"></category><category term="k-means"></category><category term="scikit-learn"></category><category term="vq"></category><category term="Uncategorized"></category></entry><entry><title>Image denoising with dictionary learning</title><link href="http://vene.ro/blog/image-denoising-with-dictionary-learning.html" rel="alternate"></link><updated>2011-07-07T20:00:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-07-07:blog/image-denoising-with-dictionary-learning.html</id><summary type="html">&lt;p&gt;I am presenting an image denoising example that fully runs under my
local scikits-learn fork. Coming soon near&amp;nbsp;you!&lt;/p&gt;
&lt;p&gt;The 400 square pixels area covering Lena&amp;#8217;s face was distorted by
additive gaussian noise with a standard deviation of 50 (pixel values
are ranged&amp;nbsp;0-256.)&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Lena image denoising using dictionary learning" src="http://localhost:8001/wp-content/uploads/2011/07/denoise3.png" title="Lena denoising" /&gt;][]&lt;/p&gt;
&lt;p&gt;The dictionary contains 100 atoms of shape 4x4 and was trained using
10000 random patches extracted from the undistorted image. Then, each
one of the four 100 square pixel areas was reconstructed using the
dictionary learning model and a different transform&amp;nbsp;method.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="caps"&gt;OMP&lt;/span&gt;-1 reconstructs each patch as the closest dictionary atom,
    multiplied by a variable coefficient. This is similar to the idea of
    gain-shape vector&amp;nbsp;quantization.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;OMP&lt;/span&gt;-2 is like &lt;span class="caps"&gt;OMP&lt;/span&gt;-1, but it considers 2 atoms instead of just one.
    This takes advantage of the fact that the natural dictionary atoms
    are of such nature to efficiently represent random image patches
    when&amp;nbsp;combined.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;LARS&lt;/span&gt; finds a reconstruction of each image patch as a solution to a
    Lasso problem, solved using least angle&amp;nbsp;regression.&lt;/li&gt;
&lt;li&gt;Thresholding is a simple and quick non-linearity that (as it is
    currently implemented, based on [&lt;a href="#footnote-1"&gt;1&lt;/a&gt;], where it is not intended
    for reconstruction but for classification) breaks the local
    brightness of the image fragment. The bottom right fragment was
    forcefully renormalized to stretch fit into the 0-256 range, but
    brightness differences can be&amp;nbsp;seen.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id="footnote-1"&gt;
[1] [**The importance of encoding versus training with sparse coding and
vector quantization**, Adam Coates and Andrew Y. Ng. In Proceedings of
the Twenty-Eighth International Conference on Machine Learning, 2011.][]

&lt;/div&gt;

&lt;p&gt;[&lt;img alt="Lena image denoising using dictionary learning" src="http://localhost:8001/wp-content/uploads/2011/07/denoise3.png" title="Lena denoising" /&gt;]: http://localhost:8001/wp-content/uploads/2011/07/denoise3.png
  [&lt;strong&gt;The importance of encoding versus training with sparse coding and
  vector quantization&lt;/strong&gt;, Adam Coates and Andrew Y. Ng. In Proceedings of
  the Twenty-Eighth International Conference on Machine Learning,
  2011.]:&amp;nbsp;http://ai.stanford.edu/~ang/papers/icml11-EncodingVsTraining.pdf&lt;/p&gt;</summary><category term="denoising"></category><category term="dictionary learning"></category><category term="scikit-learn"></category></entry><entry><title>Dictionary learning sneak peek</title><link href="http://vene.ro/blog/dictionary-learning-sneak-peek.html" rel="alternate"></link><updated>2011-06-24T12:06:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-06-24:blog/dictionary-learning-sneak-peek.html</id><summary type="html">&lt;p&gt;Closing in on the goal of integrating J. Mairal&amp;#8217;s dictionary learning in
the scikit, I stitched together a couple of&amp;nbsp;examples.&lt;/p&gt;
&lt;p&gt;The code is not yet integrated according to our standards, but here is
the kind of results you can&amp;nbsp;expect.&lt;/p&gt;
&lt;p&gt;Here is how a dictionary obtained from 8x8 patches of Lena looks like.
Pretty much it looks as expected: gabor-like wavelets with different
rotations and shifts, which means things are&amp;nbsp;working!&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Dictionary learned from lena patches" src="http://localhost:8001/wp-content/uploads/2011/06/image.png" title="Dictionary learned from lena patches" /&gt;][]&lt;/p&gt;
&lt;p&gt;And here is how it works for denoising Lena. On the left is the noisy
image and on the right is the reconstruction from a learned dictionary.
The sparse code code producing the result on the right is found using
orthogonal matching&amp;nbsp;pursuit.&lt;/p&gt;
&lt;p&gt;This is by no means a good denoising example, I have no idea at the
moment how to tweak the patch sizes and the model parameters to obtain a
better result. This is just a sneak peek and pretty soon you will see
better&amp;nbsp;stuff!&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Denoising Lena with dictionary learning and OMP" src="http://localhost:8001/wp-content/uploads/2011/06/denoise.png" title="Denoising Lena" /&gt;][]&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Dictionary learned from lena patches" src="http://localhost:8001/wp-content/uploads/2011/06/image.png" title="Dictionary learned from lena patches" /&gt;]: http://localhost:8001/wp-content/uploads/2011/06/image.png
  [&lt;img alt="Denoising Lena with dictionary learning and OMP" src="http://localhost:8001/wp-content/uploads/2011/06/denoise.png" title="Denoising Lena" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/06/denoise.png&lt;/p&gt;</summary><category term="Uncategorized"></category></entry><entry><title>Summer of Code roadmap, part 1</title><link href="http://vene.ro/blog/summer-of-code-roadmap-part-1.html" rel="alternate"></link><updated>2011-06-12T14:28:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-06-12:blog/summer-of-code-roadmap-part-1.html</id><summary type="html">&lt;p&gt;After a little busy while, I have graduated and entered the summer
vacation, which means time for serious GSoC&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Me on graduation day" src="http://localhost:8001/wp-content/uploads/2011/06/p1080283.jpg" title="Graduation day" /&gt;][]&lt;/p&gt;
&lt;p&gt;So we had a little conference in order to discuss what will be done and
when. We gathered quite a few code snippets since the official start of
the project, but it&amp;#8217;s now time to talk about integration and pull&amp;nbsp;requests.&lt;/p&gt;
&lt;p&gt;Here is the&amp;nbsp;plan:&lt;/p&gt;
&lt;h4&gt;SparsePCA&lt;/h4&gt;
&lt;p&gt;First pull request due: &lt;strong&gt;June 15&lt;/strong&gt;&lt;br /&gt;
This will be the use case I blogged about &lt;a href="http://venefrombucharest.wordpress.com/2011/05/23/sparse-pca/" title="Sparse PCA"&gt;before&lt;/a&gt;. Specifically, we
want to learn a dictionary of sparse atoms, but representations of the
data will be&amp;nbsp;dense.&lt;/p&gt;
&lt;h4&gt;SparseCoder&lt;/h4&gt;
&lt;p&gt;First pull request due: &lt;strong&gt;June 25&lt;/strong&gt;&lt;br /&gt;
This is the transpose of the SparsePCA problem. We are learning the
optimal, dense dictionary for sparse representations of the&amp;nbsp;data.&lt;/p&gt;
&lt;h4&gt;KMeansCoder&lt;/h4&gt;
&lt;p&gt;First pull request due: &lt;strong&gt;June 30&lt;/strong&gt;&lt;br /&gt;
This method builds the dictionary out of cluster centers found by&amp;nbsp;K-means.&lt;/p&gt;
&lt;h4&gt;OnlineSparseCoder&lt;/h4&gt;
&lt;p&gt;First pull request due: &lt;strong&gt;July 10&lt;/strong&gt;&lt;br /&gt;
This will involve the online learning tricks suggested in Julien
Mairal&amp;#8217;s work and will allow for faster computations of both sparse &lt;span class="caps"&gt;PCA&lt;/span&gt;
and sparse coding. In the case of sparse coding, it will make use of the
scikits.learn &lt;span class="caps"&gt;API&lt;/span&gt; for online&amp;nbsp;learning.&lt;/p&gt;
&lt;p&gt;While I will try to keep the deadlines for the initial pull requests as
strictly as I can, we did not establish deadlines for merging, since
this will depend on more factors. As long as the pull requests are up,
the code review system will push it forward towards the merge. The focus
is on teamwork and on feedback cycles as short as possible, as opposed
to falling into the trap of delaying work until the night before the&amp;nbsp;deadline.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Me on graduation day" src="http://localhost:8001/wp-content/uploads/2011/06/p1080283.jpg" title="Graduation day" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/06/p1080283.jpg&lt;/p&gt;</summary><category term="gsoc"></category><category term="Uncategorized"></category></entry><entry><title>First thoughts on Orthogonal Matching Pursuit</title><link href="http://vene.ro/blog/first-thoughts-on-orthogonal-matching-pursuit.html" rel="alternate"></link><updated>2011-05-30T13:02:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-05-30:blog/first-thoughts-on-orthogonal-matching-pursuit.html</id><summary type="html">&lt;p&gt;I am working on implementing the Orthogonal Matching Pursuit (&lt;span class="caps"&gt;OMP&lt;/span&gt;)
algorithm for the scikit. It is an elegant algorithm (that almost writes
itself in Numpy!) to compute a greedy approximation to the solution of a
sparse coding&amp;nbsp;problem:&lt;/p&gt;
&lt;p&gt;\$\$ \text{argmin} \big|\big|\gamma\big|\big|_0 \text{ subject
to }\big|\big|x-D\gamma\big|\big|_2\^2 \leq&amp;nbsp;\epsilon\$\$&lt;/p&gt;
&lt;p&gt;or (in a different&amp;nbsp;parametrization)&lt;/p&gt;
&lt;p&gt;\$\$ \text{argmin} \big|\big|x - D\gamma\big|\big|_2\^2\text{
subject to }\big|\big|\gamma\big|\big|_0 \leq&amp;nbsp;m\$\$&lt;/p&gt;
&lt;p&gt;The second formulation is interesting in that it gives one of the few
algorithms for sparse coding that can control the actual number of
non-zero entries in the solution. Some dictionary learning methods need
this (I&amp;#8217;m thinking of&amp;nbsp;K-&lt;span class="caps"&gt;SVD&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Both problems are solved by the same algorithm, with a different
stopping condition. The gist of it is to include at each iteration, the
atom with the highest correlation to the current residual. However, as
opposed to regular Matching Pursuit, here, after choosing the atom, the
input signal is orthogonally projected to the space spanned by the
chosen atoms. This involves the solution of a least squares problem at
each step. However, because the problem is almost the same at each
iteration, only with one more column added to the matrix, this can be
easily solved by maintaining a &lt;span class="caps"&gt;QR&lt;/span&gt; or Cholesky decomposition of the
dictionary matrix that is updated at each&amp;nbsp;step.&lt;/p&gt;
&lt;p&gt;Rubinstein et al. [1] came up with a clever method to optimize the
calculations, based on the fact that usually in practice we never have
to find a sparse coding for a single signal, but usually for a batch.
They called this method Batch &lt;span class="caps"&gt;OMP&lt;/span&gt;, and it is based on a straightforward
modification of the Cholesky update algorithm, taking advantage of
precomputing the Gram matrix [latex]&amp;nbsp;G=D&amp;#8217;D[/latex].&lt;/p&gt;
&lt;p&gt;Based on my experiments, their batch update is the fastest, even though
it lags behind if invoked with too small a batch. As soon as I make sure
the implementation is robust and ready for use, I will make some&amp;nbsp;benchmarks.&lt;/p&gt;
&lt;p&gt;Update: Here&amp;#8217;s a little proof that it works!&lt;br /&gt;
[&lt;img alt="Stem plot for sparse signals recovered by OMP" src="http://localhost:8001/wp-content/uploads/2011/06/omp.png" title="Orthogonal Matching Pursuit sparse signal recovery" /&gt;][]&lt;/p&gt;
&lt;p&gt;Update 2: Here&amp;#8217;s a little benchmark:&lt;br /&gt;
[&lt;img alt="Orthogonal Matching Pursuit benchmark" src="http://localhost:8001/wp-content/uploads/2011/06/omp_bench.png" title="OMP benchmark, time and error" /&gt;][]&lt;br /&gt;
[1]&amp;nbsp;http://www.cs.technion.ac.il/\~ronrubin/Publications/&lt;span class="caps"&gt;KSVD&lt;/span&gt;-&lt;span class="caps"&gt;OMP&lt;/span&gt;-v2.pdf&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Stem plot for sparse signals recovered by OMP" src="http://localhost:8001/wp-content/uploads/2011/06/omp.png" title="Orthogonal Matching Pursuit sparse signal recovery" /&gt;]: http://localhost:8001/wp-content/uploads/2011/06/omp.png
  [&lt;img alt="Orthogonal Matching Pursuit benchmark" src="http://localhost:8001/wp-content/uploads/2011/06/omp_bench.png" title="OMP benchmark, time and error" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/06/omp_bench.png&lt;/p&gt;</summary><category term="Uncategorized"></category><category term="dictionary learning"></category><category term="omp"></category><category term="orthogonal matching pursuit"></category></entry><entry><title>Sparse PCA</title><link href="http://vene.ro/blog/sparse-pca.html" rel="alternate"></link><updated>2011-05-23T15:19:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-05-23:blog/sparse-pca.html</id><summary type="html">&lt;p&gt;I have been working on the integration into the scikits.learn codebase
of a sparse principal components analysis (SparsePCA) algorithm coded by
Gaël and Alexandre and based on [[1]][]. Because the name &amp;#8220;sparse &lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;#8221;
has some inherent ambiguity, I will describe in greater depth what
problem we are actually solving, and what it can be used&amp;nbsp;for.&lt;/p&gt;
&lt;h1&gt;The&amp;nbsp;problem&lt;/h1&gt;
&lt;p&gt;Mathematically, this implementation of Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;nbsp;solves:&lt;/p&gt;
&lt;p&gt;\$latex (U\^*,&amp;nbsp;V\^*)=\underset{U,V}{\mathrm{argmin\,}}\frac{1}{2}||X-&lt;span class="caps"&gt;UV&lt;/span&gt;||_2\^2+\alpha||V||_1\$&lt;/p&gt;
&lt;p&gt;with \$latex || U_k ||_2 = 1\$ for all \$latex 0 \leq k \&amp;lt;&amp;nbsp;n_{atoms}\$&lt;/p&gt;
&lt;p&gt;This looks really abstract so let&amp;#8217;s try to interpret it. We are looking
for a matrix factorization \$latex &lt;span class="caps"&gt;UV&lt;/span&gt;\$ of \$latex X \in
\mathbf{R}\^{n_{samples}\times n_{features}}\$, just like in
ordinary &lt;span class="caps"&gt;PCA&lt;/span&gt;. The interpretation is that the \$latex n_{atoms}\$ lines
of \$latex V\$ are the extracted components, while the lines of \$latex
U\$ are the coordinates of the samples in this&amp;nbsp;projection.&lt;/p&gt;
&lt;p&gt;The most important difference between this and &lt;span class="caps"&gt;PCA&lt;/span&gt; is that we enforce
sparsity on the &lt;em&gt;components&lt;/em&gt;. In other words, we look for a
representation of the data as a linear combination of sparse&amp;nbsp;signals.&lt;/p&gt;
&lt;p&gt;Another difference is that, unlike in &lt;span class="caps"&gt;PCA&lt;/span&gt;, here we don&amp;#8217;t constrain U to
be orthogonal, just to consist of normalized column vectors. There are
different approaches where this constraint appears too, and they are on
the list for this summer, but I&amp;nbsp;digress.&lt;/p&gt;
&lt;h1&gt;The&amp;nbsp;approach&lt;/h1&gt;
&lt;p&gt;As usual, such optimization problems are solved by alternatively
minimizing one of the variables while keeping the other fixed, until
convergence is&amp;nbsp;reached.&lt;/p&gt;
&lt;p&gt;The update of \$latex V\$ (the dictionary) is computed as the solution
of a Lasso least squares problem.  We allow the user to choose between
the least angle regression method (&lt;span class="caps"&gt;LARS&lt;/span&gt;) or stochastic gradient descent
as algorithms to solve the Lasso&amp;nbsp;problem.&lt;/p&gt;
&lt;p&gt;The update of \$latex U\$ is block coordinate descent with warm restart.
This is a batch adaptation of an online algorithm proposed by Mairal et
al. in&amp;nbsp;[[1]][].&lt;/p&gt;
&lt;h1&gt;Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; as a&amp;nbsp;transformer&lt;/h1&gt;
&lt;p&gt;Of course, in order to be of practical use, the code needs to be
refactored into a scikits.learn transformer object, just like
&lt;code&gt;scikits.learn.decomposition.pca&lt;/code&gt;. This means that the optimization
problem described above corresponds to the fitting stage. The post-fit
state of the transformer is given by the learned components (the matrix
\$latex V\$&amp;nbsp;above).&lt;/p&gt;
&lt;p&gt;In order to transform new data according to the learned sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; model
(for example, prior to classification of the test data), we simply need
to do a least squares projection of the new data on the sparse&amp;nbsp;components.&lt;/p&gt;
&lt;h1&gt;What is it good&amp;nbsp;for?&lt;/h1&gt;
&lt;p&gt;For applications such as text and image processing, its great advantage
is interpretability. When running a regular &lt;span class="caps"&gt;PCA&lt;/span&gt; on a set of documents in
bag of words format, we can find an interesting visualisation on a
couple of components, and it can show discrimination or clusters. The
biggest problem is that the maximum variance components found by &lt;span class="caps"&gt;PCA&lt;/span&gt;
have very dense expressions as linear combinations of the initial
features. In practice, sometimes interpretation is made by simply
marking the \$latex k\$ variables with the highest coefficients in this
representation, and basically interpreting as if the rest are truncated
to 0 (this has been taught to me in a class on &lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;nbsp;interpretation).&lt;/p&gt;
&lt;p&gt;Such an approximation can be highly misleading, and now we offer you the
sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; code that can extract components with only few non-zero
coefficients, and therefore easy to&amp;nbsp;interpret.&lt;/p&gt;
&lt;p&gt;For image data, sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; should extract local components such as,
famously, parts of the face in the case of face&amp;nbsp;recognition.&lt;/p&gt;
&lt;p&gt;Personally I can&amp;#8217;t wait to have it ready for the scikit so that I can
play with it in some of my projects. I have two tasks where I can&amp;#8217;t wait
to see the results: one is related to &lt;a href="http://venefrombucharest.wordpress.com/2011/04/14/a-look-at-romanian-verbs-with-scikits-learn/" title="A look at Romanian verbs with scikits-learn"&gt;Romanian infinitives&lt;/a&gt;, where
&lt;span class="caps"&gt;PCA&lt;/span&gt; revealed structure, and I would love to see how it looks with sparse
n-gram components. The other task is to plug it in as feature extractor
for handwritten digit classification, for my undergraduate&amp;nbsp;thesis.&lt;/p&gt;
&lt;p&gt;&lt;span id="footnote_1"&gt;[1] &lt;a href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf"&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;[[1]]:&amp;nbsp;#footnote_1&lt;/p&gt;</summary><category term="dictionary learning"></category><category term="pca"></category><category term="sparse pca"></category><category term="SparsePCA"></category><category term="spca"></category><category term="scikit-learn"></category></entry><entry><title>Customizing scikits.learn for a specific text analysis task</title><link href="http://vene.ro/blog/customizing-scikits-learn-for-a-specific-text-analysis-task.html" rel="alternate"></link><updated>2011-04-29T14:33:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-04-29:blog/customizing-scikits-learn-for-a-specific-text-analysis-task.html</id><summary type="html">&lt;p&gt;Scikits.learn is a great general library, but machine learning has so
many different application, that it is often very helpful to be able to
extend its &lt;span class="caps"&gt;API&lt;/span&gt; to better integrate with your code. With scikits.learn,
this is extremely easy to do using inheritance and using the pipeline&amp;nbsp;module.&lt;/p&gt;
&lt;h2&gt;The&amp;nbsp;problem&lt;/h2&gt;
&lt;p&gt;While continuing the &lt;a href="http://venefrombucharest.wordpress.com/2011/04/14/a-look-at-romanian-verbs-with-scikits-learn/" title="A look at Romanian verbs with scikits-learn"&gt;morphophonetic analysis of Romanian verbal
forms&lt;/a&gt;, I found the need to streamline my workflow to allow for more
complex models. There were a lot of free model parameters and it would
have been painful to interactively tweak everything in order to find a
good&amp;nbsp;combination&lt;/p&gt;
&lt;p&gt;In my case, I needed to read a file containing infinitives and labels
corresponding to conjugation groups, and run a linear support vector
classifier on this data. The &lt;span class="caps"&gt;SVC&lt;/span&gt; has its C parameter that needs to be
tweaked, but I also had some ideas that arose from the images in my old
post. There, I compared the way the data looked when represented as
differently sized n-gram features. Furthermore, I compared the count
features (ie. features indicating the number of times an n-gram occurs
in a string) with binary features (ie. indicating only whether the
n-gram occurs in the string or not). It looked to me like, for such a
low-level text analysis task, using counts only adds&amp;nbsp;noise.&lt;/p&gt;
&lt;p&gt;For this reason, the &lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt; was not
enough for me. It only returns count features. There was also another
thing that needed to be adjusted: by default, its analyzer uses a
preprocessor that strips accented characters, and I had strong reasons
to believe that Romanian diacritics are very relevant for the learning
task. So, I needed to extend the&amp;nbsp;vectorizer.&lt;/p&gt;
&lt;h2&gt;The&amp;nbsp;solution&lt;/h2&gt;
&lt;p&gt;The code I came up with is &lt;a href="https://github.com/vene/misc-nlp/blob/master/conjugation/grid_search_example/preprocess.py"&gt;here&lt;/a&gt;. I tried to build a class that would
be as specific to my needs as possible. It is important to retain the
full &lt;span class="caps"&gt;API&lt;/span&gt;, however. Note the &lt;code&gt;y=None&lt;/code&gt; parameter in the fit functions. Its
necessity will become clear in a&amp;nbsp;moment.&lt;/p&gt;
&lt;p&gt;Another tricky part was exposing the &lt;code&gt;max_n&lt;/code&gt; parameter from the inner
analyzer. This was not really natural, but it simplified the
constructions later&amp;nbsp;on.&lt;/p&gt;
&lt;p&gt;My &lt;code&gt;InfinitivesExtractor&lt;/code&gt; class builds a data matrix from a list of
strings. After using it, the data needs to be passed to the classifier,
an instance of &lt;code&gt;svm.LinearSVC&lt;/code&gt;. The &lt;code&gt;pipeline&lt;/code&gt;module in scikits.learn
allows us to plug components into eachother in order to build a more
complex object. In this case, we would like a classifier that receives a
string as input, and directly outputs its label. We wouldn&amp;#8217;t want the
user to have to manually use the feature extractor prior to&amp;nbsp;classification.&lt;/p&gt;
&lt;p&gt;The pipeline is very easy to&amp;nbsp;build:  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;pipeline = Pipeline([('extr', InfinitivesExtractor()), ('svc', LinearSVC(multi_class=True))])&lt;/code&gt;&lt;br /&gt;
The pipeline object now works exactly as expected: we can call fit and
predict on it. It also exposes the parameters of its constituents, by
prefixing them with the name of that component. For example, the support
vector machine&amp;#8217;s C parameter can be accessed as&amp;nbsp;pipeline.svc__C.&lt;/p&gt;
&lt;p&gt;All that is left now is to see whether this is a good model, and what
combination of parameters makes it work the best. Scikits.learn provides
a great tool for choosing the parameters: the &lt;code&gt;grid_search&lt;/code&gt; module. When
working with models like support vector machines, model parameters (such
as the radial basis kernel width) usually need to be chosen by cross
validation, because intuition doesn&amp;#8217;t help much when dealing with high
dimensional&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;Grid search allows the definition of a discrete range of values for
multiple parameters. Then, for each combination of parameters, it fits
and evaluates a model using cross-validation, and the model with the
best score is the winner. Because we combined the components into a
pipeline, it is very easy to run grid search on the combined model, and
to simultaneously tweak the settings both for the extractor and for the&amp;nbsp;classifier.&lt;/p&gt;
&lt;p&gt;After running the grid search using the code &lt;a href="https://github.com/vene/misc-nlp/blob/master/conjugation/grid_search_example/gridsearch.py"&gt;here&lt;/a&gt;, I found that
indeed, using binary features instead of occurence counts improves
performance. I also found that the optimal n-gram length is 5, but the
gain is not that big when compared to a length of 3, which generates a
lot less&amp;nbsp;features.&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;I hope that I managed to show the strength of a well-designed &lt;span class="caps"&gt;API&lt;/span&gt;.
Because of it, it would be very easy to add, for example, an extra layer
for dimensionality reduction before classification. It would only
require an extra item in the pipeline constructor. A call from a
web-based frontend, for example, would be very short and simple. Because
of the consistency in the scikits.learn classes, we can write cleaner
and better code, and therefore work with greater&amp;nbsp;efficiency.&lt;/p&gt;</summary><category term="nlp"></category><category term="scikit-learn"></category></entry><entry><title>An overview of dictionary learning: Terminology</title><link href="http://vene.ro/blog/an-overview-of-dictionary-learning-terminology.html" rel="alternate"></link><updated>2011-04-15T14:10:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-04-15:blog/an-overview-of-dictionary-learning-terminology.html</id><summary type="html">&lt;p&gt;My GSoC proposal is titled &amp;#8220;Dictionary learning in scikits.learn&amp;#8221; and in
the project, I plan to implement methods used in state of the art
research and industry applications in signal and image processing. In
this post, I want to clarify the terminology&amp;nbsp;used.&lt;/p&gt;
&lt;p&gt;Usually the terms &lt;em&gt;dictionary learning&lt;/em&gt; and &lt;em&gt;sparse coding&lt;/em&gt; are used
interchangably. Also my proposal contains methods such as Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt;
which are technically not &lt;em&gt;sparse coding&lt;/em&gt; but closely related&amp;nbsp;problems.&lt;/p&gt;
&lt;p&gt;The basic idea is the approximation of a signal vector [latex] x \in
\mathbb{R}\^d[/latex] by a linear combination of components, as good as
possible, under certain constraints. This can be formulated as a basic
(unconstrained) loss function measuring the quality of the
approximation: [latex] \mathcal{L}(x, D, \alpha) = \big|\big|x -
D\alpha\big|\big|\^2_2, D \in \mathbb{R}\^{d \times r}, \alpha
\in \mathbb{R}\^r[/latex], where [latex] r[/latex] is the dimension of
the dictionary (the number of &lt;em&gt;components)&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;When working with a dataset of more signal vectors, the overall basic
loss for such an approximation is [latex] \mathcal{L}(X, D, A) =
\sum_{i = 1}\^N \mathcal{L}(x_i, D, \alpha_i) = \big|\big|X -
&lt;span class="caps"&gt;DA&lt;/span&gt;\big|\big|\^2_F[/latex]. Minimizing such a loss function amounts to
finding the closest (in the Frobenius sense) matrix factorization
[latex] &lt;span class="caps"&gt;DA&lt;/span&gt;[/latex] that approximates the data matrix [latex]&amp;nbsp;X[/latex]&lt;/p&gt;
&lt;p&gt;This generic problem is called a &lt;strong&gt;matrix factorization problem&lt;/strong&gt;. Many
classical problems are matrix factorization problems with additional
constraints. For example, &lt;span class="caps"&gt;PCA&lt;/span&gt; is a matrix factorization that constrains
[latex] D[/latex] to be orthogonal. &lt;span class="caps"&gt;NMF&lt;/span&gt; constrains both [latex]
D[/latex] and [latex] A[/latex] to have no negative elements. Sparse
variants of these two decompositions are useful for getting local
components, such as parts of faces. These are obtained by adding an
aditional constraint on the dictionary&amp;nbsp;columns.&lt;/p&gt;
&lt;p&gt;It can  be sometimes useful to consider the dictionary fixed. The signal
processing community has introduced over the years many such
dictionaries, for examples wavelets. These are used, for example, in the
&lt;span class="caps"&gt;JPEG2000&lt;/span&gt; compression&amp;nbsp;standard.&lt;/p&gt;
&lt;p&gt;A very useful represenation is when the dictionary is &lt;em&gt;overcomplete&lt;/em&gt;
([latex] r &gt; d[/latex]). The wavelets are an example of this. Given
such a dictionary, we are interested in an efficient encoding of a
vector [latex] x[/latex], in the sense of sparseness: we want to use as
few dictionary components as possible in our representation. Such a
solution is the vector [latex]\alpha[/latex] minimizing [latex]
\mathcal{L}(x, D, \alpha) +
\lambda\big|\big|\alpha\big|\big|_1[/latex] but other
sparsity-inducing constraints can be used. Such a vector is a &lt;strong&gt;sparse
coding&lt;/strong&gt; of [latex] x[/latex] and it can be solved using algorithms such
as least-angle regression and orthogonal matching&amp;nbsp;pursuit.&lt;/p&gt;
&lt;p&gt;However, we are not limited to using precomputed dictionaries. The term
&lt;strong&gt;dictionary learning&lt;/strong&gt; refers to methods of inferring, given [latex]
X[/latex], a (usually overcomplete) dictionary that will perform good at
sparsely encoding the data in [latex] X[/latex]. Such methods are more
expensive than using precomputed dictionaries, but they perform better,
since the dictionary is optimized for the current&amp;nbsp;dataset.&lt;/p&gt;
&lt;p&gt;Because usually such loss functions are non-convex in [latex] D[/latex]
and [latex] A[/latex] simultaneously, dictionary learning algorithms
alternate between minimizing each while keeping the other fixed. The
step that minimizes [latex] D[/latex] is sometimes called the
&lt;strong&gt;dictionary update&lt;/strong&gt; step, and the one minimizing [latex] A[/latex] is
(similarily to the case where the dictionary is always fixed) the
&lt;strong&gt;sparse coding&lt;/strong&gt; step. Dictionary learning algorithms differ in the
method used for each of this&amp;nbsp;step.&lt;/p&gt;
&lt;p&gt;To resume, many problems can be posed as matrix factorization problems.
Depending on the constraints imposed, the problem becomes interesting
for different applications. Dictionary learning is very good for image
reconstruction. Matrix decompositions with sparse undercomplete
dictionaries such as Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; can be used to find local features that
constitute the dataset, for example parts of faces, for a dataset of
facial images. &lt;span class="caps"&gt;NMF&lt;/span&gt; can be used in both under and overcomplete settings
and it offers a good model for additive data such as text or images. We
are interested in these variants and they are planned for implementation
in my GSoC&amp;nbsp;proposal.&lt;/p&gt;
&lt;p&gt;Julien Mairal&amp;#8217;s presentation of his work in this domain, available
&lt;a href="http://videolectures.net/icml09_mairal_odlsc/" title="Mairal dictionary learning"&gt;here&lt;/a&gt;, shows the theoretical background of such methods, along with
examples showing state of the art results in image&amp;nbsp;processing.&lt;/p&gt;</summary><category term="dictionary learning"></category><category term="scikit-learn"></category><category term="Uncategorized"></category></entry><entry><title>Newton interpolation and numerical differentiation</title><link href="http://vene.ro/blog/newton-interpolation-and-numerical-differentiation.html" rel="alternate"></link><updated>2011-04-15T13:34:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-04-15:blog/newton-interpolation-and-numerical-differentiation.html</id><summary type="html">&lt;p&gt;I am sharing some Python code code that I wrote as a school assignment.
This computes the Newton form of the interpolation polynomial of a given
set of points, and allows for the evaluation of both the polynomial and
its derivative, at a given point. This is an accurate way of estimating
the derivative of a complicated&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;Initially it plots the function, the interpolating polynomial and its
derivative. When clicking on the plot, the tangent to the interpolating
polynomial at the horizontal position of the mouse cursor is&amp;nbsp;plotted.&lt;/p&gt;
&lt;p&gt;It can be found here: &lt;a href="https://gist.github.com/921554"&gt;https://gist.github.com/921554&lt;/a&gt;&lt;/p&gt;</summary><category term="differentiation"></category><category term="interpolation"></category><category term="matplotlib"></category><category term="newton"></category><category term="numerical"></category><category term="numpy"></category><category term="python"></category></entry><entry><title>A look at Romanian verbs with scikits-learn</title><link href="http://vene.ro/blog/a-look-at-romanian-verbs-with-scikits-learn.html" rel="alternate"></link><updated>2011-04-14T01:40:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-04-14:blog/a-look-at-romanian-verbs-with-scikits-learn.html</id><summary type="html">&lt;p&gt;One of the problems we tackled here at my university is one as old as
the modern Romanian language. It is a problem for linguists, as well as
for foreigners trying to learn the language. We call it the root
alternations&amp;nbsp;problem.&lt;/p&gt;
&lt;p&gt;Similar to French and other languages, Romanian verbs are split into
four groups with different conjugation patterns. Except for the
irregular verbs, this categorization is performed based on the suffix of
the infinitive. However, the conjugation is not straightforward even
within these classes, because many verbs exhibit alternations in their
root. For example, the verb &lt;em&gt;a purta&lt;/em&gt; (to wear) becomes &lt;em&gt;eu port&lt;/em&gt; (I
wear) but &lt;em&gt;el poartă&lt;/em&gt; (he wears). It can be seen that the letter &lt;em&gt;o&lt;/em&gt; in
the root changes to &lt;em&gt;oa&lt;/em&gt; during conjugation. This makes learning the
language quite difficult, because we have no rules to describe when
these changes&amp;nbsp;occur.&lt;/p&gt;
&lt;p&gt;Attempts to formalize such rules from a computer scientific point of
view date back to &lt;span class="caps"&gt;G. C.&lt;/span&gt; Moisil in 1960. Such (incomplete) rules can be
formulated as context-sensitive grammars, since the alternations are
determined by the context in which certain characters&amp;nbsp;appear.&lt;/p&gt;
&lt;p&gt;This leads to the idea of analyzing the verbs from a machine learning
point of view: what can we find out by looking at n-gram representation
of the&amp;nbsp;infinitives?&lt;/p&gt;
&lt;p&gt;This is easy to do within scikits.learn. The &lt;code&gt;feature_extraction.text&lt;/code&gt;
package contains all the necessary tools: the &lt;code&gt;CharNGramExtractor&lt;/code&gt;,
which builds all the n-grams of a string, for n in an interval. Then, a
&lt;code&gt;CountVectorizer&lt;/code&gt; is built on top of the extractor. Its purpose is to
extract the features out of a list of documents and transform them into
a matrix representation of token counts. By postprocessing this matrix
we can obtain a binary representation, indicating only whether a token
occurs in a document or not, instead of the&amp;nbsp;count.&lt;/p&gt;
&lt;p&gt;In this case, documents are Romanian infinitives. This means we are
limited to using short n-grams, because the documents are themselves
short. There is also the question whether anything relevant can be found
out of such a representation which does not encode a lot of&amp;nbsp;information.&lt;/p&gt;
&lt;p&gt;After building the data matrix from the list of verbs, I plotted a 2D
&lt;span class="caps"&gt;PCA&lt;/span&gt; projection and here are the results. I am only posting a teaser for
now, but the results are&amp;nbsp;encouraging:&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Romanian infinitives as 2D projection" src="http://localhost:8001/wp-content/uploads/2011/04/infinitives_pca.png" title="infinitives_pca" /&gt;][]&lt;/p&gt;
&lt;p&gt;From the image it is clear that n-gram representations of the
infinitives induce clusters. Further results suggest that for certain
subclasses of the dataset, such a representation (or even a simpler one)
is enough to clearly answer whether a verb does not exhibit
alternations. This encourages further exploration of this path,
especially supervised and semi-supervised&amp;nbsp;approaches.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Romanian infinitives as 2D projection" src="http://localhost:8001/wp-content/uploads/2011/04/infinitives_pca.png" title="infinitives_pca" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/04/infinitives_pca.png&lt;/p&gt;</summary><category term="alternations"></category><category term="computational linguistics"></category><category term="infinitives"></category><category term="pca"></category><category term="principal components analysis"></category><category term="nlp"></category><category term="scikit-learn"></category></entry><entry><title>Tweaking matplotlib subplots for pretty results</title><link href="http://vene.ro/blog/tweaking-matplotlib-subplots-for-pretty-results.html" rel="alternate"></link><updated>2011-04-04T20:53:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-04-04:blog/tweaking-matplotlib-subplots-for-pretty-results.html</id><summary type="html">&lt;p&gt;When plotting multiple subplots using matplotlib, the axes rarely look
pretty with the default configuration. Since matplotlib figures are
abstract objects, designed for consistency in print as well as on
screen, tweaking their layout can get&amp;nbsp;tricky.&lt;/p&gt;
&lt;h3&gt;An&amp;nbsp;example&lt;/h3&gt;
&lt;p&gt;The following code is taken from the &lt;a href="http://scikit-learn.sourceforge.net/auto_examples/applications/plot_face_recognition.html" title="face recognition example"&gt;face recognition example&lt;/a&gt; in
scikits.learn:&lt;br /&gt;
&lt;code&gt;pl.figure(figsize=(1.8 * n_col, 2.4 * n_row))&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is very confusing at first, for somebody used to work on screen:
the quantities in there are actually inches! These are converted
implicitly to pixels through the dpi parameter, which is left as default
(80 dpi) in this&amp;nbsp;example.&lt;/p&gt;
&lt;p&gt;Then, it gets even worse: In order to tweak the positioning of the
subplots, this is what is&amp;nbsp;done:  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;pl.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)&lt;/code&gt;&lt;br /&gt;
Now, all of these are percents of the image height/width. The margins
are sort of like &lt;span class="caps"&gt;CSS&lt;/span&gt;-style margins, only relative to the bottom left
corner. In other words, &lt;code&gt;right=.99&lt;/code&gt; means that the right margin is 1%
away from the right&amp;nbsp;edge.&lt;/p&gt;
&lt;p&gt;The parameters &lt;code&gt;hspace&lt;/code&gt; and &lt;code&gt;wspace&lt;/code&gt; control the spacing between the
subplots. However these are kind of hard to get right, because,
obviously, there are more settings than there are degrees of&amp;nbsp;freedom.&lt;/p&gt;
&lt;h3&gt;My&amp;nbsp;tip&lt;/h3&gt;
&lt;p&gt;On my system, the default matplotlib backend is TkAgg. The matplotlib
backend controls the graphical environment that builds the plot windows,
as well as the rendering engine used. TkAgg has a &amp;#8220;configure subplots&amp;#8221;
button that opens a popup window with sliders to visually adjust the
parameters above. The problem is that the sliders are unlabeled, so I
needed to do an heuristic by first setting the parameters by hand and
then exploring the direction in which they need to be&amp;nbsp;changed.&lt;/p&gt;
&lt;p&gt;When I tried different backends, I found that WXAgg has labeled sliders.
This means you can adjust your subplots visually and you will have the
parameter values to use in the call to &lt;code&gt;subplots_adjust&lt;/code&gt; in one&amp;nbsp;go!&lt;/p&gt;
&lt;p&gt;You can set your backend to WXAgg by adding the line &lt;code&gt;backend: WXAgg&lt;/code&gt; in
your &lt;a href="http://matplotlib.sourceforge.net/users/customizing.html#the-matplotlibrc-file" title="Customizing matplotlib"&gt;matplotlibrc file&lt;/a&gt;.&lt;/p&gt;</summary><category term="matplotlib"></category><category term="python"></category></entry><entry><title>On setuptools subpackages</title><link href="http://vene.ro/blog/on-setuptools-subpackages.html" rel="alternate"></link><updated>2011-04-04T15:01:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-04-04:blog/on-setuptools-subpackages.html</id><summary type="html">&lt;p&gt;Today, I spent more than two hours trying to figure out why, despite
things working out fine in my development scikits.learn folder,
&lt;code&gt;python setup.py install&lt;/code&gt; would completely ignore the module I
refactored into a&amp;nbsp;subpackage.&lt;/p&gt;
&lt;p&gt;I imagined that simply adding it to the parent &lt;code&gt;__init__.py __all__&lt;/code&gt;
attribute would do, I kind of thought that setuptools automatically
finds the&amp;nbsp;subpackages.&lt;/p&gt;
&lt;p&gt;At first I thought of looking in &lt;code&gt;setup.py&lt;/code&gt;, but I only examined the one
in the topmost directory, which, in the case of scikits.learn, is two
degrees of separation away from the actual setup.py that takes care of
subpackages (ie. I was looking at &lt;code&gt;/setup.py&lt;/code&gt; instead of
&lt;code&gt;/scikits/learn/setup.py&lt;/code&gt;).  This had me fooled for a&amp;nbsp;while.&lt;/p&gt;
&lt;p&gt;The steps to add a working and installable module to a python
setuptools-based project are as&amp;nbsp;follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add a &lt;code&gt;__init__.py&lt;/code&gt; file in the folder (ie.
    &lt;code&gt;/scikits/learn/decomposition/__init__.py&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;If the module requires compiling or any special attention, add an
    appropriate  &lt;code&gt;__setup__.py&lt;/code&gt; file in the&amp;nbsp;folder.&lt;/li&gt;
&lt;li&gt;Update the &lt;code&gt;__init__.py __all__&lt;/code&gt; attribute in the parent folder (ie.
    &lt;code&gt;/scikits/learn/__init__.py&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Update the &lt;code&gt;setup.py&lt;/code&gt; in the parent folder (ie.
    &lt;code&gt;/scikits/learn/setup.py&lt;/code&gt;) by adding something like:&lt;br /&gt;
&lt;code&gt;config.add_subpackage('decomposition')&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Don&amp;#8217;t forget to do the same for the tests&amp;nbsp;subfolder!&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;While wasting so much time due to a simple beginner&amp;#8217;s mistake is not
very pleasant, I am not frustrated with setuptools. On the contrary, now
that I understand it better I can appreciate its flexibility and
clarity, when compared to, for example, MSBuild and Visual Studio
project files. Just one more reason to love&amp;nbsp;Python!&lt;/p&gt;</summary><category term="python"></category><category term="setuptools"></category></entry><entry><title>My first scikits.learn coding sprint</title><link href="http://vene.ro/blog/my-first-scikits-learn-coding-sprint.html" rel="alternate"></link><updated>2011-04-02T20:12:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-04-02:blog/my-first-scikits-learn-coding-sprint.html</id><summary type="html">&lt;p&gt;The fifth &lt;a href="http://scikit-learn.sourceforge.net/" title="scikits.learn"&gt;scikits.learn&lt;/a&gt; coding sprint took place Friday, April 1st
2011. For anyone who is not familiar with it, scikits.learn is a fast
and easy to use machine learning toolkit for the pylab environment
(Python, NumPy, SciPy,&amp;nbsp;Matplotlib.)&lt;/p&gt;
&lt;p&gt;This was a good opportunity for me to get code reviews by the developers
in order to bring my &lt;span class="caps"&gt;NMF&lt;/span&gt; code up to standards, so that it can be merged.
Though I live far from every nucleus of scikits-learn developers, I
efficiently participated via &lt;span class="caps"&gt;IRC&lt;/span&gt;. This way, I also got the chance to
help out a bit on Mathieu Blondel&amp;#8217;s Kernel &lt;span class="caps"&gt;PCA&lt;/span&gt; code, which will also be
merged into&amp;nbsp;main soon.&lt;/p&gt;
&lt;h2&gt;How it felt&amp;nbsp;like&lt;/h2&gt;
&lt;p&gt;Short answer:&amp;nbsp;awesome!&lt;/p&gt;
&lt;p&gt;Slightly longer answer: Everybody was very encouraging  and helpful.
They gave me a lot of feedback from which I learned a lot, and they
manifested the intention to merge soon. It is a pleasure to work on
projects that you like and use, especially when the projects leaders and
collaborators are so good to work&amp;nbsp;with.&lt;/p&gt;
&lt;p&gt;But the main reason why it makes me feel so good is that I&amp;#8217;m proud to
able to contribute on a project that I consider very significant and the
best in the field from many points of&amp;nbsp;view.&lt;/p&gt;
&lt;h2&gt;What I got&amp;nbsp;done&lt;/h2&gt;
&lt;p&gt;Most of my work was on the non-negative matrix factorization module that
I began some while back, but only intermitently worked on. It is now a
solid module with high test coverage, documentation, and a cool simple
example showing a sparse set of features for the digits dataset in
scikits.learn.  Apart from all the minor fixes in overall code quality
and cleanliness, probably what is the most relevant is the improvement
and the study of the initialization methods. I will look into this
further and document it on this blog, the point is that the choice of
initialization method greatly influences the speed of convergence, and
in the case of a high-tolerance setting, also the error obtained. Some
initializations are more fit for sparsity settings, while others are
more fit for dense&amp;nbsp;settings.&lt;/p&gt;
&lt;p&gt;I have a theory that I plan to test out, regarding the use of different
initialization methods for components and for data in a sparse&amp;nbsp;setting.&lt;/p&gt;
&lt;h2&gt;What I&amp;nbsp;learned&lt;/h2&gt;
&lt;p&gt;I think my greatest improvement was in terms of workflow and efficiency.
While my code was under review, I was receiving frequent comments on my
git pull request, and eventually I ended up responding to some comments
even before they were posted :). I sent small fixes as pull requests to
help other developers as much as I could. Before scikits.learn I had
never worked on a project with so many developers, and I think I handled
it well, even though I asked once or twice on the &lt;span class="caps"&gt;IRC&lt;/span&gt; channel for pieces
of&amp;nbsp;git-fu.&lt;/p&gt;
&lt;p&gt;I learned that it&amp;#8217;s difficult to tweak matplotlib subplots! I&amp;#8217;m still
staring at Alexandre Gramfort&amp;#8217;s tweak in my example and I have no idea
what he did to make it look so good. But I&amp;#8217;ll figure it out soon, I&amp;#8217;m&amp;nbsp;sure.&lt;/p&gt;
&lt;p&gt;I also learned a lot more about the intricacies of the scikits.learn
APIs, the philosophy of ease of use, and the project tree in&amp;nbsp;general.&lt;/p&gt;
&lt;p&gt;In short, the coding sprint has been a great and rewarding experience,
for which I thank all of you guys&amp;nbsp;there!&lt;/p&gt;</summary><category term="coding sprint"></category><category term="scikit-learn"></category></entry><entry><title>About</title><link href="http://vene.ro/blog/about.html" rel="alternate"></link><updated>2011-03-30T08:59:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-03-30:blog/about.html</id><summary type="html">&lt;p&gt;My name is Vlad, I am a master&amp;#8217;s student at the University of Bucharest,
I work there at the Centre for Computational Linguistics, and I am a
contributor to the Python machine learning library &lt;a href="http://scikit-learn.org" title="scikit-learn"&gt;scikit-learn&lt;/a&gt;.&lt;/p&gt;</summary></entry><entry><title>Hello world!</title><link href="http://vene.ro/blog/hello-world-2.html" rel="alternate"></link><updated>2011-03-30T08:59:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-03-30:blog/hello-world-2.html</id><summary type="html">&lt;p&gt;This is the blog where I will post updates regarding my work
on&lt;a href="http://scikit-learn.sourceforge.net/" title="scikits-learn"&gt;scikits-learn.&lt;/a&gt; I am applying as a GSoC 2011 student with the &lt;span class="caps"&gt;PSF&lt;/span&gt;
for work on this machine learning&amp;nbsp;library.&lt;/p&gt;
&lt;p&gt;I will also post related work that I do in the machine learning&amp;nbsp;field.&lt;/p&gt;
&lt;p&gt;I hope to prove an enjoyable read for everybody who stumbles on to this
site, or for those who will be forced to review it&amp;nbsp;:)&lt;/p&gt;</summary><category term="Uncategorized"></category></entry></feed>